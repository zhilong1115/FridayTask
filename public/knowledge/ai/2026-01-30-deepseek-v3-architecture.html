<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-01-30 - DeepSeek-V3 æ¶æ„åˆ†æï¼šå¦‚ä½•ç”¨æ›´å°‘çš„é’±è®­ç»ƒæ›´å¼ºçš„æ¨¡å‹</title>
  <style>
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      line-height: 1.8;
      color: #3c4043;
      padding: 20px;
      max-width: 900px;
      margin: 0 auto;
      background: #fff;
    }
    h1 { font-size: 1.4rem; margin-bottom: 8px; color: #1a73e8; }
    .meta { color: #70757a; font-size: 0.85rem; margin-bottom: 24px; }
    h2 { font-size: 1.1rem; margin: 28px 0 16px; color: #3c4043; border-bottom: 2px solid #e8f0fe; padding-bottom: 8px; }
    .bilingual { margin: 16px 0; }
    .zh { 
      border-left: 3px solid #f9ab00; 
      padding: 12px 16px; 
      margin-bottom: 12px;
      background: #fffbf0;
      border-radius: 0 8px 8px 0;
    }
    .en { 
      border-left: 3px solid #1a73e8; 
      padding: 12px 16px;
      background: #f8faff;
      border-radius: 0 8px 8px 0;
      color: #5f6368;
    }
    .highlight {
      background: #e8f0fe;
      padding: 16px;
      border-radius: 8px;
      margin: 20px 0;
    }
    .highlight h3 { color: #1a73e8; margin-bottom: 12px; font-size: 1rem; }
    ul { margin: 12px 0 12px 20px; }
    li { margin-bottom: 8px; }
    .stats {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
      gap: 12px;
      margin: 20px 0;
    }
    .stat-card {
      background: #f8f9fa;
      padding: 16px;
      border-radius: 8px;
      text-align: center;
    }
    .stat-value { font-size: 1.5rem; font-weight: 700; color: #1a73e8; }
    .stat-label { font-size: 0.75rem; color: #70757a; margin-top: 4px; }
    .design-highlight {
      background: #e6f4ea;
      padding: 16px;
      border-radius: 8px;
      margin: 20px 0;
      border-left: 4px solid #34a853;
    }
    .design-highlight h3 { color: #137333; margin-bottom: 12px; }
    code {
      background: #f1f3f4;
      padding: 2px 6px;
      border-radius: 4px;
      font-size: 0.9em;
      font-family: 'SF Mono', Monaco, monospace;
    }
    pre {
      background: #1e1e1e;
      color: #d4d4d4;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      margin: 16px 0;
      font-size: 0.8rem;
      line-height: 1.5;
    }
    .comment { color: #6a9955; }
    .keyword { color: #569cd6; }
    .terms {
      background: #f8f9fa;
      padding: 16px;
      border-radius: 8px;
      margin: 20px 0;
    }
    .terms h3 { margin-bottom: 12px; color: #3c4043; }
    .term { margin-bottom: 10px; padding-bottom: 10px; border-bottom: 1px solid #e8e8e8; }
    .term:last-child { border-bottom: none; }
    .term strong { color: #1a73e8; }
    .term-en { color: #70757a; font-size: 0.9em; }
    .market-connection {
      background: #fef7e0;
      padding: 16px;
      border-radius: 8px;
      margin: 20px 0;
      border-left: 4px solid #f9ab00;
    }
    .market-connection h3 { color: #e37400; margin-bottom: 8px; }
    .architecture-diagram {
      background: #f8f9fa;
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.75rem;
      overflow-x: auto;
      white-space: pre;
      line-height: 1.4;
    }
    .follow-up {
      margin-top: 32px;
      padding: 16px;
      border: 1px dashed #dadce0;
      border-radius: 8px;
      color: #70757a;
    }
  </style>
</head>
<body>
  <h1>ğŸ”¥ DeepSeek-V3ï¼šç”¨ $5.5M è®­ç»ƒå‡ºåª²ç¾ GPT-4 çš„æ¨¡å‹</h1>
  <p class="meta">2026-01-30 Â· AI æŠ€æœ¯ Â· GitHub: deepseek-ai/DeepSeek-V3 Â· â­ 20k+</p>

  <h2>ğŸ“– ä¸ºä»€ä¹ˆé‡è¦ / Why It Matters</h2>
  <div class="bilingual">
    <div class="zh">
      <p>è¿™å‘¨ç§‘æŠ€åœˆæœ€å¤§çš„æ–°é—»ï¼šä¸­å›½ AI å…¬å¸ <strong>DeepSeek</strong> å‘å¸ƒäº† V3 æ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘ GPT-4o å’Œ Claude-3.5ï¼Œä½†è®­ç»ƒæˆæœ¬åªæœ‰ <strong>$5.5M</strong>ï¼ˆçº¦ 278 ä¸‡ H800 GPU å°æ—¶ï¼‰ã€‚</p>
      <p>ä½œä¸ºå¯¹æ¯”ï¼ŒGPT-4 çš„è®­ç»ƒæˆæœ¬ä¼°è®¡åœ¨ <strong>$100M+</strong>ã€‚è¿™æ„å‘³ç€ä»€ä¹ˆï¼ŸAI è®­ç»ƒä¸å†æ˜¯åªæœ‰ OpenAI/Google æ‰èƒ½ç©çš„æ¸¸æˆã€‚</p>
    </div>
    <div class="en">
      <p>This week's biggest tech news: Chinese AI company <strong>DeepSeek</strong> released V3, achieving GPT-4o/Claude-3.5 level performance at only <strong>$5.5M</strong> training cost (2.78M H800 GPU hours).</p>
      <p>For comparison, GPT-4's training cost is estimated at <strong>$100M+</strong>. This democratizes large-scale AI training.</p>
    </div>
  </div>

  <div class="stats">
    <div class="stat-card">
      <div class="stat-value">671B</div>
      <div class="stat-label">æ€»å‚æ•°é‡ Total Params</div>
    </div>
    <div class="stat-card">
      <div class="stat-value">37B</div>
      <div class="stat-label">æ¿€æ´»å‚æ•° Active Params</div>
    </div>
    <div class="stat-card">
      <div class="stat-value">128K</div>
      <div class="stat-label">ä¸Šä¸‹æ–‡é•¿åº¦ Context</div>
    </div>
    <div class="stat-card">
      <div class="stat-value">$5.5M</div>
      <div class="stat-label">è®­ç»ƒæˆæœ¬ Training Cost</div>
    </div>
  </div>

  <h2>ğŸ—ï¸ æ¶æ„è®¾è®¡äº®ç‚¹ / Architecture Highlights</h2>
  
  <div class="design-highlight">
    <h3>âœ¨ äº®ç‚¹ 1: Mixture of Experts (MoE) æ¶æ„</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>é—®é¢˜ï¼š</strong>æ¨¡å‹è¶Šå¤§æ€§èƒ½è¶Šå¥½ï¼Œä½†å‚æ•°å¤šæ„å‘³ç€æ¨ç†æ—¶è®¡ç®—é‡å¤§ã€æˆæœ¬é«˜ã€‚</p>
        <p><strong>è§£å†³æ–¹æ¡ˆï¼š</strong>MoE è®©æ¨¡å‹æœ‰ 671B å‚æ•°ï¼Œä½†æ¯æ¬¡æ¨ç†åªæ¿€æ´» 37Bï¼ˆçº¦ 5.5%ï¼‰ã€‚å°±åƒä¸€ä¸ªæœ‰ 100 ä¸ªä¸“å®¶çš„å…¬å¸ï¼Œæ¯ä¸ªé—®é¢˜åªæ‰¾æœ€ç›¸å…³çš„å‡ ä¸ªä¸“å®¶æ¥å›ç­”ã€‚</p>
        <p><strong>ä¸ºä»€ä¹ˆå¥½ï¼š</strong>æ¨¡å‹å®¹é‡å¤§ï¼ˆçŸ¥è¯†å¤šï¼‰ï¼Œä½†æ¨ç†æˆæœ¬ä½ã€‚</p>
      </div>
      <div class="en">
        <p><strong>Problem:</strong> Larger models perform better, but more params = higher inference cost.</p>
        <p><strong>Solution:</strong> MoE has 671B params but only activates 37B (~5.5%) per inference. Like a company with 100 experts, but only the most relevant few answer each question.</p>
        <p><strong>Why it's good:</strong> High capacity (more knowledge) with low inference cost.</p>
      </div>
    </div>
  </div>

  <div class="architecture-diagram">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DeepSeek-V3 MoE Layer                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Token                                                â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Router  â”‚ â”€â”€â”€â”€ å†³å®šæ¿€æ´»å“ªäº› Expert                        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ E1  â”‚ E2  â”‚ E3  â”‚ ... â”‚E255 â”‚E256 â”‚  256 ä¸ª Expert       â”‚
â”‚  â”‚ âœ“   â”‚     â”‚ âœ“   â”‚     â”‚     â”‚ âœ“   â”‚  æ¯æ¬¡æ¿€æ´» 8 ä¸ª        â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”˜                     â”‚
â”‚     â”‚           â”‚                 â”‚                         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                 â–¼                                           â”‚
â”‚           Weighted Sum â†’ Output                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </div>

  <div class="design-highlight">
    <h3>âœ¨ äº®ç‚¹ 2: FP8 æ··åˆç²¾åº¦è®­ç»ƒ</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>åˆ›æ–°ï¼š</strong>é¦–æ¬¡åœ¨è¶…å¤§è§„æ¨¡æ¨¡å‹ä¸ŠéªŒè¯ FP8ï¼ˆ8ä½æµ®ç‚¹æ•°ï¼‰è®­ç»ƒçš„å¯è¡Œæ€§ã€‚</p>
        <p><strong>ä¼ ç»Ÿåšæ³•ï¼š</strong>ç”¨ FP32 æˆ– FP16/BF16 è®­ç»ƒï¼Œç²¾åº¦é«˜ä½†æ˜¾å­˜å ç”¨å¤§ã€‚</p>
        <p><strong>DeepSeek åšæ³•ï¼š</strong>å…³é”®è®¡ç®—ç”¨ FP8ï¼Œå‡å°‘ ~50% æ˜¾å­˜ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒç¨³å®šã€‚</p>
        <p><strong>ä¸ºä»€ä¹ˆå¥½ï¼š</strong>åŒæ ·çš„ GPU èƒ½è®­ç»ƒæ›´å¤§çš„æ¨¡å‹ï¼Œæˆæœ¬ç›´æ¥ç åŠã€‚</p>
      </div>
      <div class="en">
        <p><strong>Innovation:</strong> First to validate FP8 training feasibility at extreme scale.</p>
        <p><strong>Traditional:</strong> FP32 or FP16/BF16 trainingâ€”high precision but high memory.</p>
        <p><strong>DeepSeek:</strong> Use FP8 for key computations, ~50% memory reduction while staying stable.</p>
        <p><strong>Why it's good:</strong> Train larger models on same GPUs, cut costs in half.</p>
      </div>
    </div>
  </div>

  <div class="design-highlight">
    <h3>âœ¨ äº®ç‚¹ 3: Auxiliary-Loss-Free Load Balancing</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>é—®é¢˜ï¼š</strong>MoE æ¶æ„ä¸­ï¼Œå¦‚æœæŸäº› Expert è¢«è¿‡åº¦ä½¿ç”¨ï¼Œä¼šé€ æˆè´Ÿè½½ä¸å‡ï¼Œè®­ç»ƒæ•ˆç‡ä¸‹é™ã€‚</p>
        <p><strong>ä¼ ç»Ÿåšæ³•ï¼š</strong>åŠ ä¸€ä¸ªè¾…åŠ© loss å¼ºåˆ¶å¹³è¡¡ï¼Œä½†ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚</p>
        <p><strong>DeepSeek åˆ›æ–°ï¼š</strong>ä¸ç”¨è¾…åŠ© lossï¼Œé€šè¿‡å·§å¦™çš„ routing ç®—æ³•å®ç°è‡ªç„¶å¹³è¡¡ã€‚</p>
        <p><strong>ä¸ºä»€ä¹ˆå¥½ï¼š</strong>é±¼å’Œç†ŠæŒå…¼å¾—â€”â€”æ—¢æœ‰è´Ÿè½½å‡è¡¡ï¼Œåˆä¸ç‰ºç‰²æ¨¡å‹æ€§èƒ½ã€‚</p>
      </div>
      <div class="en">
        <p><strong>Problem:</strong> In MoE, if some experts are overused, it causes load imbalance and inefficiency.</p>
        <p><strong>Traditional:</strong> Add auxiliary loss to force balance, but hurts model performance.</p>
        <p><strong>DeepSeek:</strong> No auxiliary lossâ€”achieve natural balance through clever routing.</p>
        <p><strong>Why it's good:</strong> Best of both worldsâ€”balanced load without performance penalty.</p>
      </div>
    </div>
  </div>

  <div class="design-highlight">
    <h3>âœ¨ äº®ç‚¹ 4: Multi-Token Prediction (MTP)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>ä¼ ç»Ÿåšæ³•ï¼š</strong>æ¨¡å‹æ¯æ¬¡åªé¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚</p>
        <p><strong>DeepSeek åˆ›æ–°ï¼š</strong>è®­ç»ƒæ—¶è®©æ¨¡å‹åŒæ—¶é¢„æµ‹å¤šä¸ªæœªæ¥ tokenã€‚</p>
        <p><strong>å¥½å¤„ 1ï¼š</strong>è¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´æ·±å±‚çš„è¯­è¨€ç»“æ„å’Œé•¿ç¨‹ä¾èµ–ã€‚</p>
        <p><strong>å¥½å¤„ 2ï¼š</strong>å¯ä»¥ç”¨äº Speculative Decodingï¼Œæ¨ç†æ—¶ä¸€æ¬¡ç”Ÿæˆå¤šä¸ª tokenï¼ŒåŠ é€Ÿ 2-3xã€‚</p>
      </div>
      <div class="en">
        <p><strong>Traditional:</strong> Model predicts only the next token.</p>
        <p><strong>DeepSeek:</strong> Train model to predict multiple future tokens simultaneously.</p>
        <p><strong>Benefit 1:</strong> Forces model to learn deeper language structure and long-range dependencies.</p>
        <p><strong>Benefit 2:</strong> Enables Speculative Decoding for 2-3x faster inference.</p>
      </div>
    </div>
  </div>

  <div class="market-connection">
    <h3>ğŸ’° ä¸é‡‘èå¸‚åœºçš„å…³è”</h3>
    <div class="bilingual">
      <div class="zh">
        <p>DeepSeek çš„å‘å¸ƒç›´æ¥å†²å‡»äº†ç¾è‚¡ç§‘æŠ€æ¿å—ï¼š</p>
        <ul>
          <li><strong>NVIDIA è‚¡ä»·ä¸‹è·Œ</strong>ï¼šå¦‚æœè®­ç»ƒ AI ä¸éœ€è¦é‚£ä¹ˆå¤š GPUï¼Œå¯¹èŠ¯ç‰‡çš„éœ€æ±‚é¢„æœŸå°±ä¼šä¸‹é™</li>
          <li><strong>ä¼°å€¼é€»è¾‘åŠ¨æ‘‡</strong>ï¼šæŠ•èµ„è€…é‡æ–°è¯„ä¼° AI å…¬å¸çš„æŠ¤åŸæ²³â€”â€”å¦‚æœä¸­å›½èƒ½ç”¨ 1/20 çš„æˆæœ¬åšåˆ°åŒæ ·æ•ˆæœï¼Œç¾å›½å…¬å¸çš„ä¼˜åŠ¿åœ¨å“ªï¼Ÿ</li>
        </ul>
        <p>è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæœ€è¿‘ç§‘æŠ€è‚¡ç‰¹åˆ«æ˜¯ AI æ¦‚å¿µè‚¡æ³¢åŠ¨åŠ å‰§ã€‚</p>
      </div>
      <div class="en">
        <p>DeepSeek's release directly impacted US tech stocks:</p>
        <ul>
          <li><strong>NVIDIA dropped</strong>: If AI training needs fewer GPUs, demand expectations fall</li>
          <li><strong>Valuation concerns</strong>: Investors reassessing AI moatsâ€”if China can achieve the same at 1/20 cost, where's the US advantage?</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="terms">
    <h3>ğŸ“– å…³é”®æœ¯è¯­ / Key Terms</h3>
    <div class="term">
      <strong>MoE (Mixture of Experts)</strong>
      <p>æ··åˆä¸“å®¶æ¨¡å‹ã€‚æ¨¡å‹åŒ…å«å¤šä¸ª"ä¸“å®¶"å­ç½‘ç»œï¼Œæ¯æ¬¡æ¨ç†åªæ¿€æ´»å…¶ä¸­ä¸€éƒ¨åˆ†ï¼Œå®ç°å¤§å®¹é‡ä½æˆæœ¬ã€‚</p>
      <p class="term-en">A model architecture with multiple "expert" sub-networks, activating only a subset per inference for high capacity at low cost.</p>
    </div>
    <div class="term">
      <strong>FP8 / FP16 / FP32</strong>
      <p>æµ®ç‚¹æ•°ç²¾åº¦ã€‚FP8 ç”¨ 8 ä½è¡¨ç¤ºä¸€ä¸ªæ•°ï¼Œæ¯” FP16(16ä½)ã€FP32(32ä½) å ç”¨æ›´å°‘æ˜¾å­˜ï¼Œä½†ç²¾åº¦è¾ƒä½ã€‚</p>
      <p class="term-en">Floating point precision. FP8 uses 8 bits per numberâ€”less memory than FP16/FP32 but lower precision.</p>
    </div>
    <div class="term">
      <strong>Speculative Decoding</strong>
      <p>æ¨æµ‹è§£ç ã€‚ç”¨å°æ¨¡å‹å¿«é€Ÿ"çŒœ"å¤šä¸ª tokenï¼Œå†ç”¨å¤§æ¨¡å‹éªŒè¯ï¼ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</p>
      <p class="term-en">Use a small model to "guess" multiple tokens quickly, then verify with the large model to speed up inference.</p>
    </div>
    <div class="term">
      <strong>Load Balancing</strong>
      <p>è´Ÿè½½å‡è¡¡ã€‚ç¡®ä¿ MoE ä¸­å„ä¸ª Expert è¢«å‡åŒ€ä½¿ç”¨ï¼Œé¿å…"çƒ­é—¨ Expert"è¿‡è½½ã€‚</p>
      <p class="term-en">Ensuring even utilization across experts in MoE to avoid overloading popular experts.</p>
    </div>
  </div>

  <div class="highlight">
    <h3>ğŸ¯ å¯å­¦ä¹ çš„è®¾è®¡æ¨¡å¼ / Patterns to Learn</h3>
    <div class="bilingual">
      <div class="zh">
        <ol>
          <li><strong>ç¨€ç–æ¿€æ´»</strong>ï¼šä¸æ˜¯æ‰€æœ‰å‚æ•°éƒ½éœ€è¦æ¯æ¬¡éƒ½ç”¨ï¼ŒæŒ‰éœ€æ¿€æ´»å¯ä»¥å¤§å¹…é™ä½æˆæœ¬</li>
          <li><strong>ç²¾åº¦æ¢æ•ˆç‡</strong>ï¼šåœ¨ä¸å½±å“ç»“æœçš„å‰æä¸‹ï¼Œé™ä½è®¡ç®—ç²¾åº¦æ˜¯æ€§ä»·æ¯”æé«˜çš„ä¼˜åŒ–</li>
          <li><strong>å»æ‰ä¸å¿…è¦çš„ loss</strong>ï¼šå¾ˆå¤š"å¿…è¦"çš„æ­£åˆ™åŒ–å…¶å®å¯ä»¥ç”¨æ›´å¥½çš„æ¶æ„è®¾è®¡æ›¿ä»£</li>
          <li><strong>ä¸€çŸ³äºŒé¸Ÿ</strong>ï¼šMTP æ—¢æå‡è®­ç»ƒæ•ˆæœï¼Œåˆèƒ½åŠ é€Ÿæ¨ç†ï¼Œå¥½çš„è®¾è®¡å¾€å¾€æœ‰å¤šé‡æ”¶ç›Š</li>
        </ol>
      </div>
      <div class="en">
        <ol>
          <li><strong>Sparse activation</strong>: Not all params need to be used every time; activate on-demand</li>
          <li><strong>Precision for efficiency</strong>: Lower precision is a high-ROI optimization when results aren't affected</li>
          <li><strong>Remove unnecessary losses</strong>: "Required" regularization can often be replaced with better architecture</li>
          <li><strong>Kill two birds</strong>: MTP improves training AND speeds up inferenceâ€”good design has multiple payoffs</li>
        </ol>
      </div>
    </div>
  </div>

  <div class="follow-up">
    <h3>ğŸ’¬ åç»­è®¨è®º / Follow-up Discussion</h3>
    <p>æœ‰é—®é¢˜å¯ä»¥ç›´æ¥é—®æˆ‘ï¼Œè®¨è®ºåæˆ‘ä¼šæ›´æ–°è¿™ä¸ªæ–‡æ¡£ï¼</p>
    <p style="margin-top: 8px; font-size: 0.85em;">ä¾‹å¦‚ï¼šMoE çš„ Router å…·ä½“æ€ä¹ˆå·¥ä½œï¼ŸFP8 è®­ç»ƒä¸ºä»€ä¹ˆä¹‹å‰æ²¡äººåšæˆåŠŸï¼Ÿ</p>
  </div>
</body>
</html>
