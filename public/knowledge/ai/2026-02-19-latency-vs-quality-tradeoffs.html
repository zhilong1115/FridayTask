<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-19 - Latency vs Quality Tradeoffs in ML Systems</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.7; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 15px 0; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #1a73e8; border-bottom: 2px solid #1a73e8; padding-bottom: 10px; }
    h2 { color: #3c4043; margin-top: 35px; }
    h3 { color: #5f6368; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 13px; }
    code { font-family: 'SF Mono', 'Fira Code', monospace; }
    .highlight { background: #fff3cd; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-blue { background: #d1ecf1; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-green { background: #d4edda; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-red { background: #f8d7da; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .diagram { background: #f0f4f8; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: monospace; white-space: pre; font-size: 13px; line-height: 1.4; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    th, td { border: 1px solid #dadce0; padding: 10px; text-align: left; }
    th { background: #f8f9fa; font-weight: 600; }
    .interview { background: #e8f5e9; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #4caf50; }
    .series-nav { background: #e3f2fd; padding: 15px; border-radius: 8px; margin: 20px 0; }
    .tag { display: inline-block; padding: 2px 8px; border-radius: 4px; font-size: 12px; margin: 2px; }
    .tag-latency { background: #ffcdd2; color: #c62828; }
    .tag-quality { background: #c8e6c9; color: #2e7d32; }
    .tag-cost { background: #fff9c4; color: #f57f17; }
  </style>
</head>
<body>

<h1>âš–ï¸ Latency vs Quality Tradeoffs in ML Systems</h1>
<p>ğŸ“… 2026-02-19 | Phase 2.2 Day 5 | ML ç³»ç»Ÿè®¾è®¡è¦ç‚¹ç³»åˆ—</p>

<div class="series-nav">
  <strong>ğŸ“š Phase 2.2 ç³»åˆ—å¯¼èˆªï¼š</strong><br>
  Day 1: <a href="2026-02-15-ml-system-design-patterns.html">ML é€šç”¨æ¶æ„æ¨¡å¼ (7 Patterns)</a> |
  Day 2: <a href="2026-02-16-feature-store-design.html">Feature Store è®¾è®¡</a> |
  Day 3: <a href="2026-02-17-model-serving-architecture.html">Model Serving æ¶æ„</a> |
  Day 4: <a href="2026-02-18-ab-testing-ml-systems.html">A/B Testing for ML</a> |
  <strong>Day 5: Latency vs Quality Tradeoffs â† ä½ åœ¨è¿™é‡Œ</strong>
</div>

<h2>ğŸ“– æ ¸å¿ƒé—®é¢˜ / The Core Problem</h2>
<div class="bilingual">
  <div class="zh">
    <p>ML ç³»ç»Ÿè®¾è®¡ä¸­æœ€æ™®éçš„å¼ åŠ›ï¼š<strong>æ›´å¥½çš„æ¨¡å‹æ›´æ…¢ï¼Œæ›´å¿«çš„æ¨¡å‹æ›´å·®</strong>ã€‚</p>
    <p>è¿™ä¸ä»…æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæ›´æ˜¯äº§å“é—®é¢˜ã€‚ç”¨æˆ·ç­‰ 3 ç§’å¾—åˆ°å®Œç¾ç­”æ¡ˆï¼Œå’Œç­‰ 200ms å¾—åˆ° 90% æ­£ç¡®çš„ç­”æ¡ˆï¼Œå“ªä¸ªæ›´å¥½ï¼Ÿç­”æ¡ˆå–å†³äºåœºæ™¯ã€‚</p>
    <p>ä»Šå¤©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªç³»ç»ŸåŒ–çš„æ¡†æ¶æ¥åˆ†æå’Œåšå‡ºè¿™äº›æƒè¡¡ï¼ŒæŠŠä¹‹å‰ 4 å¤©çš„æ‰€æœ‰ç»„ä»¶ä¸²è”èµ·æ¥ã€‚</p>
  </div>
  <div class="en">
    <p>The most pervasive tension in ML system design: <strong>better models are slower, faster models are worse</strong>.</p>
    <p>This isn't just technical â€” it's a product decision. A perfect answer in 3 seconds vs. a 90% correct answer in 200ms â€” which is better? It depends on the use case.</p>
    <p>Today we build a systematic framework for analyzing and making these tradeoffs, connecting all components from the past 4 days.</p>
  </div>
</div>

<h2>ğŸ—ï¸ æƒè¡¡ä¸‰è§’ / The Tradeoff Triangle</h2>

<div class="diagram">
                    Quality (è´¨é‡)
                       /\
                      /  \
                     /    \
                    /  ML  \
                   / System \
                  /  Design  \
                 /____________\
          Latency              Cost
          (å»¶è¿Ÿ)               (æˆæœ¬)

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ä½ æœ€å¤šä¼˜åŒ–ä¸¤ä¸ªã€‚ç¬¬ä¸‰ä¸ªå¿…ç„¶å—æŸã€‚             â”‚
  â”‚  You can optimize at most two.                â”‚
  â”‚  The third will suffer.                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Real-time chat â†’ Latency + Quality â†’ ğŸ’° é«˜æˆæœ¬ (GPU fleet)
  Batch analysis â†’ Quality + Cost   â†’ â±ï¸ é«˜å»¶è¿Ÿ (overnight)
  Edge device   â†’ Latency + Cost   â†’ ğŸ“‰ è´¨é‡ä¸‹é™ (small model)
</div>

<h2>ğŸ”§ å…­å¤§æƒè¡¡ç­–ç•¥ / Six Tradeoff Strategies</h2>

<h3>ç­–ç•¥ 1: Model Cascade (æ¨¡å‹çº§è”)</h3>
<div class="bilingual">
  <div class="zh">
    <p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šå…ˆç”¨å°æ¨¡å‹ï¼Œä¸ç¡®å®šæ—¶å‡çº§åˆ°å¤§æ¨¡å‹ã€‚Google 2024 å¹´æå‡ºçš„ Speculative Cascade å°†æ­¤ä¸ Speculative Decoding ç»“åˆã€‚</p>
    <p><strong>é€‚ç”¨åœºæ™¯</strong>ï¼š70-80% çš„è¯·æ±‚å¯ä»¥è¢«å°æ¨¡å‹é«˜è´¨é‡å¤„ç†ã€‚</p>
  </div>
  <div class="en">
    <p><strong>Core idea</strong>: Start with a small model, escalate to a larger one when uncertain. Google's 2024 Speculative Cascade combines this with speculative decoding.</p>
    <p><strong>Best for</strong>: When 70-80% of requests can be handled well by a small model.</p>
  </div>
</div>

<div class="diagram">
  Request â†’ [Small Model (3B)] â”€â”€confidence > Î¸â”€â”€â†’ Response (P50: 50ms)
                   â”‚
                   â””â”€â”€confidence â‰¤ Î¸â”€â”€â†’ [Large Model (70B)] â†’ Response (P50: 800ms)

  æ•´ä½“æ•ˆæœ:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Approach   â”‚ Latency   â”‚ Quality  â”‚ Cost     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Always 70B â”‚ 800ms     â”‚ 95%      â”‚ $1.00    â”‚
  â”‚ Always 3B  â”‚ 50ms      â”‚ 82%      â”‚ $0.05    â”‚
  â”‚ Cascade    â”‚ ~250ms    â”‚ 93%      â”‚ $0.25    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>

<pre><code>class ModelCascade:
    def __init__(self, small_model, large_model, threshold=0.85):
        self.small = small_model
        self.large = large_model
        self.threshold = threshold
        self.router = ConfidenceEstimator()  # å¯ä»¥æ˜¯å°åˆ†ç±»å™¨æˆ– LLM è‡ªè¯„

    async def predict(self, request):
        # Stage 1: å°æ¨¡å‹å¿«é€Ÿæ¨ç†
        result = await self.small.generate(request)
        confidence = self.router.estimate(request, result)

        if confidence >= self.threshold:
            return result  # å¿«é€Ÿè¿”å› âš¡

        # Stage 2: å¤§æ¨¡å‹å…œåº•
        return await self.large.generate(request)

    def calibrate_threshold(self, eval_set):
        """åŸºäº A/B æµ‹è¯•æ•°æ®è°ƒæ•´é˜ˆå€¼
        - æé«˜ threshold â†’ æ›´å¤šèµ°å¤§æ¨¡å‹ â†’ æ›´é«˜è´¨é‡ã€æ›´é«˜å»¶è¿Ÿ
        - é™ä½ threshold â†’ æ›´å¤šèµ°å°æ¨¡å‹ â†’ æ›´ä½å»¶è¿Ÿã€å¯èƒ½æŸè´¨é‡
        """
        best_score = 0
        for t in np.arange(0.5, 0.99, 0.01):
            quality = self.eval_quality(eval_set, threshold=t)
            latency = self.eval_latency(eval_set, threshold=t)
            # Pareto æœ€ä¼˜ï¼šåœ¨ latency é¢„ç®—å†…æœ€å¤§åŒ– quality
            if latency <= self.latency_budget and quality > best_score:
                best_score = quality
                self.threshold = t</code></pre>

<div class="highlight">
  <strong>ğŸ’¡ é¢è¯•å…³é”®ç‚¹</strong>ï¼šCascade çš„ confidence estimation æœ¬èº«ä¸èƒ½å¤ªæ…¢ï¼Œå¦åˆ™æŠµæ¶ˆæ”¶ç›Šã€‚å¸¸ç”¨æ–¹æ³•ï¼š(1) LLM è¾“å‡º logprobs çš„ entropy (2) è½»é‡åˆ†ç±»å™¨ (3) åŸºäº query ç‰¹å¾çš„è§„åˆ™è·¯ç”±ã€‚
</div>

<h3>ç­–ç•¥ 2: Speculative Decoding (æŠ•æœºè§£ç )</h3>
<div class="bilingual">
  <div class="zh">
    <p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šå°æ¨¡å‹å…ˆ"çŒœ"å¤šä¸ª tokenï¼Œå¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯ã€‚è´¨é‡ç­‰åŒå¤§æ¨¡å‹ï¼Œå»¶è¿Ÿæ¥è¿‘å°æ¨¡å‹ã€‚</p>
    <p>è¿™æ˜¯å”¯ä¸€èƒ½åšåˆ° <strong>é›¶è´¨é‡æŸå¤±</strong> çš„åŠ é€Ÿæ–¹æ³•ï¼</p>
  </div>
  <div class="en">
    <p><strong>Core idea</strong>: Small model "drafts" multiple tokens, large model verifies in parallel. Quality equals the large model, latency approaches the small model.</p>
    <p>This is the only acceleration method with <strong>zero quality loss</strong>!</p>
  </div>
</div>

<div class="diagram">
  Draft Model (å°): "The capital of France is Pa-ri-s ,-"  (7 tokens, å¿«)
                                    â†“
  Target Model (å¤§): Verify â†’ âœ…âœ…âœ…âœ…âœ…âœ…âœ…  (å…¨éƒ¨æ¥å—ï¼1æ¬¡forward pass)
                            â†’ âœ…âœ…âœ…âŒ        (å‰3ä¸ªå¯¹ï¼Œç¬¬4ä¸ªé‡é‡‡æ ·)

  åŠ é€Ÿæ¯” = draft_length Ã— acceptance_rate / (draft_cost + verify_cost)
  å…¸å‹ç»“æœ: 2-3x speedup, 100% quality preservation
</div>

<pre><code>def speculative_decode(draft_model, target_model, prompt, gamma=5):
    """æŠ•æœºè§£ç æ ¸å¿ƒç®—æ³•"""
    output_tokens = []

    while not done:
        # 1. Draft: å°æ¨¡å‹è¿ç»­ç”Ÿæˆ gamma ä¸ª token
        draft_tokens, draft_probs = draft_model.generate(
            prompt + output_tokens, num_tokens=gamma
        )

        # 2. Verify: å¤§æ¨¡å‹ä¸€æ¬¡ forward pass éªŒè¯æ‰€æœ‰ draft tokens
        target_probs = target_model.get_probs(
            prompt + output_tokens + draft_tokens
        )

        # 3. Accept/Reject: é€ä¸ªæ£€æŸ¥
        accepted = 0
        for i in range(gamma):
            # å…³é”®ï¼šrejection sampling ä¿è¯åˆ†å¸ƒç­‰ä»·äº target model
            r = random.random()
            if r < min(1, target_probs[i] / draft_probs[i]):
                output_tokens.append(draft_tokens[i])
                accepted += 1
            else:
                # ä»ä¿®æ­£åˆ†å¸ƒé‡‡æ ·
                corrected = sample(max(0, target_probs[i] - draft_probs[i]))
                output_tokens.append(corrected)
                break

    return output_tokens  # æ•°å­¦ä¸Šç­‰ä»·äºç›´æ¥ä» target model é‡‡æ ·ï¼</code></pre>

<div class="highlight-blue">
  <strong>ğŸ”¬ ä¸ºä»€ä¹ˆèƒ½ä¿è¯è´¨é‡ï¼Ÿ</strong> Rejection sampling çš„æ•°å­¦æ€§è´¨ç¡®ä¿ï¼šå³ä½¿ draft model çš„é¢„æµ‹è¢«æ‹’ç»ï¼Œä¿®æ­£åçš„åˆ†å¸ƒä»ç„¶ç²¾ç¡®ç­‰äº target model çš„åˆ†å¸ƒã€‚è¿™ä¸æ˜¯è¿‘ä¼¼ â€” æ˜¯ç²¾ç¡®ç­‰ä»·ã€‚
</div>

<h3>ç­–ç•¥ 3: Quantization Ladder (é‡åŒ–é˜¶æ¢¯)</h3>
<div class="bilingual">
  <div class="zh">
    <p>åŒä¸€ä¸ªæ¨¡å‹ï¼Œä¸åŒç²¾åº¦ = ä¸åŒçš„å»¶è¿Ÿ-è´¨é‡ç‚¹ã€‚å½¢æˆä¸€ä¸ªå¯ä»¥åŠ¨æ€é€‰æ‹©çš„"é˜¶æ¢¯"ã€‚</p>
  </div>
  <div class="en">
    <p>Same model, different precisions = different latency-quality points. Forms a "ladder" you can dynamically select from.</p>
  </div>
</div>

<table>
  <tr><th>ç²¾åº¦</th><th>æ¨¡å‹å¤§å°</th><th>å»¶è¿Ÿ (ç›¸å¯¹)</th><th>è´¨é‡æŸå¤±</th><th>é€‚ç”¨åœºæ™¯</th></tr>
  <tr><td>FP32</td><td>100%</td><td>1.0x</td><td>0%</td><td>è®­ç»ƒ / åŸºå‡†</td></tr>
  <tr><td>FP16/BF16</td><td>50%</td><td>~0.5x</td><td>~0%</td><td>æ ‡å‡†æ¨ç†</td></tr>
  <tr><td>INT8 (W8A8)</td><td>25%</td><td>~0.3x</td><td>0.5-1%</td><td>é«˜ååæ¨ç†</td></tr>
  <tr><td>INT4 (GPTQ/AWQ)</td><td>12.5%</td><td>~0.2x</td><td>1-3%</td><td>è¾¹ç¼˜éƒ¨ç½² / æˆæœ¬æ•æ„Ÿ</td></tr>
  <tr><td>INT2 (å®éªŒ)</td><td>6.25%</td><td>~0.15x</td><td>5-15%</td><td>æç«¯å—é™åœºæ™¯</td></tr>
</table>

<pre><code>class QuantizationLadder:
    """æ ¹æ®è´Ÿè½½åŠ¨æ€é€‰æ‹©é‡åŒ–çº§åˆ«"""

    def __init__(self):
        self.models = {
            'fp16':  ModelEndpoint('model-fp16',  latency_p99=200),
            'int8':  ModelEndpoint('model-int8',  latency_p99=80),
            'int4':  ModelEndpoint('model-int4',  latency_p99=40),
        }
        self.current_load = LoadMonitor()

    def select_precision(self, request):
        load = self.current_load.get()
        priority = request.priority  # premium / standard / batch

        if priority == 'premium':
            return 'fp16'  # æ°¸è¿œç»™æœ€å¥½çš„
        elif load > 0.8:  # é«˜è´Ÿè½½
            return 'int4'  # é™çº§ä»¥ä¿æŠ¤å»¶è¿Ÿ SLA
        else:
            return 'int8'  # é»˜è®¤å¹³è¡¡ç‚¹</code></pre>

<h3>ç­–ç•¥ 4: Progressive Generation (æ¸è¿›å¼ç”Ÿæˆ)</h3>
<div class="bilingual">
  <div class="zh">
    <p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šå…ˆå¿«é€Ÿè¿”å›ç²—ç³™ç»“æœï¼Œåå°å¼‚æ­¥ç²¾åŒ–ã€‚ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿæä½ã€‚</p>
    <p>ç±»ä¼¼ Google æœç´¢ï¼šå…ˆå‡ºæ–‡å­—ç»“æœï¼ŒKnowledge Panel ç¨ååŠ è½½ã€‚</p>
  </div>
  <div class="en">
    <p><strong>Core idea</strong>: Return a rough result fast, refine asynchronously in background. Perceived latency is very low.</p>
    <p>Like Google Search: text results appear first, Knowledge Panel loads later.</p>
  </div>
</div>

<div class="diagram">
  t=0ms    [Cache]      â†’ æœ‰ç¼“å­˜? ç›´æ¥è¿”å› (P50: 5ms)
  t=50ms   [Small LLM]  â†’ å¿«é€Ÿè‰ç¨¿å›å¤ (streaming å¼€å§‹)
  t=200ms  [Retrieval]   â†’ RAG æ£€ç´¢ç»“æœåˆ°è¾¾, æ³¨å…¥ context
  t=500ms  [Large LLM]   â†’ ç²¾åŒ–å›å¤ (æ›¿æ¢/è¡¥å…… streaming å†…å®¹)
  t=800ms  [Verification] â†’ äº‹å®æ£€æŸ¥, æ·»åŠ å¼•ç”¨

  ç”¨æˆ·æ„ŸçŸ¥: 50ms å¼€å§‹çœ‹åˆ°å†…å®¹ (TTFT = 50ms!)
  å®é™…è´¨é‡: ç­‰ä»·äº 800ms çš„å®Œæ•´ pipeline
</div>

<pre><code>async def progressive_response(query, stream):
    # Phase 1: å³æ—¶ â€” ç¼“å­˜å‘½ä¸­ (5ms)
    cached = await semantic_cache.lookup(query)
    if cached:
        await stream.send(cached, final=True)
        return

    # Phase 2: å¿«é€Ÿè‰ç¨¿ (50ms)
    draft_task = asyncio.create_task(
        small_model.generate(query, max_tokens=100)
    )

    # Phase 3: å¹¶è¡Œå¯åŠ¨æ…¢æ“ä½œ
    retrieve_task = asyncio.create_task(retriever.search(query))
    
    # å…ˆå‘é€è‰ç¨¿
    draft = await draft_task
    await stream.send(draft, final=False)  # ç”¨æˆ·å¼€å§‹çœ‹åˆ°å†…å®¹!

    # Phase 4: ç”¨ RAG ç»“æœç²¾åŒ–
    docs = await retrieve_task
    refined = await large_model.generate(
        query, context=docs, draft=draft, max_tokens=500
    )
    await stream.replace(refined, final=True)  # å¹³æ»‘æ›¿æ¢</code></pre>

<h3>ç­–ç•¥ 5: Adaptive Compute (è‡ªé€‚åº”è®¡ç®—)</h3>
<div class="bilingual">
  <div class="zh">
    <p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šæ ¹æ®è¾“å…¥éš¾åº¦åŠ¨æ€è°ƒæ•´è®¡ç®—é‡ã€‚ç®€å•é—®é¢˜å°‘ç®—ï¼Œéš¾é¢˜å¤šç®—ã€‚</p>
    <p>DeepSeek-V3 çš„ MoE å°±æ˜¯è¿™ä¸ªæ€æƒ³ â€” æ¯ä¸ª token åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ã€‚</p>
  </div>
  <div class="en">
    <p><strong>Core idea</strong>: Dynamically adjust compute based on input difficulty. Easy inputs get less computation, hard ones get more.</p>
    <p>DeepSeek-V3's MoE embodies this â€” each token activates only a subset of experts.</p>
  </div>
</div>

<table>
  <tr><th>æŠ€æœ¯</th><th>ç²’åº¦</th><th>Quality ä¿è¯</th><th>Latency å‡å°‘</th></tr>
  <tr><td>MoE (Mixture of Experts)</td><td>Token-level</td><td>è®­ç»ƒæ—¶å­¦ä¹ è·¯ç”±</td><td>2-4x (vs Dense)</td></tr>
  <tr><td>Early Exit</td><td>Layer-level</td><td>ä¸­é—´å±‚ç½®ä¿¡åº¦æ£€æµ‹</td><td>1.5-3x</td></tr>
  <tr><td>Token Pruning</td><td>Token-level</td><td>è·³è¿‡ä¸é‡è¦ token</td><td>1.2-2x</td></tr>
  <tr><td>Adaptive Depth (æ€è€ƒé¢„ç®—)</td><td>Request-level</td><td>æ ¹æ®éš¾åº¦åˆ†é… CoT é•¿åº¦</td><td>Variable</td></tr>
</table>

<pre><code>class AdaptiveThinkingBudget:
    """æ ¹æ®é—®é¢˜éš¾åº¦åˆ†é… thinking token é¢„ç®—
    è¿æ¥åˆ°: Day 1 çš„ Model Routing pattern
    """

    DIFFICULTY_BUDGETS = {
        'trivial':  {'thinking_tokens': 0,    'model': 'small'},    # "What's 2+2?"
        'simple':   {'thinking_tokens': 100,  'model': 'small'},    # Simple lookup
        'moderate': {'thinking_tokens': 500,  'model': 'medium'},   # Reasoning needed
        'complex':  {'thinking_tokens': 2000, 'model': 'large'},    # Multi-step
        'expert':   {'thinking_tokens': 8000, 'model': 'frontier'}, # Research-level
    }

    def classify_and_route(self, query):
        # ç”¨è½»é‡æ¨¡å‹ (~10ms) ä¼°ç®—éš¾åº¦
        difficulty = self.difficulty_classifier.predict(query)
        budget = self.DIFFICULTY_BUDGETS[difficulty]
        return budget</code></pre>

<div class="highlight-green">
  <strong>ğŸ¯ è¡Œä¸šè¶‹åŠ¿ (2025-2026)</strong>ï¼šClaudeã€GPTã€Gemini éƒ½åœ¨æ¨ "thinking budget" æ¦‚å¿µ â€” è®©ç”¨æˆ·æˆ–ç³»ç»Ÿæ§åˆ¶æ¨ç†æ·±åº¦ã€‚è¿™æœ¬è´¨å°±æ˜¯ Adaptive Compute çš„äº§å“åŒ–ã€‚
</div>

<h3>ç­–ç•¥ 6: Caching Hierarchy (ç¼“å­˜å±‚çº§)</h3>
<div class="bilingual">
  <div class="zh">
    <p>æœ€å¥½çš„å»¶è¿Ÿä¼˜åŒ–æ˜¯ä¸è®¡ç®—ã€‚ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼ˆDay 4 RAG Production å·²è®²ï¼‰ï¼Œè¿™é‡Œè¡¥å…… LLM ç‰¹æœ‰çš„ç¼“å­˜ç­–ç•¥ã€‚</p>
  </div>
  <div class="en">
    <p>The best latency optimization is no computation at all. Three-layer caching (covered in RAG Day 4), here we add LLM-specific caching strategies.</p>
  </div>
</div>

<div class="diagram">
  Layer 1: Exact Match Cache     â†’ Hash(prompt) lookup      â†’ Hit rate: 5-15%  â†’ 1ms
  Layer 2: Semantic Cache         â†’ Embedding similarity     â†’ Hit rate: 15-30% â†’ 10ms
  Layer 3: KV Cache (Prefix)     â†’ Shared system prompt KV  â†’ Savings: 30-50%  â†’ modelå†…éƒ¨
  Layer 4: Prompt Cache (Anthropic/OpenAI) â†’ Provider-level â†’ Savings: 50-90%  â†’ æŒ‰tokenè®¡

  ç»„åˆæ•ˆæœ: 40-60% çš„è¯·æ±‚å¯ä»¥æ˜¾è‘—åŠ é€Ÿæˆ–å®Œå…¨è·³è¿‡æ¨ç†
</div>

<h2>ğŸ¯ é¢è¯•æ¡†æ¶ï¼šå¦‚ä½•å›ç­” Latency vs Quality é—®é¢˜</h2>

<div class="architecture">
<h3>äº”æ­¥æ¡†æ¶ / Five-Step Framework</h3>

<p><strong>Step 1: Clarify SLA (æ˜ç¡® SLA)</strong></p>
<ul>
  <li>P50 / P99 latency target?</li>
  <li>Minimum acceptable quality metric?</li>
  <li>Cost budget per request?</li>
</ul>

<p><strong>Step 2: Profile the Pipeline (åˆ†æç“¶é¢ˆ)</strong></p>
<pre>Total Latency = Network + Preprocessing + Retrieval + Inference + Postprocessing
                  ~10ms     ~20ms           ~50ms       ~500ms     ~30ms
                                                        ^^^^^^ é€šå¸¸æ˜¯ç“¶é¢ˆ</pre>

<p><strong>Step 3: Match Strategy to Constraint (åŒ¹é…ç­–ç•¥)</strong></p>
<table>
  <tr><th>çº¦æŸæ¡ä»¶</th><th>é¦–é€‰ç­–ç•¥</th><th>å¤‡é€‰ç­–ç•¥</th></tr>
  <tr><td>å»¶è¿Ÿå¿…é¡» < 100ms</td><td>Small model + Cache</td><td>Quantization</td></tr>
  <tr><td>è´¨é‡ä¸èƒ½é™</td><td>Speculative Decoding</td><td>KV Cache ä¼˜åŒ–</td></tr>
  <tr><td>æˆæœ¬å¿…é¡»é™ 50%</td><td>Model Cascade</td><td>Adaptive Compute</td></tr>
  <tr><td>ç”¨æˆ·ä½“éªŒä¼˜å…ˆ</td><td>Progressive Generation</td><td>Streaming + Cache</td></tr>
  <tr><td>æµé‡çªå‘ 10x</td><td>Quantization Ladder</td><td>Graceful Degradation</td></tr>
</table>

<p><strong>Step 4: Design Feedback Loop (è®¾è®¡åé¦ˆå¾ªç¯)</strong></p>
<ul>
  <li>A/B Testing (Day 4) éªŒè¯æƒè¡¡æ•ˆæœ</li>
  <li>ç›‘æ§ quality metrics ç¡®ä¿ä¸ regress</li>
  <li>åŠ¨æ€è°ƒæ•´ threshold / budget</li>
</ul>

<p><strong>Step 5: Discuss Failure Modes (è®¨è®ºå¤±è´¥æ¨¡å¼)</strong></p>
<ul>
  <li>Cascade çš„ confidence ä¼°é”™ â†’ quality çªé™</li>
  <li>Cache è¿‡æœŸ â†’ è¿”å›è¿‡æ—¶ä¿¡æ¯</li>
  <li>Quantization åœ¨ç‰¹å®š domain è´¨é‡å´©å¡Œ</li>
</ul>
</div>

<h2>ğŸ”— ç³»åˆ—æ€»ç»“ï¼šPhase 2.2 å…¨æ™¯å›¾</h2>

<div class="diagram">
  Day 1: 7 å¤§é€šç”¨ Pattern â†â†’ "è®¾è®¡è¯æ±‡è¡¨"
    â†“
  Day 2: Feature Store â†â†’ "æ•°æ®åœ°åŸº"
    â†“
  Day 3: Model Serving â†â†’ "æ¨ç†å¼•æ“"
    â†“
  Day 4: A/B Testing â†â†’ "éªŒè¯é—­ç¯"
    â†“
  Day 5: Latency vs Quality â†â†’ "æƒè¡¡å†³ç­–" (ä½ åœ¨è¿™é‡Œ!)

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  å®Œæ•´ ML ç³»ç»Ÿ = Feature Store â†’ Model Serving       â”‚
  â”‚    â†’ Latency/Quality æƒè¡¡ â†’ A/B Testing éªŒè¯       â”‚
  â”‚    â†’ æ‰€æœ‰ç»„ä»¶éµå¾ª 7 å¤§ Pattern                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>

<h2>ğŸ’¬ é¢è¯•é«˜é¢‘é—®é¢˜</h2>

<div class="interview">
  <h4>Q1: ä½ çš„ LLM åº”ç”¨ P99 å»¶è¿Ÿæ˜¯ 3 ç§’ï¼Œäº§å“è¦æ±‚é™åˆ° 500msï¼Œæ€ä¹ˆåšï¼Ÿ</h4>
  <p><strong>æ€è·¯</strong>ï¼š(1) Profile æ‰¾ç“¶é¢ˆ â€” é€šå¸¸æ˜¯ inference (2) å…ˆåŠ  semantic cache è¦†ç›–é«˜é¢‘ query (3) å®ç° model cascadeï¼Œ70% è¯·æ±‚èµ°å°æ¨¡å‹ (4) streaming é™ä½ TTFT (5) KV cache / prefix caching ä¼˜åŒ–ç³»ç»Ÿ prompt (6) å¦‚æœè¿˜ä¸å¤Ÿï¼Œquantizationã€‚ç›‘æ§ quality regressionï¼Œç”¨ A/B testing éªŒè¯æ¯æ­¥ã€‚</p>
</div>

<div class="interview">
  <h4>Q2: Speculative Decoding ä»€ä¹ˆæ—¶å€™ä¸ workï¼Ÿ</h4>
  <p><strong>ç­”</strong>ï¼š(1) Draft model å’Œ target model åˆ†å¸ƒå·®å¼‚å¤ªå¤§ â†’ acceptance rate ä½ â†’ æ²¡æœ‰åŠ é€Ÿ (2) Target model æœ¬èº«å°±å¾ˆå¿«ï¼ˆå°æ¨¡å‹ï¼‰â†’ draft overhead æŠµæ¶ˆæ”¶ç›Š (3) éœ€è¦ structured output (JSON) â†’ draft å®¹æ˜“åç¦»æ ¼å¼ (4) Batch åœºæ™¯ â†’ GPU åˆ©ç”¨ç‡å·²ç»å¾ˆé«˜ï¼Œspeculative åè€Œå¢åŠ æ˜¾å­˜å‹åŠ›ã€‚</p>
</div>

<div class="interview">
  <h4>Q3: å¦‚ä½•åœ¨ä¸é™è´¨é‡çš„æƒ…å†µä¸‹å‡å°‘ 50% æ¨ç†æˆæœ¬ï¼Ÿ</h4>
  <p><strong>ç­”</strong>ï¼šç»„åˆæ‹³ï¼š(1) Semantic cache â†’ 30% è¯·æ±‚ç›´æ¥å‘½ä¸­ (2) Prefix/KV caching â†’ å‡å°‘ 50% é‡å¤è®¡ç®—ï¼ˆshared system promptï¼‰(3) Speculative decoding â†’ 2x throughputï¼Œè´¨é‡ä¸å˜ (4) Prompt compression â†’ å‡å°‘ input token (5) Batch optimization â†’ æé«˜ GPU åˆ©ç”¨ç‡ã€‚è¿™äº›æ–¹æ³•å‡ä¸æŸè´¨é‡ï¼Œç»„åˆå¯è¾¾ 50-70% æˆæœ¬é™ä½ã€‚</p>
</div>

<div class="interview">
  <h4>Q4: è®¾è®¡ä¸€ä¸ªæ”¯æŒ "è´¨é‡æ¡£ä½" çš„ APIï¼Œè®©å®¢æˆ·è‡ªé€‰ latency-quality å¹³è¡¡ç‚¹</h4>
  <p><strong>ç­”</strong>ï¼šä¸‰æ¡£è®¾è®¡ï¼š<br>
  <code>tier: "instant"</code> â†’ INT4 å°æ¨¡å‹ + cacheï¼ŒP99 < 100msï¼Œè´¨é‡ 85%<br>
  <code>tier: "balanced"</code> â†’ INT8 ä¸­ç­‰æ¨¡å‹ + cascadeï¼ŒP99 < 500msï¼Œè´¨é‡ 92%<br>
  <code>tier: "best"</code> â†’ FP16 å¤§æ¨¡å‹ + verificationï¼ŒP99 < 3sï¼Œè´¨é‡ 97%<br>
  æ¯æ¡£æœ‰ä¸åŒå®šä»·ã€‚å†…éƒ¨ç”¨ Model Router + Quantization Ladder + A/B Testing æŒç»­ä¼˜åŒ–æ¯æ¡£çš„ Pareto æœ€ä¼˜é…ç½®ã€‚</p>
</div>

<h2>ğŸ“ Phase 2.2 å®Œç»“å›é¡¾</h2>

<div class="highlight-green">
  <strong>ğŸ‰ Phase 2.2 ML ç³»ç»Ÿè®¾è®¡è¦ç‚¹ â€” å®Œæˆ!</strong><br><br>
  5 å¤©è¦†ç›–äº† ML ç³»ç»Ÿä»æ•°æ®åˆ°éƒ¨ç½²åˆ°éªŒè¯åˆ°ä¼˜åŒ–çš„å…¨é“¾è·¯ï¼š
  <ol>
    <li><strong>é€šç”¨ Pattern</strong> â€” 7 ä¸ªè·¨ç³»ç»Ÿå¤ç”¨çš„è®¾è®¡æ¨¡å¼</li>
    <li><strong>Feature Store</strong> â€” Training-Serving Skew çš„æ ¹æ²»æ–¹æ¡ˆ</li>
    <li><strong>Model Serving</strong> â€” ä»æ¨ç†å¼•æ“åˆ° GPU-Aware Scaling</li>
    <li><strong>A/B Testing</strong> â€” ç§‘å­¦éªŒè¯ ML æ”¹åŠ¨æ•ˆæœ</li>
    <li><strong>Latency vs Quality</strong> â€” è´¯ç©¿æ‰€æœ‰ç¯èŠ‚çš„æ ¸å¿ƒæƒè¡¡</li>
  </ol>
  <br>
  <strong>ä¸‹ä¸€é˜¶æ®µé¢„å‘Š</strong>ï¼šPhase 2.3 LLM ç‰¹æœ‰é—®é¢˜ â€” Token é™åˆ¶ã€æˆæœ¬ä¼˜åŒ–ã€Hallucinationã€æµå¼è¾“å‡ºã€å¤šè½®å¯¹è¯ã€‚æ›´åŠ è´´è¿‘æ—¥å¸¸ LLM åº”ç”¨å¼€å‘ï¼
</div>

<div class="follow-up">
  <h3>ğŸ’¬ æ€è€ƒé¢˜ / Thought Question</h3>
  <p>å‡è®¾ä½ åœ¨è®¾è®¡ä¸€ä¸ª AI å®¢æœç³»ç»Ÿï¼šé«˜å³°æœŸ QPS æ˜¯å¹³æ—¶çš„ 10 å€ï¼Œä½†è´¨é‡ä¸èƒ½æ˜æ˜¾ä¸‹é™ï¼ˆå®¢æˆ·æŠ•è¯‰ â†’ ç›´æ¥ä¸¢å•ï¼‰ã€‚ä½ ä¼šç»„åˆå“ªå‡ ç§ç­–ç•¥ï¼Ÿå¦‚ä½•è®¾è®¡è‡ªåŠ¨é™çº§æœºåˆ¶ï¼Ÿé™çº§åå¦‚ä½•è‡ªåŠ¨æ¢å¤ï¼Ÿ</p>
</div>

</body>
</html>
