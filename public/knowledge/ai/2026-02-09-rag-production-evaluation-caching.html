<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-09 - Production RAG: Evaluation, Caching & Cost Optimization</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.7; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 15px 0; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; border-bottom: 2px solid #f9ab00; padding-bottom: 10px; }
    h2 { color: #1a73e8; margin-top: 35px; }
    h3 { color: #5f6368; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; font-family: monospace; white-space: pre; overflow-x: auto; font-size: 13px; line-height: 1.4; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 13px; }
    code { font-family: 'SF Mono', Monaco, monospace; }
    .highlight { background: #fff3cd; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-blue { background: #d1ecf1; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-green { background: #d4edda; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .interview { background: #f8d7da; padding: 15px; border-radius: 8px; margin: 20px 0; border-left: 4px solid #dc3545; }
    .interview h4 { color: #dc3545; margin-top: 0; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    th, td { padding: 8px 12px; border: 1px solid #dadce0; text-align: left; }
    th { background: #f8f9fa; }
    .series-nav { background: #e8f0fe; padding: 12px; border-radius: 8px; margin: 15px 0; }
    .metric-card { border: 1px solid #dadce0; border-radius: 8px; padding: 15px; margin: 10px 0; }
    .metric-card h4 { margin-top: 0; color: #1a73e8; }
  </style>
</head>
<body>

<div class="series-nav">
  ğŸ“š <strong>RAG ç³»åˆ—</strong>: 
  <a href="2026-02-05-rag-pipeline-architecture.html">Day 1: Pipeline æ¶æ„</a> â†’ 
  <a href="2026-02-07-rag-advanced-chunking.html">Day 2: Chunking ç­–ç•¥</a> â†’ 
  <a href="2026-02-08-rag-advanced-retrieval.html">Day 3: æ£€ç´¢ç­–ç•¥</a> â†’ 
  <strong>Day 4: ç”Ÿäº§åŒ– (æœ¬ç¯‡) ğŸ¯</strong>
</div>

<h1>ğŸ­ Production RAG: Evaluation, Caching & Cost Optimization</h1>
<p>RAG ç³»åˆ—å®Œç»“ç¯‡ | ğŸ“… 2026-02-09</p>

<h2>ğŸ“– æ¦‚è§ˆ / Overview</h2>
<div class="bilingual">
  <div class="zh">
    <p>å‰ä¸‰ç¯‡æˆ‘ä»¬æå®šäº† RAG çš„æ ¸å¿ƒæŠ€æœ¯ï¼šPipeline æ¶æ„ã€Chunking ç­–ç•¥ã€æ£€ç´¢ä¼˜åŒ–ã€‚ä½†æŠ€æœ¯èƒ½è·‘ â‰  èƒ½ä¸Šçº¿ã€‚</p>
    <p>ä»Šå¤©è®² <strong>ç”Ÿäº§åŒ–</strong>ï¼šæ€ä¹ˆçŸ¥é“ä½ çš„ RAG å¥½ä¸å¥½ï¼Ÿæ€ä¹ˆæ§æˆæœ¬ï¼Ÿæ€ä¹ˆè®©å®ƒåˆå¿«åˆä¾¿å®œï¼Ÿè¿™æ‰æ˜¯é¢è¯•å’Œå®é™…å·¥ä½œä¸­æœ€å…³é”®çš„éƒ¨åˆ†ã€‚</p>
  </div>
  <div class="en">
    <p>We've covered RAG core tech across 3 days. But working in a notebook â‰  production-ready.</p>
    <p>Today: <strong>Evaluation</strong> (how do you know it's good?), <strong>Caching</strong> (how to make it fast?), and <strong>Cost Optimization</strong> (how to keep the bill sane?).</p>
  </div>
</div>

<h2>ğŸ—ï¸ ç”Ÿäº§ RAG æ¶æ„å…¨æ™¯ / Production Architecture</h2>

<div class="architecture">
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚              Production RAG                  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                â”‚                                â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ EVALUATE â”‚                     â”‚   CACHE    â”‚                   â”‚ OPTIMIZE  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                                â”‚                                â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚      â”‚      â”‚              â”‚          â”‚          â”‚          â”‚         â”‚         â”‚
Retrieval Gen  E2E      Semantic    Exact-Match  Result    Model      Token     Async
Quality  Quality       Cache       Cache       Cache    Routing    Budget   Processing
  â”‚      â”‚      â”‚         â”‚          â”‚          â”‚          â”‚         â”‚         â”‚
Context Faithful RAGAS  Embedding   Query     LLM      Bigâ†’Small  Prompt   Batch
Precision -ness  Score  Similarity  Hash     Response   Cascade   Compress  Queue
</div>

<h2>ğŸ“Š Part 1: RAG è¯„ä¼°ä½“ç³» / Evaluation Framework</h2>

<div class="bilingual">
  <div class="zh">
    <p>RAG è¯„ä¼°çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š<strong>æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ</strong>ã€‚ä¸åƒåˆ†ç±»ä»»åŠ¡æœ‰ accuracyï¼ŒRAG çš„"å¥½ä¸å¥½"å¾ˆä¸»è§‚ã€‚ä¸šç•Œè§£æ³•æ˜¯ç”¨ LLM å½“è¯„å§” (LLM-as-Judge)ã€‚</p>
  </div>
  <div class="en">
    <p>Core challenge: <strong>no ground truth</strong> for most queries. Solution: LLM-as-Judge â€” use another LLM to evaluate the RAG output programmatically.</p>
  </div>
</div>

<h3>ğŸ¯ RAGAS æ¡†æ¶ â€” è¡Œä¸šæ ‡å‡†è¯„ä¼°</h3>

<div class="highlight">
  <strong>RAGAS</strong> (Retrieval-Augmented Generation Assessment) æ˜¯ç›®å‰æœ€ä¸»æµçš„ RAG è¯„ä¼°æ¡†æ¶ã€‚æ ¸å¿ƒæ€è·¯ï¼šæŠŠ RAG çš„å¥½åæ‹†è§£æˆç‹¬ç«‹å¯æµ‹çš„ç»´åº¦ã€‚
</div>

<h4>å››å¤§æ ¸å¿ƒæŒ‡æ ‡</h4>

<div class="metric-card">
  <h4>1. Faithfulness (å¿ å®åº¦) â€” æœ€é‡è¦ï¼</h4>
  <p><strong>é—®é¢˜</strong>: ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Ÿè¿˜æ˜¯åœ¨ç¼–ï¼Ÿ</p>
  <p><strong>è®¡ç®—æ–¹æ³•</strong>:</p>
  <ol>
    <li>LLM æŠŠ answer æ‹†æˆå¤šä¸ª claimï¼ˆåŸå­å£°æ˜ï¼‰</li>
    <li>é€ä¸ªæ£€æŸ¥æ¯ä¸ª claim æ˜¯å¦èƒ½åœ¨ context ä¸­æ‰¾åˆ°æ”¯æŒ</li>
    <li>Faithfulness = æœ‰æ”¯æŒçš„ claims / æ€» claims</li>
  </ol>
  <p><strong>èŒƒå›´</strong>: 0~1ï¼Œè¶Šé«˜è¶Šå¥½ã€‚ä½äº 0.7 è¯´æ˜æœ‰ä¸¥é‡å¹»è§‰é—®é¢˜ã€‚</p>
</div>

<div class="metric-card">
  <h4>2. Answer Relevancy (ç­”æ¡ˆç›¸å…³æ€§)</h4>
  <p><strong>é—®é¢˜</strong>: ç­”æ¡ˆæ˜¯å¦çœŸçš„åœ¨å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ</p>
  <p><strong>è®¡ç®—æ–¹æ³•</strong>: è®© LLM æ ¹æ® answer åå‘ç”Ÿæˆé—®é¢˜ï¼Œè®¡ç®—ç”Ÿæˆçš„é—®é¢˜å’ŒåŸå§‹é—®é¢˜çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚å¦‚æœç­”æ¡ˆè·‘é¢˜äº†ï¼Œåå‘ç”Ÿæˆçš„é—®é¢˜ä¼šå’ŒåŸé—®é¢˜å¾ˆä¸åŒã€‚</p>
</div>

<div class="metric-card">
  <h4>3. Context Precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦)</h4>
  <p><strong>é—®é¢˜</strong>: æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­ï¼ŒçœŸæ­£æœ‰ç”¨çš„æ’åœ¨å‰é¢äº†å—ï¼Ÿ</p>
  <p><strong>è®¡ç®—æ–¹æ³•</strong>: ç±»ä¼¼ Precision@Kï¼Œæ£€æŸ¥ top-K æ£€ç´¢ç»“æœä¸­ï¼Œç›¸å…³æ–‡æ¡£çš„æ’åä½ç½®ã€‚ç›¸å…³æ–‡æ¡£æ’å¾—è¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜ã€‚</p>
</div>

<div class="metric-card">
  <h4>4. Context Recall (ä¸Šä¸‹æ–‡å¬å›ç‡)</h4>
  <p><strong>é—®é¢˜</strong>: å›ç­”é—®é¢˜éœ€è¦çš„ä¿¡æ¯ï¼Œéƒ½æ£€ç´¢åˆ°äº†å—ï¼Ÿ</p>
  <p><strong>è®¡ç®—æ–¹æ³•</strong>: éœ€è¦ ground truth answerã€‚æŠŠ ground truth æ‹†æˆ claimsï¼Œæ£€æŸ¥æœ‰å¤šå°‘èƒ½åœ¨æ£€ç´¢ç»“æœä¸­æ‰¾åˆ°æ”¯æŒã€‚</p>
</div>

<pre><code class="language-python"># RAGAS ä½¿ç”¨ç¤ºä¾‹
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®
eval_data = {
    "question": ["What is RAG?", "How does chunking work?"],
    "answer": ["RAG combines retrieval with generation...", "Chunking splits documents..."],
    "contexts": [["RAG is a technique that..."], ["Document chunking involves..."]],
    "ground_truth": ["RAG retrieves relevant documents...", "Chunking divides text..."]
}

dataset = Dataset.from_dict(eval_data)

# è¿è¡Œè¯„ä¼°
results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
)
print(results)
# {'faithfulness': 0.85, 'answer_relevancy': 0.92, 
#  'context_precision': 0.78, 'context_recall': 0.88}</code></pre>

<div class="highlight">
  <strong>ğŸ’¡ é¢è¯•å…³é”®ç‚¹</strong>: è¢«é—®"æ€ä¹ˆè¯„ä¼° RAG"æ—¶ï¼Œå…ˆè¯´è¿™å››ä¸ªæŒ‡æ ‡ï¼Œç„¶åè§£é‡Š <strong>Faithfulness æœ€é‡è¦</strong>ï¼ˆå› ä¸ºå¹»è§‰æ˜¯ RAG æœ€å¤§çš„é£é™©ï¼‰ï¼Œå†æåˆ°è¯„ä¼°æœ¬èº«ä¹Ÿä¼šå¼•å…¥ LLM è¯¯å·® â†’ éœ€è¦äººå·¥æŠ½æ ·æ ¡éªŒã€‚
</div>

<h3>ğŸ“ˆ è¯„ä¼°æµæ°´çº¿è®¾è®¡</h3>

<div class="architecture">
  Offline Evaluation Pipeline:
  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Test Set  â”‚â”€â”€â”€â–¶â”‚ RAG Query â”‚â”€â”€â”€â–¶â”‚  RAGAS   â”‚â”€â”€â”€â–¶â”‚ Dashboard â”‚
  â”‚ (Q+A+Ctx) â”‚    â”‚  Pipeline  â”‚    â”‚ Evaluate  â”‚    â”‚ & Alerts  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                                    â”‚
       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Human Review â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚  (sample 5%) â”‚     Low score â†’ flag
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Online Monitoring:
  
  User Query â”€â”€â–¶ RAG â”€â”€â–¶ Response â”€â”€â–¶ [Lightweight checks]
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚         â”‚         â”‚
                          Latency   Token     User
                          P50/P99   Count    Thumbs
</div>

<div class="bilingual">
  <div class="zh">
    <p><strong>ç¦»çº¿è¯„ä¼°</strong>ï¼šå®šæœŸè·‘æµ‹è¯•é›†ï¼Œå…¨é¢æ£€æŸ¥è´¨é‡ã€‚é€‚åˆç‰ˆæœ¬å‘å¸ƒå‰ã€‚</p>
    <p><strong>åœ¨çº¿ç›‘æ§</strong>ï¼šè½»é‡çº§æŒ‡æ ‡ï¼ˆå»¶è¿Ÿã€token ç”¨é‡ã€ç”¨æˆ·åé¦ˆï¼‰ï¼Œå®æ—¶å‘ç°é—®é¢˜ã€‚ä¸è·‘ LLM-as-Judgeï¼ˆå¤ªè´µå¤ªæ…¢ï¼‰ã€‚</p>
    <p><strong>äººå·¥æ ¡éªŒ</strong>ï¼šæŠ½æ · 5% åš human reviewï¼Œæ ¡æ­£è‡ªåŠ¨åŒ–æŒ‡æ ‡çš„åå·®ã€‚</p>
  </div>
  <div class="en">
    <p><strong>Offline</strong>: Full RAGAS suite on test set. Run before releases.</p>
    <p><strong>Online</strong>: Lightweight metrics only (latency, tokens, thumbs up/down). No LLM-as-Judge in hot path.</p>
    <p><strong>Human review</strong>: Sample 5% to calibrate automated scores.</p>
  </div>
</div>

<h2>âš¡ Part 2: Caching ç­–ç•¥ / Caching Strategies</h2>

<div class="bilingual">
  <div class="zh">
    <p>RAG ç³»ç»Ÿæœ‰ä¸‰ä¸ªæ˜‚è´µæ“ä½œï¼šEmbedding è®¡ç®—ã€å‘é‡æ£€ç´¢ã€LLM ç”Ÿæˆã€‚æ¯ä¸€å±‚éƒ½å¯ä»¥ç¼“å­˜ã€‚</p>
  </div>
  <div class="en">
    <p>Three expensive operations in RAG: embedding computation, vector search, and LLM generation. Each can be cached at different levels.</p>
  </div>
</div>

<h3>ä¸‰å±‚ç¼“å­˜æ¶æ„</h3>

<div class="architecture">
  User Query
      â”‚
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ L1: Exact Match Cache     â”‚  â† Query hash â†’ cached response
  â”‚     (Redis/Memcached)     â”‚     Hit rate: 15-30%
  â”‚     TTL: 1-24 hours       â”‚     Latency: ~1ms
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ miss
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ L2: Semantic Cache         â”‚  â† Embedding similarity > threshold
  â”‚     (Vector DB + Redis)    â”‚     Hit rate: 20-40%  
  â”‚     Similarity > 0.95      â”‚     Latency: ~50ms
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ miss
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ L3: Result Cache           â”‚  â† Cache retrieval results
  â”‚     (skip LLM if same ctx) â”‚     Hit rate: 30-50%
  â”‚     Hash(top-K docs)       â”‚     Latency: ~100ms
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ miss
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Full RAG Pipeline          â”‚     Latency: 2-10s
  â”‚ (Embed â†’ Retrieve â†’ LLM)  â”‚     Cost: $$$
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>

<h4>L1: Exact Match Cache â€” ç®€å•æœ‰æ•ˆ</h4>

<pre><code class="language-typescript">// æœ€ç®€å•çš„ç¼“å­˜ï¼šquery hash
import { createHash } from 'crypto';

class ExactMatchCache {
  private redis: Redis;
  private ttl: number;

  async get(query: string): Promise&lt;string | null&gt; {
    const key = `rag:exact:${this.hashQuery(query)}`;
    return this.redis.get(key);
  }

  async set(query: string, response: string): Promise&lt;void&gt; {
    const key = `rag:exact:${this.hashQuery(query)}`;
    await this.redis.set(key, response, 'EX', this.ttl);
  }

  private hashQuery(query: string): string {
    // Normalize: lowercase, trim, remove extra spaces
    const normalized = query.toLowerCase().trim().replace(/\s+/g, ' ');
    return createHash('sha256').update(normalized).digest('hex');
  }
}</code></pre>

<h4>L2: Semantic Cache â€” æ ¸å¿ƒåˆ›æ–°</h4>

<pre><code class="language-typescript">// è¯­ä¹‰ç¼“å­˜ï¼šç›¸ä¼¼é—®é¢˜å¤ç”¨ç­”æ¡ˆ
class SemanticCache {
  private vectorDB: VectorStore;  // ç¼“å­˜ query embeddings
  private redis: Redis;           // ç¼“å­˜ responses
  private threshold = 0.95;       // ç›¸ä¼¼åº¦é˜ˆå€¼

  async get(query: string): Promise&lt;string | null&gt; {
    // 1. Embed the query
    const queryEmbedding = await embed(query);
    
    // 2. Search for similar cached queries
    const results = await this.vectorDB.search(queryEmbedding, { topK: 1 });
    
    if (results[0] && results[0].score > this.threshold) {
      // 3. Similar query found â€” return cached response
      const cachedId = results[0].metadata.cacheId;
      return this.redis.get(`rag:semantic:${cachedId}`);
    }
    
    return null; // Cache miss
  }

  async set(query: string, response: string): Promise&lt;void&gt; {
    const cacheId = crypto.randomUUID();
    const queryEmbedding = await embed(query);
    
    // Store embedding for future similarity search
    await this.vectorDB.upsert({
      id: cacheId,
      embedding: queryEmbedding,
      metadata: { cacheId, query, createdAt: Date.now() }
    });
    
    // Store response
    await this.redis.set(`rag:semantic:${cacheId}`, response, 'EX', 3600);
  }
}</code></pre>

<div class="highlight-blue">
  <strong>ğŸ¤” Semantic Cache çš„å‘</strong>ï¼šé˜ˆå€¼å¾ˆæ•æ„Ÿã€‚0.95 å¤ªé«˜ â†’ å‘½ä¸­ç‡ä½ï¼›0.90 å¤ªä½ â†’ è¯­ä¹‰ä¸åŒçš„é—®é¢˜è¢«è¯¯åŒ¹é…ã€‚å»ºè®®ï¼šå…ˆ 0.95 ä¸Šçº¿ï¼Œæ”¶é›†æ•°æ®åè°ƒæ•´ã€‚è¿˜è¦è€ƒè™‘ <strong>cache invalidation</strong> â€” å½“æ–‡æ¡£æ›´æ–°æ—¶ï¼Œç›¸å…³ç¼“å­˜å¿…é¡»å¤±æ•ˆã€‚
</div>

<h4>L3: Result Cache â€” æ£€ç´¢ç»“æœå¤ç”¨</h4>

<div class="bilingual">
  <div class="zh">
    <p>æ€è·¯ï¼šå¦‚æœä¸¤ä¸ªä¸åŒé—®é¢˜æ£€ç´¢åˆ°äº†<strong>å®Œå…¨ç›¸åŒçš„æ–‡æ¡£é›†åˆ</strong>ï¼Œå¯ä»¥è·³è¿‡ LLM é‡æ–°ç”Ÿæˆã€‚ç”¨ top-K æ–‡æ¡£çš„ ID ç»„åˆåš hashã€‚</p>
    <p>å®é™…ä¸­æ•ˆæœæœ‰é™ï¼Œå› ä¸ºä¸åŒé—®é¢˜å³ä½¿æ£€ç´¢åˆ°ç›¸åŒæ–‡æ¡£ï¼Œprompt ä¸åŒä¹Ÿéœ€è¦ä¸åŒçš„ç­”æ¡ˆã€‚æ›´é€‚åˆç”¨åœ¨<strong>æ£€ç´¢ç»“æœç¼“å­˜</strong>ï¼ˆè·³è¿‡å‘é‡æœç´¢ï¼Œä¸è·³è¿‡ LLMï¼‰ã€‚</p>
  </div>
  <div class="en">
    <p>Hash the top-K document IDs. If same docs retrieved, potentially reuse. In practice, more useful for caching the retrieval step itself (skip vector search, still run LLM).</p>
  </div>
</div>

<h2>ğŸ’° Part 3: Cost Optimization / æˆæœ¬ä¼˜åŒ–</h2>

<h3>æˆæœ¬æ‹†è§£</h3>

<table>
  <tr>
    <th>Component</th>
    <th>Cost Driver</th>
    <th>Typical %</th>
    <th>Optimization</th>
  </tr>
  <tr>
    <td>LLM Generation</td>
    <td>Output tokens</td>
    <td>60-80%</td>
    <td>Model routing, prompt compression</td>
  </tr>
  <tr>
    <td>Embedding</td>
    <td>Input tokens</td>
    <td>5-15%</td>
    <td>Caching, batch processing</td>
  </tr>
  <tr>
    <td>Vector DB</td>
    <td>Storage + QPS</td>
    <td>5-10%</td>
    <td>Index optimization, filtering</td>
  </tr>
  <tr>
    <td>Reranking</td>
    <td>Per call</td>
    <td>5-10%</td>
    <td>Reduce candidate set</td>
  </tr>
</table>

<h3>ğŸ¯ ç­–ç•¥ 1: Model Routing (æ¨¡å‹è·¯ç”±)</h3>

<div class="bilingual">
  <div class="zh">
    <p><strong>æ ¸å¿ƒæ€è·¯</strong>ï¼šä¸æ˜¯æ‰€æœ‰é—®é¢˜éƒ½éœ€è¦ GPT-4ã€‚ç®€å•é—®é¢˜ç”¨å°æ¨¡å‹ï¼Œå¤æ‚é—®é¢˜æ‰ç”¨å¤§æ¨¡å‹ã€‚</p>
  </div>
  <div class="en">
    <p><strong>Key insight</strong>: Route simple queries to cheaper models, reserve expensive models for complex ones.</p>
  </div>
</div>

<pre><code class="language-typescript">// Model routing: åˆ†çº§ç­–ç•¥
interface ModelTier {
  name: string;
  model: string;
  costPer1kTokens: number;
  maxComplexity: number;
}

const tiers: ModelTier[] = [
  { name: 'fast',    model: 'gpt-4o-mini',   costPer1kTokens: 0.00015, maxComplexity: 3 },
  { name: 'balanced',model: 'gpt-4o',        costPer1kTokens: 0.0025,  maxComplexity: 7 },
  { name: 'premium', model: 'claude-opus',   costPer1kTokens: 0.015,   maxComplexity: 10 },
];

async function routeQuery(query: string, context: string[]): Promise&lt;ModelTier&gt; {
  // æ–¹æ³• 1: è§„åˆ™è·¯ç”±ï¼ˆä¾¿å®œï¼Œå¿«ï¼‰
  if (query.split(' ').length < 10 && context.length <= 2) {
    return tiers[0]; // Simple query, few docs â†’ fast model
  }
  
  // æ–¹æ³• 2: LLM åˆ†ç±»å™¨è·¯ç”±ï¼ˆæ›´å‡†ï¼Œæœ‰é¢å¤–æˆæœ¬ï¼‰
  const complexity = await classifyComplexity(query); // ç”¨å°æ¨¡å‹æ‰“åˆ† 1-10
  return tiers.find(t => complexity <= t.maxComplexity) || tiers[2];
}

// æ•ˆæœï¼šå¯é™ä½ 40-60% çš„ LLM æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒ >95% çš„ç­”æ¡ˆè´¨é‡</code></pre>

<h3>ğŸ¯ ç­–ç•¥ 2: Prompt Compression (Prompt å‹ç¼©)</h3>

<pre><code class="language-typescript">// å‹ç¼© retrieved contextï¼Œå‡å°‘ input tokens
async function compressContext(
  query: string, 
  documents: Document[], 
  tokenBudget: number
): Promise&lt;string&gt; {
  // æ–¹æ³• 1: Extractive â€” åªä¿ç•™ç›¸å…³å¥å­
  const relevantSentences = [];
  for (const doc of documents) {
    const sentences = doc.text.split('. ');
    for (const sentence of sentences) {
      const relevance = await quickRelevanceScore(query, sentence);
      if (relevance > 0.7) {
        relevantSentences.push(sentence);
      }
    }
  }
  
  // æ–¹æ³• 2: Token budget â€” æŒ‰ç›¸å…³æ€§æ’åºåæˆªæ–­
  let totalTokens = 0;
  const selected = [];
  for (const sent of relevantSentences.sort((a, b) => b.score - a.score)) {
    const tokens = countTokens(sent);
    if (totalTokens + tokens > tokenBudget) break;
    selected.push(sent);
    totalTokens += tokens;
  }
  
  return selected.join('. ');
}
// æ•ˆæœï¼šContext é•¿åº¦å¯å‹ç¼© 50-70%ï¼ŒLLM æˆæœ¬ç›¸åº”é™ä½</code></pre>

<h3>ğŸ¯ ç­–ç•¥ 3: Async + Batch Processing</h3>

<div class="bilingual">
  <div class="zh">
    <p>ä¸æ˜¯æ‰€æœ‰ RAG è¯·æ±‚éƒ½éœ€è¦å®æ—¶å“åº”ï¼š</p>
    <ul>
      <li><strong>å®æ—¶</strong>ï¼šç”¨æˆ·ç›´æ¥æé—® â†’ èµ°å…¨ pipeline</li>
      <li><strong>Near-real-time</strong>ï¼šåå°æ€»ç»“ã€æŠ¥å‘Šç”Ÿæˆ â†’ æ”’æ‰¹å¤„ç†ï¼Œç”¨ Batch APIï¼ˆOpenAI Batch API ä¾¿å®œ 50%ï¼‰</li>
      <li><strong>ç¦»çº¿</strong>ï¼šæ–‡æ¡£ç´¢å¼•ã€è¯„ä¼° â†’ å®šæ—¶ä»»åŠ¡ï¼Œç”¨æœ€ä¾¿å®œçš„æ¨¡å‹</li>
    </ul>
  </div>
  <div class="en">
    <p>Not all RAG needs real-time response. Use Batch APIs (50% cheaper) for background tasks. Reserve real-time pipeline for interactive queries only.</p>
  </div>
</div>

<h3>ğŸ’¸ ç»¼åˆæˆæœ¬ä¼˜åŒ–æ•ˆæœ</h3>

<table>
  <tr>
    <th>Strategy</th>
    <th>Cost Reduction</th>
    <th>Quality Impact</th>
    <th>Implementation Effort</th>
  </tr>
  <tr>
    <td>Semantic Cache</td>
    <td>20-40%</td>
    <td>None (exact reuse)</td>
    <td>Medium</td>
  </tr>
  <tr>
    <td>Model Routing</td>
    <td>40-60%</td>
    <td>Minimal (~2% drop)</td>
    <td>Medium</td>
  </tr>
  <tr>
    <td>Prompt Compression</td>
    <td>30-50%</td>
    <td>Low (~3% drop)</td>
    <td>Low</td>
  </tr>
  <tr>
    <td>Batch Processing</td>
    <td>50% (batch API)</td>
    <td>None</td>
    <td>Low</td>
  </tr>
  <tr>
    <td><strong>Combined</strong></td>
    <td><strong>70-85%</strong></td>
    <td><strong>~5% quality drop</strong></td>
    <td><strong>High</strong></td>
  </tr>
</table>

<h2>ğŸ“š å¯å­¦ä¹ çš„æ¨¡å¼ / Patterns to Apply</h2>

<div class="highlight-green">
  <h4>Pattern 1: Layered Caching (åˆ†å±‚ç¼“å­˜)</h4>
  <p>ä»»ä½•æ¶‰åŠå¤šæ­¥éª¤ pipeline çš„ç³»ç»Ÿï¼Œéƒ½å¯ä»¥åœ¨æ¯ä¸€æ­¥åŠ ç¼“å­˜ã€‚ä¸ä»…é™äº RAG â€” API gatewayã€å¾®æœåŠ¡ã€CI/CD éƒ½é€‚ç”¨ã€‚å…³é”®æ˜¯æ¯å±‚ç¼“å­˜ç­–ç•¥ä¸åŒï¼šç²¾ç¡®åŒ¹é… â†’ è¯­ä¹‰ç›¸ä¼¼ â†’ ç»“æœå¤ç”¨ã€‚</p>
</div>

<div class="highlight-green">
  <h4>Pattern 2: Quality-Cost Tradeoff via Routing</h4>
  <p>Model Routing æœ¬è´¨æ˜¯"æŒ‰éœ€åˆ†é…èµ„æº"ã€‚åŒæ ·çš„æ€è·¯å¯ä»¥ç”¨åœ¨ï¼šé€‰æ‹©ä¸åŒçš„æ£€ç´¢ç­–ç•¥ã€ä¸åŒçš„ rerankerã€ä¸åŒçš„ prompt æ¨¡æ¿ã€‚æ ¸å¿ƒï¼šåˆ†ç±» â†’ åˆ†çº§ â†’ æŒ‰çº§åˆ†é…ã€‚</p>
</div>

<div class="highlight-green">
  <h4>Pattern 3: LLM-as-Judge for Automated Evaluation</h4>
  <p>ä»»ä½•éœ€è¦è¯„ä¼°"è´¨é‡"ä½†æ²¡æœ‰ ground truth çš„åœºæ™¯ï¼Œéƒ½å¯ä»¥è€ƒè™‘ LLM-as-Judgeã€‚ç¿»è¯‘è´¨é‡ã€ä»£ç è´¨é‡ã€æ‘˜è¦è´¨é‡â€¦ å…³é”®æ˜¯è®¾è®¡å¥½è¯„åˆ†ç»´åº¦ï¼ˆåƒ RAGAS çš„ 4 ç»´ï¼‰å’Œå®šæœŸäººå·¥æ ¡å‡†ã€‚</p>
</div>

<h2>ğŸ¤ é¢è¯•é¢˜ / Interview Questions</h2>

<div class="interview">
  <h4>Q1: "ä½ æ€ä¹ˆè¯„ä¼°ä¸€ä¸ª RAG ç³»ç»Ÿçš„è´¨é‡ï¼Ÿ"</h4>
  <p><strong>ç­”é¢˜æ¡†æ¶</strong>:</p>
  <ol>
    <li><strong>æŒ‡æ ‡ç»´åº¦</strong>: Retrieval quality (Context Precision/Recall) + Generation quality (Faithfulness + Answer Relevancy)</li>
    <li><strong>è¯„ä¼°æ–¹æ³•</strong>: RAGAS æ¡†æ¶ï¼ŒLLM-as-Judge</li>
    <li><strong>Faithfulness æœ€é‡è¦</strong>: å› ä¸ºå¹»è§‰æ˜¯ RAG æœ€å¤§é£é™©</li>
    <li><strong>å±€é™æ€§</strong>: LLM è¯„ä¼°æœ¬èº«æœ‰åå·® â†’ éœ€è¦äººå·¥æŠ½æ ·æ ¡éªŒ</li>
    <li><strong>çº¿ä¸Š vs çº¿ä¸‹</strong>: çº¿ä¸‹å…¨é¢è¯„ä¼° (RAGAS)ï¼Œçº¿ä¸Šè½»é‡ç›‘æ§ (latency + user feedback)</li>
  </ol>
</div>

<div class="interview">
  <h4>Q2: "RAG ç³»ç»Ÿä¸Šçº¿åæˆæœ¬å¤ªé«˜ï¼Œä½ æ€ä¹ˆä¼˜åŒ–ï¼Ÿ"</h4>
  <p><strong>ç­”é¢˜æ¡†æ¶</strong>:</p>
  <ol>
    <li><strong>å…ˆåˆ†æç“¶é¢ˆ</strong>: LLM token é€šå¸¸å  60-80% æˆæœ¬</li>
    <li><strong>ç¼“å­˜</strong>: ä¸‰å±‚ç¼“å­˜ï¼ˆç²¾ç¡®åŒ¹é… â†’ è¯­ä¹‰ç¼“å­˜ â†’ ç»“æœç¼“å­˜ï¼‰ï¼Œå‡å°‘é‡å¤è®¡ç®—</li>
    <li><strong>æ¨¡å‹è·¯ç”±</strong>: ç®€å•æŸ¥è¯¢ç”¨å°æ¨¡å‹ï¼Œå¤æ‚æŸ¥è¯¢ç”¨å¤§æ¨¡å‹</li>
    <li><strong>Prompt å‹ç¼©</strong>: å‡å°‘ context tokenï¼Œåªä¿ç•™ç›¸å…³å†…å®¹</li>
    <li><strong>å¼‚æ­¥æ‰¹å¤„ç†</strong>: éå®æ—¶ä»»åŠ¡ç”¨ Batch API</li>
    <li><strong>é‡åŒ–æ•ˆæœ</strong>: ç»¼åˆå¯é™ä½ 70-85% æˆæœ¬</li>
  </ol>
</div>

<div class="interview">
  <h4>Q3: "Semantic Cache çš„é˜ˆå€¼æ€ä¹ˆè®¾ï¼Ÿæœ‰ä»€ä¹ˆé™·é˜±ï¼Ÿ"</h4>
  <p><strong>è¦ç‚¹</strong>:</p>
  <ul>
    <li>èµ·å§‹å€¼ 0.95ï¼Œè§‚å¯Ÿä¸€å‘¨åè°ƒæ•´</li>
    <li>é™·é˜± 1: åŒä¹‰ä¸åŒæ„ â€” "How to delete my account" vs "How to remove my profile" å¯èƒ½è¯­ä¹‰å¾ˆè¿‘ä½†ç­”æ¡ˆä¸åŒ</li>
    <li>é™·é˜± 2: Cache invalidation â€” æ–‡æ¡£æ›´æ–°åç¼“å­˜çš„ç­”æ¡ˆè¿‡æ—¶</li>
    <li>é™·é˜± 3: å¤šè½®å¯¹è¯ä¸­ï¼ŒåŒä¸€ä¸ª query çš„ä¸Šä¸‹æ–‡ä¸åŒ â†’ ä¸èƒ½åªçœ‹ query ç›¸ä¼¼åº¦</li>
    <li>è§£æ³•: æŠŠ session context ä¹Ÿçº³å…¥ cache key</li>
  </ul>
</div>

<h2>ğŸ“ RAG ç³»åˆ—æ€»ç»“ / Series Recap</h2>

<div class="highlight-blue">
  <h4>å››å¤©å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ</h4>
  <table>
    <tr><th>Day</th><th>Topic</th><th>æ ¸å¿ƒæ”¶è·</th></tr>
    <tr><td>Day 1</td><td>Pipeline Architecture</td><td>Offline/Online åˆ†ç¦»ã€Hybrid Search + RRF èåˆ</td></tr>
    <tr><td>Day 2</td><td>Advanced Chunking</td><td>Semantic/Parent-Child/Agentic ä¸‰ç§ç­–ç•¥çš„æƒè¡¡</td></tr>
    <tr><td>Day 3</td><td>Advanced Retrieval</td><td>5 ç§ Query Transform + Multi-Hop ç»„åˆ</td></tr>
    <tr><td>Day 4</td><td>Production</td><td>RAGAS è¯„ä¼°ã€ä¸‰å±‚ç¼“å­˜ã€æˆæœ¬ä¼˜åŒ– 70-85%</td></tr>
  </table>
  <p><strong>Phase 1.3 RAG ç³»ç»Ÿè®¾è®¡ âœ… å®Œæˆï¼</strong></p>
  <p>ä¸‹ä¸€é˜¶æ®µï¼šPhase 2 â€” AI System Design é¢è¯•é¢˜ï¼ˆè®¾è®¡ ChatGPTã€æ¨èç³»ç»Ÿç­‰ï¼‰</p>
</div>

<div class="series-nav">
  ğŸ“š <strong>RAG ç³»åˆ—å®Œç»“ï¼</strong> 
  <a href="2026-02-05-rag-pipeline-architecture.html">Day 1</a> â†’ 
  <a href="2026-02-07-rag-advanced-chunking.html">Day 2</a> â†’ 
  <a href="2026-02-08-rag-advanced-retrieval.html">Day 3</a> â†’ 
  <strong>Day 4 âœ…</strong>
</div>

</body>
</html>
