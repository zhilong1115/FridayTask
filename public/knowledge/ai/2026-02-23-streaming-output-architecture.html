<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-23 - LLM æµå¼è¾“å‡ºæ¶æ„</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.7; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; }
    h2 { color: #1a73e8; border-bottom: 1px solid #e8eaed; padding-bottom: 8px; }
    h3 { color: #5f6368; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 13px; }
    code { font-family: 'SF Mono', Monaco, monospace; }
    .highlight { background: #fff3cd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-blue { background: #d1ecf1; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-green { background: #d4edda; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-red { background: #f8d7da; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .diagram { background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: monospace; white-space: pre; font-size: 13px; line-height: 1.4; }
    .follow-up { margin-top: 30px; padding: 15px; border: 1px dashed #dadce0; border-radius: 8px; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    th, td { border: 1px solid #dadce0; padding: 10px; text-align: left; }
    th { background: #f1f3f4; }
    .series-nav { background: #e8f0fe; padding: 15px; border-radius: 8px; margin: 20px 0; }
    .tag { display: inline-block; background: #e8eaed; padding: 2px 8px; border-radius: 4px; font-size: 12px; margin: 2px; }
    @media (max-width: 768px) { .bilingual { grid-template-columns: 1fr; } }
  </style>
</head>
<body>

<div class="series-nav">
  ğŸ“š <strong>LLM ç‰¹æœ‰é—®é¢˜ç³»åˆ— Day 4/5</strong> |
  <a href="2026-02-20-token-limit-handling-strategies.html">Day 1: Token é™åˆ¶</a> â†’
  <a href="2026-02-21-llm-cost-optimization.html">Day 2: æˆæœ¬ä¼˜åŒ–</a> â†’
  <a href="2026-02-22-hallucination-detection-mitigation.html">Day 3: Hallucination</a> â†’
  <strong>Day 4: æµå¼è¾“å‡º</strong> â†’
  Day 5: Multi-turn å¯¹è¯
</div>

<h1>âš¡ LLM æµå¼è¾“å‡ºæ¶æ„ / Streaming Output Architecture</h1>
<p>
  <span class="tag">Phase 2.3</span>
  <span class="tag">System Design</span>
  <span class="tag">é¢è¯•é«˜é¢‘</span>
  ğŸ“… 2026-02-23
</p>

<h2>ğŸ“– ä¸ºä»€ä¹ˆéœ€è¦æµå¼è¾“å‡º / Why Streaming?</h2>
<div class="bilingual">
  <div class="zh">
    <p>LLM ç”Ÿæˆä¸€ä¸ªå®Œæ•´å›å¤å¯èƒ½éœ€è¦ <strong>3-30 ç§’</strong>ã€‚å¦‚æœç­‰å…¨éƒ¨ç”Ÿæˆå®Œå†è¿”å›ï¼Œç”¨æˆ·ä½“éªŒæå·®ã€‚æµå¼è¾“å‡ºè®©ç¬¬ä¸€ä¸ª token åœ¨ <strong>200-500ms</strong> å†…å°±åˆ°è¾¾ç”¨æˆ·å±å¹•ï¼ˆTTFT - Time To First Tokenï¼‰ï¼Œç»™ç”¨æˆ·"AI æ­£åœ¨æ€è€ƒ"çš„å³æ—¶åé¦ˆã€‚</p>
    <p><strong>æ ¸å¿ƒæŒ‡æ ‡ï¼š</strong></p>
    <ul>
      <li><strong>TTFT</strong>ï¼ˆé¦– token æ—¶é—´ï¼‰ï¼š&lt;500ms ä¼˜ç§€</li>
      <li><strong>TPS</strong>ï¼ˆtoken/ç§’ååï¼‰ï¼š30-80 tok/s å…¸å‹</li>
      <li><strong>æ„ŸçŸ¥å»¶è¿Ÿ</strong>ï¼šæµå¼é™ä½ 80%+ æ„ŸçŸ¥ç­‰å¾…</li>
    </ul>
  </div>
  <div class="en">
    <p>An LLM takes <strong>3-30 seconds</strong> to generate a full response. Waiting for completion creates terrible UX. Streaming delivers the first token in <strong>200-500ms</strong> (TTFT), giving users immediate "thinking" feedback.</p>
    <p><strong>Key metrics:</strong></p>
    <ul>
      <li><strong>TTFT</strong> (Time To First Token): &lt;500ms is great</li>
      <li><strong>TPS</strong> (Tokens Per Second): 30-80 tok/s typical</li>
      <li><strong>Perceived latency</strong>: 80%+ reduction with streaming</li>
    </ul>
  </div>
</div>

<h2>ğŸ—ï¸ æ•´ä½“æ¶æ„ / End-to-End Architecture</h2>

<div class="diagram">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Complete Streaming Pipeline                    â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Client   â”‚    â”‚   API    â”‚    â”‚  Stream  â”‚    â”‚   LLM    â”‚   â”‚
â”‚  â”‚(Browser/  â”‚â—„â”€â”€â–ºâ”‚ Gateway  â”‚â—„â”€â”€â–ºâ”‚ Processorâ”‚â—„â”€â”€â–ºâ”‚ Engine   â”‚   â”‚
â”‚  â”‚  App)     â”‚    â”‚(Nginx/   â”‚    â”‚(Node/Go) â”‚    â”‚(vLLM/    â”‚   â”‚
â”‚  â”‚          â”‚    â”‚ Kong)    â”‚    â”‚          â”‚    â”‚ TGI)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚               â”‚               â”‚               â”‚          â”‚
â”‚       â”‚   SSE/WS      â”‚    gRPC/      â”‚   Internal    â”‚          â”‚
â”‚       â”‚   HTTP/2      â”‚    HTTP/2     â”‚   Iterator    â”‚          â”‚
â”‚       â”‚               â”‚               â”‚               â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ UI State â”‚    â”‚Connectionâ”‚    â”‚  Token   â”‚    â”‚ KV Cache â”‚   â”‚
â”‚  â”‚ Machine  â”‚    â”‚  Pool    â”‚    â”‚  Buffer  â”‚    â”‚ Manager  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>

<h2>ğŸ”Œ ä¼ è¾“åè®®å¯¹æ¯” / Protocol Comparison: SSE vs WebSocket vs HTTP/2</h2>

<table>
  <tr>
    <th>ç»´åº¦</th>
    <th>SSE (Server-Sent Events)</th>
    <th>WebSocket</th>
    <th>HTTP/2 Stream</th>
  </tr>
  <tr>
    <td><strong>æ–¹å‘</strong></td>
    <td>å•å‘ (Serverâ†’Client)</td>
    <td>åŒå‘</td>
    <td>åŒå‘ (å¤šè·¯å¤ç”¨)</td>
  </tr>
  <tr>
    <td><strong>åè®®</strong></td>
    <td>HTTP/1.1 or HTTP/2</td>
    <td>ç‹¬ç«‹ ws:// åè®®</td>
    <td>HTTP/2</td>
  </tr>
  <tr>
    <td><strong>è‡ªåŠ¨é‡è¿</strong></td>
    <td>âœ… å†…ç½® (EventSource API)</td>
    <td>âŒ éœ€è‡ªè¡Œå®ç°</td>
    <td>âŒ éœ€è‡ªè¡Œå®ç°</td>
  </tr>
  <tr>
    <td><strong>è´Ÿè½½å‡è¡¡</strong></td>
    <td>âœ… æ ‡å‡† HTTPï¼Œç®€å•</td>
    <td>âš ï¸ éœ€è¦ sticky session</td>
    <td>âœ… HTTP åŸç”Ÿ</td>
  </tr>
  <tr>
    <td><strong>CDN/Proxy å…¼å®¹</strong></td>
    <td>âœ… æ™®éæ”¯æŒ</td>
    <td>âš ï¸ éœ€ç‰¹æ®Šé…ç½®</td>
    <td>âœ… åŸç”Ÿæ”¯æŒ</td>
  </tr>
  <tr>
    <td><strong>è°ƒè¯•</strong></td>
    <td>âœ… curl å³å¯</td>
    <td>âš ï¸ éœ€ä¸“ç”¨å·¥å…·</td>
    <td>âœ… æ ‡å‡† HTTP å·¥å…·</td>
  </tr>
  <tr>
    <td><strong>å…¸å‹ä½¿ç”¨è€…</strong></td>
    <td>OpenAI, Anthropic, Gemini</td>
    <td>å®æ—¶åä½œã€æ¸¸æˆ</td>
    <td>gRPC (å†…éƒ¨é€šä¿¡)</td>
  </tr>
</table>

<div class="highlight">
  <h4>ğŸ¯ é¢è¯•è¦ç‚¹ï¼šä¸ºä»€ä¹ˆ LLM API å‡ ä¹éƒ½é€‰ SSEï¼Ÿ</h4>
  <p><strong>1. éœ€æ±‚åŒ¹é…</strong>ï¼šLLM ç”Ÿæˆæ˜¯"æœåŠ¡ç«¯â†’å®¢æˆ·ç«¯"å•å‘æ¨é€ï¼Œä¸éœ€è¦åŒå‘é€šä¿¡</p>
  <p><strong>2. è¿ç»´å‹å¥½</strong>ï¼šSSE æ˜¯æ ‡å‡† HTTPï¼Œæ‰€æœ‰è´Ÿè½½å‡è¡¡/CDN/ä»£ç†éƒ½åŸç”Ÿæ”¯æŒ</p>
  <p><strong>3. è‡ªåŠ¨é‡è¿</strong>ï¼šæµè§ˆå™¨ EventSource API å†…ç½®é‡è¿ + Last-Event-ID æ–­ç‚¹ç»­ä¼ </p>
  <p><strong>4. ç®€å•è°ƒè¯•</strong>ï¼š<code>curl</code> åŠ  <code>--no-buffer</code> å°±èƒ½çœ‹åˆ°æµå¼è¾“å‡º</p>
</div>

<h2>ğŸ’» SSE åè®®è¯¦è§£ / SSE Protocol Deep Dive</h2>

<h3>OpenAI é£æ ¼çš„ SSE æ ¼å¼</h3>
<pre><code>// HTTP Response Headers
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive
Transfer-Encoding: chunked

// SSE Event Stream - æ¯ä¸ª chunk æ˜¯ä¸€ä¸ª JSON
data: {"id":"chatcmpl-abc","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":" world"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":"!"},"finish_reason":"stop"}]}

data: [DONE]</code></pre>

<h3>æœåŠ¡ç«¯å®ç° (Node.js / TypeScript)</h3>
<pre><code>// === ç”Ÿäº§çº§ SSE æµå¼å¤„ç†æœåŠ¡ ===

interface StreamOptions {
  model: string;
  messages: Message[];
  onToken?: (token: string) => void;  // ä¸­é—´å¤„ç† hook
  signal?: AbortSignal;               // å®¢æˆ·ç«¯å–æ¶ˆ
}

async function streamCompletion(
  req: Request, res: Response, options: StreamOptions
) {
  // 1ï¸âƒ£ è®¾ç½® SSE Headers
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache, no-transform',  // no-transform é˜² CDN å‹ç¼©
    'Connection': 'keep-alive',
    'X-Accel-Buffering': 'no',  // ç¦ç”¨ Nginx ç¼“å†²ï¼å…³é”®ï¼
  });

  // 2ï¸âƒ£ å¿ƒè·³ä¿æ´» â€” é˜²æ­¢ä»£ç†/LB è¶…æ—¶æ–­è¿
  const heartbeat = setInterval(() => {
    res.write(': heartbeat\n\n');  // SSE æ³¨é‡Šæ ¼å¼ï¼Œå®¢æˆ·ç«¯å¿½ç•¥
  }, 15_000);

  // 3ï¸âƒ£ è°ƒç”¨ LLM è·å– stream
  const stream = await openai.chat.completions.create({
    ...options,
    stream: true,
  });

  let fullContent = '';
  let tokenCount = 0;

  try {
    for await (const chunk of stream) {
      // 4ï¸âƒ£ æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­è¿
      if (req.socket.destroyed || options.signal?.aborted) {
        stream.controller.abort();  // åœæ­¢ LLM ç”Ÿæˆï¼ŒèŠ‚çœæˆæœ¬ï¼
        break;
      }

      const delta = chunk.choices[0]?.delta?.content;
      if (delta) {
        fullContent += delta;
        tokenCount++;

        // 5ï¸âƒ£ ä¸­é—´å¤„ç† hookï¼ˆsafety filterã€PII æ£€æµ‹ç­‰ï¼‰
        options.onToken?.(delta);

        // 6ï¸âƒ£ å‘é€ SSE event
        res.write(`data: ${JSON.stringify(chunk)}\n\n`);
      }

      // 7ï¸âƒ£ æ£€æŸ¥å®ŒæˆåŸå› 
      if (chunk.choices[0]?.finish_reason) {
        break;
      }
    }

    // 8ï¸âƒ£ å‘é€å®Œæˆä¿¡å·
    res.write('data: [DONE]\n\n');
  } catch (err) {
    // 9ï¸âƒ£ é”™è¯¯å¤„ç† â€” å‘é€ SSE error event
    res.write(`event: error\ndata: ${JSON.stringify({
      error: 'Stream interrupted',
      partial: fullContent,  // è¿”å›å·²ç”Ÿæˆçš„éƒ¨åˆ†
      tokens: tokenCount,
    })}\n\n`);
  } finally {
    clearInterval(heartbeat);
    res.end();

    // ğŸ”Ÿ å¼‚æ­¥ä¿å­˜å®Œæ•´å“åº”ï¼ˆä¸é˜»å¡è¿æ¥å…³é—­ï¼‰
    saveToDatabase(fullContent, tokenCount).catch(console.error);
  }
}</code></pre>

<h3>å®¢æˆ·ç«¯å®ç° (TypeScript / React)</h3>
<pre><code>// === ç”Ÿäº§çº§å®¢æˆ·ç«¯ SSE æ¶ˆè´¹ ===

interface StreamState {
  status: 'idle' | 'connecting' | 'streaming' | 'done' | 'error';
  content: string;
  tokenCount: number;
  ttft: number | null;  // Time To First Token
}

function useStreamChat() {
  const [state, setState] = useState&lt;StreamState&gt;({
    status: 'idle', content: '', tokenCount: 0, ttft: null
  });
  const abortRef = useRef&lt;AbortController | null&gt;(null);

  const startStream = async (messages: Message[]) => {
    // 1ï¸âƒ£ å–æ¶ˆä¸Šä¸€ä¸ªæµï¼ˆå¦‚æœè¿˜åœ¨è¿›è¡Œï¼‰
    abortRef.current?.abort();
    const controller = new AbortController();
    abortRef.current = controller;

    setState({ status: 'connecting', content: '', tokenCount: 0, ttft: null });
    const startTime = performance.now();

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ messages, stream: true }),
        signal: controller.signal,
      });

      if (!response.ok) throw new Error(`HTTP ${response.status}`);

      // 2ï¸âƒ£ ä½¿ç”¨ ReadableStream è§£æï¼ˆæ¯” EventSource æ›´çµæ´»ï¼‰
      const reader = response.body!.getReader();
      const decoder = new TextDecoder();
      let buffer = '';  // å¤„ç†è·¨ chunk çš„ä¸å®Œæ•´è¡Œ
      let content = '';
      let tokenCount = 0;
      let firstToken = true;

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });

        // 3ï¸âƒ£ é€è¡Œè§£æ SSE
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';  // æœ€åä¸€è¡Œå¯èƒ½ä¸å®Œæ•´

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') {
              setState(s => ({ ...s, status: 'done' }));
              return;
            }

            const chunk = JSON.parse(data);
            const delta = chunk.choices[0]?.delta?.content;
            if (delta) {
              // 4ï¸âƒ£ è®°å½• TTFT
              if (firstToken) {
                const ttft = performance.now() - startTime;
                setState(s => ({ ...s, ttft, status: 'streaming' }));
                firstToken = false;
              }

              content += delta;
              tokenCount++;

              // 5ï¸âƒ£ æ‰¹é‡æ›´æ–° UIï¼ˆé¿å…æ¯ä¸ª token éƒ½è§¦å‘ re-renderï¼‰
              // React 18 è‡ªåŠ¨ batching å·²å¤„ç†ï¼Œä½†å¯è¿›ä¸€æ­¥ä¼˜åŒ–
              setState(s => ({ ...s, content, tokenCount }));
            }
          }
        }
      }
    } catch (err) {
      if (err.name !== 'AbortError') {
        setState(s => ({ ...s, status: 'error' }));
      }
    }
  };

  const stopStream = () => abortRef.current?.abort();

  return { ...state, startStream, stopStream };
}</code></pre>

<h2>âš¡ å…­å¤§å·¥ç¨‹æŒ‘æˆ˜ / Six Engineering Challenges</h2>

<div class="highlight">
  <h4>æŒ‘æˆ˜ 1ï¼šNginx/ä»£ç†ç¼“å†² (The #1 Production Gotcha)</h4>
  <div class="bilingual">
    <div class="zh">
      <p>Nginx é»˜è®¤ä¼šç¼“å†²åç«¯å“åº”ï¼Œå¯¼è‡´ tokens ç§¯æ”’åæ‰¹é‡å‘é€ï¼Œç ´åé€ token æµå¼æ•ˆæœã€‚</p>
      <p><strong>è§£å†³æ–¹æ¡ˆï¼š</strong></p>
      <ul>
        <li><code>proxy_buffering off;</code> â€” å…¨å±€å…³é—­</li>
        <li><code>X-Accel-Buffering: no</code> â€” æŒ‰è¯·æ±‚å…³é—­ï¼ˆæ¨èï¼‰</li>
        <li><code>chunked_transfer_encoding on;</code></li>
        <li>AWS ALB: å¯ç”¨ "HTTP/2 streaming" æ¨¡å¼</li>
      </ul>
    </div>
    <div class="en">
      <p>Nginx buffers upstream responses by default, batching tokens together and destroying the streaming experience.</p>
      <p><strong>Solutions:</strong></p>
      <ul>
        <li><code>proxy_buffering off;</code> â€” global disable</li>
        <li><code>X-Accel-Buffering: no</code> â€” per-request (recommended)</li>
        <li>AWS ALB: Enable "HTTP/2 streaming" mode</li>
      </ul>
    </div>
  </div>
</div>

<div class="highlight-blue">
  <h4>æŒ‘æˆ˜ 2ï¼šå®¢æˆ·ç«¯æ–­è¿æ£€æµ‹ä¸èµ„æºå›æ”¶</h4>
  <div class="bilingual">
    <div class="zh">
      <p>ç”¨æˆ·å…³é—­é¡µé¢/åˆ‡æ¢å¯¹è¯æ—¶ï¼ŒæœåŠ¡ç«¯å¿…é¡»<strong>ç«‹å³åœæ­¢ LLM æ¨ç†</strong>ï¼Œå¦åˆ™æµªè´¹ GPU å’Œ tokenã€‚</p>
      <pre><code>// æœåŠ¡ç«¯æ£€æµ‹æ–­è¿
req.on('close', () => {
  stream.controller.abort();   // åœæ­¢ LLM
  clearInterval(heartbeat);    // æ¸…ç†å¿ƒè·³
  // GPU èµ„æºå³æ—¶é‡Šæ”¾å›æ¨ç†å¼•æ“
});

// å®¢æˆ·ç«¯ä¸»åŠ¨å–æ¶ˆï¼ˆStop Generating æŒ‰é’®ï¼‰
const controller = new AbortController();
fetch(url, { signal: controller.signal });
// ç”¨æˆ·ç‚¹å‡» Stop â†’ controller.abort();</code></pre>
    </div>
    <div class="en">
      <p>When a user closes the tab or switches conversation, the server must <strong>immediately stop LLM inference</strong> to save GPU and tokens.</p>
    </div>
  </div>
</div>

<div class="highlight-green">
  <h4>æŒ‘æˆ˜ 3ï¼šæµå¼ Safety Filtering</h4>
  <div class="bilingual">
    <div class="zh">
      <p>æµå¼è¾“å‡ºæ—¶ï¼Œæ¯ä¸ª token ç‹¬ç«‹åˆ°è¾¾ï¼Œä½†å®‰å…¨æ£€æµ‹éœ€è¦<strong>è¯­ä¹‰ä¸Šä¸‹æ–‡</strong>ã€‚æŒ‘æˆ˜ï¼šå¦‚ä½•åœ¨ä¸å¢åŠ å»¶è¿Ÿçš„æƒ…å†µä¸‹åšå®‰å…¨è¿‡æ»¤ï¼Ÿ</p>
      <p><strong>ä¸‰å±‚æ–¹æ¡ˆï¼š</strong></p>
      <ol>
        <li><strong>Token-level</strong>ï¼šå…³é”®è¯é»‘åå•ï¼Œå³æ—¶æ‹¦æˆªï¼ˆå»¶è¿Ÿ &lt;1msï¼‰</li>
        <li><strong>Sliding-window</strong>ï¼šæ¯ç§¯ç´¯ N ä¸ª token åšä¸€æ¬¡åˆ†ç±»ï¼ˆå»¶è¿Ÿ ~50msï¼‰</li>
        <li><strong>Post-hoc</strong>ï¼šå®Œæ•´å“åº”ååšæ·±åº¦æ£€æŸ¥ï¼Œå‘ç°é—®é¢˜å‘ <code>retract</code> äº‹ä»¶</li>
      </ol>
    </div>
    <div class="en">
      <p><strong>Three-layer approach:</strong></p>
      <ol>
        <li><strong>Token-level</strong>: keyword blocklist, instant (&lt;1ms)</li>
        <li><strong>Sliding-window</strong>: classify every N tokens (~50ms)</li>
        <li><strong>Post-hoc</strong>: full response check, send <code>retract</code> event if needed</li>
      </ol>
    </div>
  </div>
</div>

<div class="highlight">
  <h4>æŒ‘æˆ˜ 4ï¼šç»“æ„åŒ–è¾“å‡ºçš„æµå¼å¤„ç† (Streaming Structured Output)</h4>
  <div class="bilingual">
    <div class="zh">
      <p>å½“ LLM éœ€è¦è¿”å› JSON æ—¶ï¼Œæµå¼è¾“å‡ºé¢ä¸´ç‰¹æ®ŠæŒ‘æˆ˜ï¼šä¸å®Œæ•´çš„ JSON æ— æ³•è§£æã€‚</p>
      <p><strong>ä¸‰ç§ç­–ç•¥ï¼š</strong></p>
      <ol>
        <li><strong>å¢é‡ JSON è§£æå™¨</strong>ï¼šç”¨ streaming JSON parser (å¦‚ <code>partial-json</code>)ï¼Œæ¯æ¬¡å°½å¯èƒ½å¤šè§£æ</li>
        <li><strong>åŒé€šé“</strong>ï¼šä¸€ä¸ª SSE æµç»™ UI æ˜¾ç¤ºåŸæ–‡ï¼Œå¦ä¸€ä¸ªé€šé“åœ¨å®Œæˆåè¿”å›ç»“æ„åŒ–ç»“æœ</li>
        <li><strong>Function Calling æ¨¡å¼</strong>ï¼šOpenAI çš„ tool_calls æµå¼è¿”å› arguments å­—æ®µï¼ŒæŒ‰ index æ‹¼æ¥</li>
      </ol>
    </div>
    <div class="en">
      <p>When LLM returns JSON, incomplete JSON can't be parsed. Solutions:</p>
      <ol>
        <li><strong>Incremental parser</strong>: streaming JSON parser like <code>partial-json</code></li>
        <li><strong>Dual channel</strong>: stream raw text for UI, return structured result on completion</li>
        <li><strong>Function Calling</strong>: OpenAI streams <code>tool_calls.arguments</code> incrementally</li>
      </ol>
    </div>
  </div>
</div>

<div class="highlight-blue">
  <h4>æŒ‘æˆ˜ 5ï¼šå¤šç”¨æˆ·å¹¶å‘ä¸èƒŒå‹æ§åˆ¶ (Backpressure)</h4>
  <div class="bilingual">
    <div class="zh">
      <p>LLM ç”Ÿæˆé€Ÿåº¦ (30-80 tok/s) é€šå¸¸ <strong>å¿«äºç½‘ç»œå‘é€é€Ÿåº¦</strong>ï¼ˆæ…¢å®¢æˆ·ç«¯ï¼‰ã€‚Token åœ¨æœåŠ¡ç«¯å †ç§¯ = OOM é£é™©ã€‚</p>
      <p><strong>èƒŒå‹ç­–ç•¥ï¼š</strong></p>
      <ul>
        <li><strong>æœ‰ç•Œç¼“å†²</strong>ï¼šæ¯ä¸ªè¿æ¥æœ€å¤šç¼“å†² 100 tokensï¼Œæ»¡åˆ™æš‚åœç”Ÿæˆ</li>
        <li><strong>Drain äº‹ä»¶</strong>ï¼šNode.js <code>res.write()</code> è¿”å› <code>false</code> æ—¶ï¼Œç­‰ <code>drain</code> äº‹ä»¶</li>
        <li><strong>è¶…æ—¶æ–­è¿</strong>ï¼šå¦‚æœ 30s å†…ç¼“å†²æœªæ¸…ç©ºï¼Œä¸»åŠ¨æ–­è¿</li>
      </ul>
    </div>
    <div class="en">
      <p>LLM generates at 30-80 tok/s, often <strong>faster than slow clients can consume</strong>. Buffer buildup â†’ OOM.</p>
      <p><strong>Backpressure strategies:</strong></p>
      <ul>
        <li><strong>Bounded buffer</strong>: max 100 tokens per connection</li>
        <li><strong>Drain event</strong>: pause when <code>res.write()</code> returns <code>false</code></li>
        <li><strong>Timeout disconnect</strong>: drop connection if buffer not drained in 30s</li>
      </ul>
    </div>
  </div>
</div>

<div class="highlight-green">
  <h4>æŒ‘æˆ˜ 6ï¼šæ–­ç‚¹ç»­ä¼ ä¸å®¹é”™ (Resumability)</h4>
  <div class="bilingual">
    <div class="zh">
      <p>ç½‘ç»œä¸ç¨³å®šæ—¶ï¼Œæµå¼è¿æ¥ä¸­æ–­ã€‚å¦‚ä½•ä»æ–­ç‚¹æ¢å¤ï¼Ÿ</p>
      <pre><code>// æ–¹æ¡ˆï¼šSSE Last-Event-ID + æœåŠ¡ç«¯ token ç¼“å­˜
// æœåŠ¡ç«¯ï¼šæ¯ä¸ª event æºå¸¦ id
res.write(`id: ${requestId}-${tokenIndex}\n`);
res.write(`data: ${JSON.stringify(chunk)}\n\n`);

// å®¢æˆ·ç«¯é‡è¿æ—¶è‡ªåŠ¨æºå¸¦ Last-Event-ID
// æœåŠ¡ç«¯æ£€æŸ¥ req.headers['last-event-id']
// ä» Redis/å†…å­˜ç¼“å­˜ä¸­æ¢å¤å·²ç”Ÿæˆçš„ tokens
const lastId = req.headers['last-event-id'];
if (lastId) {
  const [reqId, idx] = lastId.split('-');
  const cached = await redis.lrange(`stream:${reqId}`, +idx + 1, -1);
  // é‡æ”¾ç¼“å­˜çš„ tokensï¼Œç„¶åç»§ç»­ç”Ÿæˆ
  for (const token of cached) {
    res.write(`data: ${token}\n\n`);
  }
}</code></pre>
    </div>
    <div class="en">
      <p>SSE has built-in <code>Last-Event-ID</code> for resumption. Cache tokens in Redis keyed by request ID. On reconnect, replay missed tokens then continue generation.</p>
    </div>
  </div>
</div>

<h2>ğŸ”„ ç”Ÿäº§çº§æ¶æ„ï¼šAPI Gateway å±‚ / Production Gateway Layer</h2>

<div class="diagram">
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Load Balancer        â”‚
                    â”‚  (L7, long-lived conn)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                    â”‚                     â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚ API Server  â”‚     â”‚ API Server  â”‚      â”‚ API Server  â”‚
     â”‚  Instance 1 â”‚     â”‚  Instance 2 â”‚      â”‚  Instance 3 â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Stream Router         â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Rate Limiter       â”‚  â”‚
                    â”‚  â”‚ Auth / Quota       â”‚  â”‚
                    â”‚  â”‚ Model Router       â”‚  â”‚
                    â”‚  â”‚ Token Counter      â”‚  â”‚
                    â”‚  â”‚ Safety Filter      â”‚  â”‚
                    â”‚  â”‚ Usage Billing      â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                    â”‚                     â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚  GPU Pool   â”‚     â”‚  GPU Pool   â”‚      â”‚  GPU Pool   â”‚
     â”‚  (vLLM)     â”‚     â”‚  (TGI)      â”‚      â”‚  (Ollama)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>

<div class="highlight">
  <h4>ğŸ¯ å…³é”®è®¾è®¡å†³ç­–</h4>
  <table>
    <tr><th>ç»„ä»¶</th><th>è®¾è®¡é€‰æ‹©</th><th>åŸå› </th></tr>
    <tr>
      <td>LB â†’ API Server</td>
      <td>HTTP/2 é•¿è¿æ¥</td>
      <td>å‡å°‘ TLS æ¡æ‰‹å¼€é”€ï¼Œå¤šè·¯å¤ç”¨</td>
    </tr>
    <tr>
      <td>API â†’ GPU</td>
      <td>gRPC streaming</td>
      <td>å†…éƒ¨é€šä¿¡æ•ˆç‡é«˜ï¼ŒåŸç”Ÿæµæ”¯æŒ</td>
    </tr>
    <tr>
      <td>Token è®¡è´¹</td>
      <td>æµå¼ä¸­é—´ä»¶æ‹¦æˆª</td>
      <td>å®æ—¶ç»Ÿè®¡ input/output tokensï¼Œæµç»“æŸåå†™å…¥è®¡è´¹ç³»ç»Ÿ</td>
    </tr>
    <tr>
      <td>è¿æ¥ç®¡ç†</td>
      <td>30s idle timeout + heartbeat</td>
      <td>é˜²æ­¢è¿æ¥æ³„éœ²ï¼Œä½†ä¸è¯¯æ€æ´»è·ƒæµ</td>
    </tr>
  </table>
</div>

<h2>ğŸ“Š å®¹é‡ä¼°ç®— / Capacity Estimation</h2>

<div class="architecture">
  <h4>åœºæ™¯ï¼š100K DAU çš„ ChatBot æœåŠ¡</h4>
  <table>
    <tr><th>å‚æ•°</th><th>ä¼°ç®—</th></tr>
    <tr><td>å¹¶å‘å¯¹è¯</td><td>100K Ã— 5% åŒæ—¶åœ¨çº¿ = 5,000 å¹¶å‘æµ</td></tr>
    <tr><td>å¹³å‡æµæŒç»­æ—¶é—´</td><td>~10 ç§’ï¼ˆ200 tokens Ã— 50ms/tokenï¼‰</td></tr>
    <tr><td>æ¯ä¸ªè¿æ¥å†…å­˜</td><td>~50KBï¼ˆSSE buffer + metadataï¼‰</td></tr>
    <tr><td>æ€»å†…å­˜ (è¿æ¥)</td><td>5,000 Ã— 50KB = 250MB</td></tr>
    <tr><td>å¸¦å®½</td><td>5,000 Ã— 4KB/sï¼ˆtoken dataï¼‰= 20MB/s</td></tr>
    <tr><td>API Server</td><td>å•æœº ~2,000 å¹¶å‘ SSE â†’ 3 å°ï¼ˆ+ å†—ä½™ï¼‰</td></tr>
    <tr><td>GPU (A100)</td><td>~50 å¹¶å‘/GPU â†’ 100 GPUsï¼ˆæˆ–ç”¨é‡åŒ–å 200 GPUs ç­‰æ•ˆï¼‰</td></tr>
  </table>
  <p><strong>ç“¶é¢ˆåœ¨ GPUï¼Œä¸åœ¨ç½‘ç»œã€‚</strong>SSE è¿æ¥éå¸¸è½»é‡ï¼ŒçœŸæ­£çš„æˆæœ¬åœ¨ LLM æ¨ç†ã€‚</p>
</div>

<h2>ğŸ†š æµå¼ vs éæµå¼ï¼šä½•æ—¶ä¸ç”¨æµå¼ / When NOT to Stream</h2>

<table>
  <tr><th>åœºæ™¯</th><th>æ¨è</th><th>åŸå› </th></tr>
  <tr>
    <td>ç”¨æˆ·å¯¹è¯ï¼ˆChatBotï¼‰</td>
    <td>âœ… æµå¼</td>
    <td>TTFT ä½“éªŒå…³é”®</td>
  </tr>
  <tr>
    <td>Batch æ•°æ®å¤„ç†</td>
    <td>âŒ éæµå¼</td>
    <td>ä¸éœ€è¦å®æ—¶åé¦ˆï¼ŒBatch API æ›´ä¾¿å®œ (50% off)</td>
  </tr>
  <tr>
    <td>Tool Calling / Function</td>
    <td>âš ï¸ éƒ¨åˆ†æµå¼</td>
    <td>æ˜¾ç¤º"æ­£åœ¨è°ƒç”¨å·¥å…·â€¦"çŠ¶æ€ï¼Œä½†ç»“æœéœ€å®Œæ•´ JSON</td>
  </tr>
  <tr>
    <td>Agent å¤šæ­¥æ¨ç†</td>
    <td>âš ï¸ äº‹ä»¶æµ</td>
    <td>æµå¼å‘é€æ­¥éª¤çŠ¶æ€ï¼ˆthinking/tool_use/resultï¼‰ï¼Œä¸æ˜¯ token çº§</td>
  </tr>
  <tr>
    <td>ç»“æ„åŒ–æ•°æ®æå–</td>
    <td>âŒ éæµå¼</td>
    <td>éœ€è¦å®Œæ•´ JSON æ‰èƒ½ä½¿ç”¨ï¼Œæµå¼å¢åŠ å¤æ‚åº¦æ— æ”¶ç›Š</td>
  </tr>
</table>

<h2>ğŸ“š å¯å­¦ä¹ çš„æ¨¡å¼ / Patterns to Apply</h2>

<div class="highlight-green">
  <h4>Pattern 1: SSE + Heartbeat ä¿æ´»</h4>
  <p>ä»»ä½•é•¿è¿æ¥åœºæ™¯éƒ½éœ€è¦å¿ƒè·³ã€‚SSE æ³¨é‡Š (<code>: heartbeat</code>) æ˜¯é›¶æˆæœ¬ä¿æ´»ï¼Œä¸ä¼šè§¦å‘å®¢æˆ·ç«¯äº‹ä»¶å¤„ç†ã€‚é€‚ç”¨äºï¼šAPI Gateway è¶…æ—¶æ£€æµ‹ã€CDN keep-aliveã€‚</p>

  <h4>Pattern 2: Abort Propagationï¼ˆå–æ¶ˆä¼ æ’­ï¼‰</h4>
  <p>å®¢æˆ·ç«¯ AbortController â†’ API Server â†’ LLM Engineã€‚å–æ¶ˆä¿¡å·å¿…é¡»å…¨é“¾è·¯ä¼ æ’­ï¼Œå¦åˆ™æµªè´¹ GPU èµ„æºã€‚è¿™æ˜¯ Go çš„ <code>context.Context</code> è®¾è®¡å“²å­¦ã€‚</p>

  <h4>Pattern 3: Dual-Mode APIï¼ˆåŒæ¨¡å¼ APIï¼‰</h4>
  <p>åŒä¸€ä¸ª endpointï¼Œ<code>stream: true</code> è¿”å› SSEï¼Œ<code>stream: false</code> è¿”å›æ ‡å‡† JSONã€‚å†…éƒ¨å…±äº«åŒä¸€ä¸ª LLM è°ƒç”¨é€»è¾‘ï¼Œåªæ˜¯è¾“å‡ºé€‚é…å±‚ä¸åŒã€‚OpenAI/Anthropic éƒ½æ˜¯è¿™ç§è®¾è®¡ã€‚</p>

  <h4>Pattern 4: æµå¼ä¸­é—´ä»¶ç®¡é“</h4>
  <p>Token Counter â†’ Safety Filter â†’ PII Redactor â†’ SSE Writerã€‚æ¯ä¸ªä¸­é—´ä»¶æ˜¯ä¸€ä¸ª Transform Streamï¼Œå¯æ’æ‹”ç»„åˆã€‚Node.js Streams / Go io.Reader ç®¡é“å®Œç¾é€‚é…ã€‚</p>
</div>

<h2>ğŸ¤ é¢è¯•é«˜é¢‘é¢˜ / Interview Questions</h2>

<div class="highlight-red">
  <h4>Q1: è®¾è®¡ä¸€ä¸ªæ”¯æŒæµå¼è¾“å‡ºçš„ ChatBot APIï¼Œä½ ä¼šé€‰ä»€ä¹ˆåè®®ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ</h4>
  <p><strong>ç­”é¢˜æ¡†æ¶ï¼š</strong></p>
  <ol>
    <li>åˆ†æéœ€æ±‚ï¼šå•å‘æ¨é€ â†’ SSE æœ€åˆé€‚</li>
    <li>å¯¹æ¯” WebSocketï¼šè¿‡é‡ï¼Œéœ€è¦çŠ¶æ€ç®¡ç†ï¼ŒLB éœ€ sticky session</li>
    <li>SSE ä¼˜åŠ¿ï¼šHTTP åŸç”Ÿã€è‡ªåŠ¨é‡è¿ã€CDN å‹å¥½ã€curl å¯è°ƒè¯•</li>
    <li>è¡¥å……ï¼šå†…éƒ¨æœåŠ¡é—´ç”¨ gRPC streamingï¼ˆé«˜æ•ˆ + ç±»å‹å®‰å…¨ï¼‰</li>
  </ol>

  <h4>Q2: æµå¼è¾“å‡ºæ—¶å¦‚ä½•åš Safety Filteringï¼Ÿ</h4>
  <p><strong>ç­”é¢˜æ¡†æ¶ï¼š</strong></p>
  <ol>
    <li>æŒ‘æˆ˜ï¼šå•ä¸ª token æ— è¯­ä¹‰ï¼Œä½†ä¸èƒ½ç­‰å®Œæˆåæ‰è¿‡æ»¤ï¼ˆç”¨æˆ·å·²ç»çœ‹åˆ°äº†ï¼‰</li>
    <li>ä¸‰å±‚æ–¹æ¡ˆï¼štoken çº§å…³é”®è¯ â†’ æ»‘åŠ¨çª—å£åˆ†ç±» â†’ å®Œæˆåæ·±åº¦æ£€æŸ¥</li>
    <li>å…œåº•ï¼šPost-hoc æ£€æµ‹åˆ°é—®é¢˜åå‘ <code>retract</code> eventï¼Œå®¢æˆ·ç«¯åˆ é™¤/æ›¿æ¢å·²æ˜¾ç¤ºå†…å®¹</li>
    <li>æƒè¡¡ï¼šå»¶è¿Ÿ vs å®‰å…¨ã€‚å…³é”®è¯åŒ¹é…é›¶å»¶è¿Ÿä½†æ¼æ£€é«˜ï¼ŒLLM åˆ¤æ–­å‡†ä½†å¢åŠ å»¶è¿Ÿ</li>
  </ol>

  <h4>Q3: ç”¨æˆ·ç‚¹å‡» "Stop Generating"ï¼Œç³»ç»Ÿéœ€è¦åšä»€ä¹ˆï¼Ÿ</h4>
  <p><strong>ç­”é¢˜æ¡†æ¶ï¼ˆå…¨é“¾è·¯ï¼‰ï¼š</strong></p>
  <ol>
    <li>å®¢æˆ·ç«¯ï¼šAbortController.abort() â†’ æ–­å¼€ fetch/SSE è¿æ¥</li>
    <li>API Gatewayï¼šæ£€æµ‹åˆ°è¿æ¥å…³é—­ â†’ å‘é€ cancel åˆ°åç«¯</li>
    <li>LLM Engineï¼šåœæ­¢å½“å‰ KV cache çš„æ¨ç† â†’ é‡Šæ”¾ GPU æ˜¾å­˜</li>
    <li>è®¡è´¹ï¼šåªè®¡ç®—å·²ç”Ÿæˆçš„ tokensï¼ˆä¸è®¡å–æ¶ˆåçš„éƒ¨åˆ†ï¼‰</li>
    <li>æ—¥å¿—ï¼šè®°å½• partial response + cancel reasonï¼ˆåˆ†æç”¨æˆ·è¡Œä¸ºï¼‰</li>
  </ol>

  <h4>Q4: å¦‚ä½•å¤„ç†æµå¼è¾“å‡ºä¸­çš„ç½‘ç»œä¸­æ–­å’Œæ¢å¤ï¼Ÿ</h4>
  <p><strong>ç­”é¢˜æ¡†æ¶ï¼š</strong></p>
  <ol>
    <li>SSE å†…ç½® <code>Last-Event-ID</code> + è‡ªåŠ¨é‡è¿æœºåˆ¶</li>
    <li>æœåŠ¡ç«¯ï¼šæ¯ä¸ª event å¸¦ idï¼Œtoken ç¼“å­˜åˆ° Redisï¼ˆTTL 5minï¼‰</li>
    <li>é‡è¿æ—¶ï¼šä» Last-Event-ID ä¹‹åçš„ä½ç½®é‡æ”¾ç¼“å­˜ tokens</li>
    <li>è¶…æ—¶ç­–ç•¥ï¼šå¦‚æœ LLM å·²å®Œæˆç”Ÿæˆï¼Œä»ç¼“å­˜ç›´æ¥è¿”å›å‰©ä½™ tokensï¼›å¦‚æœæœªå®Œæˆï¼Œç»§ç»­ç”Ÿæˆ</li>
    <li>æƒè¡¡ï¼šç¼“å­˜å ç”¨ vs ç”¨æˆ·ä½“éªŒã€‚å¤§å¤šæ•°åœºæ™¯ä¸‹ 5min TTL è¶³å¤Ÿ</li>
  </ol>
</div>

<div class="series-nav">
  <h3>ğŸ“š ç³»åˆ—å›é¡¾ / Series Recap</h3>
  <p>
    <a href="2026-02-20-token-limit-handling-strategies.html">Day 1: Token é™åˆ¶å¤„ç†</a> â€” å…­å¤§ç­–ç•¥ç ´è§£ä¸Šä¸‹æ–‡çª—å£<br>
    <a href="2026-02-21-llm-cost-optimization.html">Day 2: æˆæœ¬ä¼˜åŒ–</a> â€” ç¼“å­˜/è·¯ç”±/åˆ†æµèŠ‚çœ 91%<br>
    <a href="2026-02-22-hallucination-detection-mitigation.html">Day 3: Hallucination</a> â€” å…­å±‚æ£€æµ‹ + å…­å±‚é˜²å¾¡<br>
    <strong>Day 4: æµå¼è¾“å‡ºæ¶æ„</strong> â€” SSE åè®® + å…­å¤§å·¥ç¨‹æŒ‘æˆ˜ â† ä»Šå¤©<br>
    Day 5: Multi-turn å¯¹è¯çŠ¶æ€ç®¡ç† â€” æ˜å¤©ï¼
  </p>
</div>

<div class="follow-up">
  <h3>ğŸ’¬ åç»­è®¨è®º / Follow-up</h3>
  <p>ä½ å¯ä»¥é—®æˆ‘ï¼š</p>
  <ul>
    <li>å¦‚ä½•åœ¨ Next.js / Edge Runtime ä¸­å®ç° SSEï¼Ÿ</li>
    <li>gRPC streaming vs SSE åœ¨å†…éƒ¨å¾®æœåŠ¡ä¸­çš„é€‰æ‹©ï¼Ÿ</li>
    <li>OpenClaw çš„æµå¼æ¶æ„æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ</li>
    <li>Anthropic çš„ SSE æ ¼å¼å’Œ OpenAI æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</li>
  </ul>
</div>

</body>
</html>
