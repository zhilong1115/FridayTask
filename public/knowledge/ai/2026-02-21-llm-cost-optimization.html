<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-21 - LLM æˆæœ¬ä¼˜åŒ–ï¼šç¼“å­˜ã€è·¯ç”±ã€å°æ¨¡å‹åˆ†æµ</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.7; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 15px 0; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; border-bottom: 2px solid #f9ab00; padding-bottom: 10px; }
    h2 { color: #1a73e8; margin-top: 35px; }
    h3 { color: #5f6368; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 13px; }
    code { font-family: 'SF Mono', Monaco, monospace; }
    .highlight { background: #fff3cd; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-blue { background: #e3f2fd; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .highlight-green { background: #e8f5e9; padding: 12px 15px; border-radius: 8px; margin: 10px 0; }
    .diagram { text-align: left; margin: 20px 0; background: #f8f9fa; padding: 15px; border-radius: 8px; overflow-x: auto; }
    .follow-up { margin-top: 30px; padding: 15px; border: 1px dashed #dadce0; border-radius: 8px; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    th, td { border: 1px solid #dadce0; padding: 10px 12px; text-align: left; }
    th { background: #f8f9fa; font-weight: 600; }
    .series-nav { background: #e8eaf6; padding: 12px; border-radius: 8px; margin: 15px 0; }
    .cost-tag { display: inline-block; background: #ef5350; color: white; padding: 2px 8px; border-radius: 4px; font-size: 12px; }
    .save-tag { display: inline-block; background: #4caf50; color: white; padding: 2px 8px; border-radius: 4px; font-size: 12px; }
    @media (max-width: 768px) { .bilingual { grid-template-columns: 1fr; } }
  </style>
</head>
<body>
  <div class="series-nav">
    ğŸ“š <strong>LLM ç‰¹æœ‰é—®é¢˜ç³»åˆ—</strong> Day 2/5 &nbsp;|&nbsp; 
    <a href="2026-02-20-token-limit-handling-strategies.html">â† Day 1: Token é™åˆ¶å¤„ç†</a> &nbsp;|&nbsp; 
    Day 3: Hallucination æ£€æµ‹ â†’
  </div>

  <h1>ğŸ’° LLM æˆæœ¬ä¼˜åŒ–ï¼šç¼“å­˜ã€è·¯ç”±ã€å°æ¨¡å‹åˆ†æµ</h1>
  <p>Phase 2.3 LLM ç‰¹æœ‰é—®é¢˜ Â· Day 2 Â· 2026-02-21</p>

  <h2>ğŸ“– ä¸ºä»€ä¹ˆæˆæœ¬ä¼˜åŒ–æ˜¯ LLM å·¥ç¨‹çš„æ ¸å¿ƒæŠ€èƒ½</h2>
  <div class="bilingual">
    <div class="zh">
      <p>GPT-4 çº§æ¨¡å‹å¤„ç† 100K è¯·æ±‚/å¤©ï¼Œä»… API è´¹ç”¨å°±å¯èƒ½è¾¾ $3,000/å¤©ã€‚ä½†ç°å®ä¸­ <strong>60-80% çš„è¯·æ±‚å…¶å®ä¸éœ€è¦æœ€å¼ºæ¨¡å‹</strong>ã€‚</p>
      <p>æˆæœ¬ä¼˜åŒ–ä¸æ˜¯"çœé’±"é‚£ä¹ˆç®€å•â€”â€”å®ƒæ˜¯ä¸€ä¸ª<strong>ç³»ç»Ÿè®¾è®¡é—®é¢˜</strong>ï¼šå¦‚ä½•åœ¨è´¨é‡ã€å»¶è¿Ÿã€æˆæœ¬ä¸‰è§’ä¸­æ‰¾åˆ°æœ€ä¼˜è§£ã€‚</p>
      <p>é¢è¯•ä¸­ï¼Œ"å¦‚æœä½ çš„ LLM ç³»ç»Ÿæˆæœ¬å¤ªé«˜ï¼Œä½ æ€ä¹ˆä¼˜åŒ–ï¼Ÿ"æ˜¯é«˜é¢‘é—®é¢˜ã€‚ä»Šå¤©ç»™ä½ ä¸€ä¸ªå®Œæ•´çš„å…­å±‚ä¼˜åŒ–æ¡†æ¶ã€‚</p>
    </div>
    <div class="en">
      <p>A GPT-4 class model handling 100K requests/day can cost ~$3,000/day in API fees alone. But in practice, <strong>60-80% of requests don't need the strongest model</strong>.</p>
      <p>Cost optimization isn't just "saving money"â€”it's a <strong>system design problem</strong>: finding the optimal point in the quality-latency-cost triangle.</p>
      <p>In interviews, "How would you optimize costs for your LLM system?" is a top question. Today we cover a complete six-layer optimization framework.</p>
    </div>
  </div>

  <h2>ğŸ—ï¸ å…­å±‚æˆæœ¬ä¼˜åŒ–æ¶æ„</h2>
  <div class="diagram">
<pre style="background: #263238; color: #eeffff; font-size: 12px;">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Request Arrives                        â”‚
â”‚                         â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Layer 1: EXACT CACHE                         â”‚       â”‚
â”‚  â”‚  Hash(prompt) â†’ cached response               â”‚       â”‚
â”‚  â”‚  Hit rate: 15-30%  â”‚  Savings: 100% per hit   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚
â”‚             â”‚ miss                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Layer 2: SEMANTIC CACHE                     â”‚         â”‚
â”‚  â”‚  Embed(prompt) â†’ nearest neighbor â†’ response â”‚         â”‚
â”‚  â”‚  Hit rate: 10-25%  â”‚  Savings: ~100% per hit â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚
â”‚             â”‚ miss                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Layer 3: PROMPT OPTIMIZATION                â”‚         â”‚
â”‚  â”‚  Compress / Truncate / Template              â”‚         â”‚
â”‚  â”‚  Token reduction: 30-60%                     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚
â”‚             â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Layer 4: MODEL ROUTER                       â”‚         â”‚
â”‚  â”‚  Classify complexity â†’ route to model         â”‚         â”‚
â”‚  â”‚  70% â†’ small model (10x cheaper)             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚
â”‚             â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Layer 5: CASCADE / FALLBACK                 â”‚         â”‚
â”‚  â”‚  Small â†’ verify â†’ Large if uncertain          â”‚         â”‚
â”‚  â”‚  Only 20-30% escalate                         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚
â”‚             â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Layer 6: BATCHING & ASYNC                   â”‚         â”‚
â”‚  â”‚  Group requests, defer non-urgent             â”‚         â”‚
â”‚  â”‚  Throughput â†‘, per-request cost â†“             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Combined savings: 70-90% cost reduction
</pre>
  </div>

  <h2>ğŸ—„ï¸ Layer 1 & 2: ç¼“å­˜ç­–ç•¥</h2>
  
  <h3>Exact Cache â€” ç²¾ç¡®åŒ¹é…ç¼“å­˜</h3>
  <div class="bilingual">
    <div class="zh">
      <p><strong>åŸç†</strong>ï¼šå¯¹ prompt åš hashï¼Œå®Œå…¨ä¸€æ ·çš„è¯·æ±‚ç›´æ¥è¿”å›ç¼“å­˜ç»“æœã€‚</p>
      <p><strong>é€‚ç”¨åœºæ™¯</strong>ï¼šåˆ†ç±»ä»»åŠ¡ã€å›ºå®šæ¨¡æ¿ç”Ÿæˆã€FAQ é—®ç­”ã€‚</p>
      <p><strong>ä¸é€‚ç”¨</strong>ï¼šå¯¹è¯åœºæ™¯ï¼ˆæ¯æ¬¡ä¸Šä¸‹æ–‡ä¸åŒï¼‰ï¼Œåˆ›æ„ç”Ÿæˆï¼ˆéœ€è¦å¤šæ ·æ€§ï¼‰ã€‚</p>
      <p><strong>å®ç°è¦ç‚¹</strong>ï¼š</p>
      <ul>
        <li>Key = hash(model + prompt + temperature + system_prompt)</li>
        <li>TTL æ ¹æ®ä¸šåŠ¡è®¾ç½®ï¼ˆFAQ: 24hï¼Œå®æ—¶æ•°æ®: 5minï¼‰</li>
        <li>Redis/Memcached å­˜å‚¨ï¼Œåºåˆ—åŒ–å®Œæ•´å“åº”</li>
      </ul>
    </div>
    <div class="en">
      <p><strong>Principle</strong>: Hash the prompt; return cached response for exact matches.</p>
      <p><strong>Good for</strong>: Classification, template generation, FAQ.</p>
      <p><strong>Not for</strong>: Conversations (context varies), creative generation (needs diversity).</p>
      <p><strong>Key details</strong>:</p>
      <ul>
        <li>Key = hash(model + prompt + temperature + system_prompt)</li>
        <li>TTL varies by use case (FAQ: 24h, real-time data: 5min)</li>
        <li>Redis/Memcached storage, serialize full response</li>
      </ul>
    </div>
  </div>

  <pre><code>// Exact Cache Implementation
import { createHash } from 'crypto';
import Redis from 'ioredis';

class LLMExactCache {
  private redis: Redis;
  
  async get(request: LLMRequest): Promise<LLMResponse | null> {
    const key = this.buildKey(request);
    const cached = await this.redis.get(key);
    if (cached) {
      metrics.increment('cache.exact.hit');
      return JSON.parse(cached);
    }
    metrics.increment('cache.exact.miss');
    return null;
  }
  
  private buildKey(req: LLMRequest): string {
    // Include ALL parameters that affect output
    const payload = JSON.stringify({
      model: req.model,
      messages: req.messages,
      temperature: req.temperature,
      system: req.systemPrompt,
      // Exclude: stream, max_tokens (if we want partial match)
    });
    return `llm:exact:${createHash('sha256').update(payload).digest('hex')}`;
  }
  
  async set(request: LLMRequest, response: LLMResponse, ttl: number) {
    // Only cache deterministic requests (temperature = 0)
    if (request.temperature > 0) return;
    const key = this.buildKey(request);
    await this.redis.setex(key, ttl, JSON.stringify(response));
  }
}</code></pre>

  <h3>Semantic Cache â€” è¯­ä¹‰ç¼“å­˜</h3>
  <div class="bilingual">
    <div class="zh">
      <p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼š"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·" å’Œ "å¤©æ°”å¦‚ä½•" æ˜¯åŒä¸€ä¸ªé—®é¢˜ï¼Œåº”è¯¥è¿”å›åŒä¸€ä¸ªç¼“å­˜ã€‚</p>
      <p><strong>å®ç°</strong>ï¼šæŠŠ prompt è½¬æˆ embeddingï¼Œåœ¨å‘é‡åº“ä¸­æ‰¾ <strong>cosine similarity > threshold</strong> çš„æœ€è¿‘é‚»ã€‚</p>
      <p><strong>å…³é”®å‚æ•°</strong>ï¼š</p>
      <ul>
        <li><strong>ç›¸ä¼¼åº¦é˜ˆå€¼</strong>ï¼š0.95+ ä¿å®ˆï¼ˆé«˜ç²¾åº¦ï¼‰ï¼Œ0.90 æ¿€è¿›ï¼ˆé«˜å‘½ä¸­ç‡ï¼‰</li>
        <li><strong>Embedding æ¨¡å‹</strong>ï¼šç”¨å°æ¨¡å‹å¦‚ text-embedding-3-smallï¼ˆ$0.02/1M tokensï¼‰</li>
        <li><strong>å·¥å…·</strong>ï¼šGPTCache, LangChain CacheBackedEmbeddings, è‡ªå»º</li>
      </ul>
    </div>
    <div class="en">
      <p><strong>Core idea</strong>: "What's the weather today" and "How's the weather" are the same questionâ€”return the same cache.</p>
      <p><strong>Implementation</strong>: Embed the prompt, find nearest neighbor in vector store with <strong>cosine similarity > threshold</strong>.</p>
      <p><strong>Key parameters</strong>:</p>
      <ul>
        <li><strong>Similarity threshold</strong>: 0.95+ conservative (high precision), 0.90 aggressive (high hit rate)</li>
        <li><strong>Embedding model</strong>: Use small models like text-embedding-3-small ($0.02/1M tokens)</li>
        <li><strong>Tools</strong>: GPTCache, LangChain CacheBackedEmbeddings, custom</li>
      </ul>
    </div>
  </div>

  <div class="highlight">
    <h4>âš ï¸ Semantic Cache çš„é™·é˜±</h4>
    <p><strong>False positive æ¯” false negative å±é™©å¾—å¤š</strong>ï¼šè¿”å›é”™è¯¯ç¼“å­˜ = ç»™ç”¨æˆ·é”™è¯¯ç­”æ¡ˆã€‚æ‰€ä»¥é˜ˆå€¼å®é«˜å‹¿ä½ã€‚</p>
    <p><strong>ä¸Šä¸‹æ–‡æ•æ„Ÿ</strong>ï¼šåŒä¸€ä¸ªé—®é¢˜åœ¨ä¸åŒå¯¹è¯ä¸Šä¸‹æ–‡ä¸­å¯èƒ½æœ‰ä¸åŒç­”æ¡ˆã€‚ç¼“å­˜ key éœ€è¦åŒ…å«å…³é”®ä¸Šä¸‹æ–‡ã€‚</p>
    <p><strong>æ—¶æ•ˆæ€§</strong>ï¼šçŸ¥è¯†æœ‰ä¿è´¨æœŸï¼ŒTTL + ä¸»åŠ¨å¤±æ•ˆç»“åˆä½¿ç”¨ã€‚</p>
  </div>

  <h2>ğŸ”€ Layer 4: Model Router â€” æ¨¡å‹è·¯ç”±</h2>
  
  <div class="bilingual">
    <div class="zh">
      <p>è¿™æ˜¯æˆæœ¬ä¼˜åŒ–ä¸­<strong>ROI æœ€é«˜</strong>çš„ç­–ç•¥ã€‚æ ¸å¿ƒæ€æƒ³ï¼š</p>
      <blockquote>"ä¸æ˜¯æ‰€æœ‰é—®é¢˜éƒ½éœ€è¦åšå£«æ¥å›ç­”ï¼Œå¤§éƒ¨åˆ†é—®é¢˜æœ¬ç§‘ç”Ÿå°±å¤Ÿäº†ã€‚"</blockquote>
      <p>ä¸€ä¸ªå¥½çš„ Router å¯ä»¥æŠŠ 70% çš„è¯·æ±‚è·¯ç”±åˆ°ä¾¿å®œ 10-50 å€çš„å°æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒ 95%+ çš„è´¨é‡ã€‚</p>
    </div>
    <div class="en">
      <p>This is the <strong>highest-ROI</strong> cost optimization strategy. Core idea:</p>
      <blockquote>"Not every question needs a PhD to answerâ€”most can be handled by an undergrad."</blockquote>
      <p>A good Router can route 70% of requests to models that are 10-50x cheaper, while maintaining 95%+ quality.</p>
    </div>
  </div>

  <h3>ä¸‰ç§ Router æ¶æ„</h3>

  <table>
    <tr>
      <th>æ–¹å¼</th>
      <th>åŸç†</th>
      <th>å»¶è¿Ÿå¼€é”€</th>
      <th>å‡†ç¡®ç‡</th>
      <th>é€‚ç”¨åœºæ™¯</th>
    </tr>
    <tr>
      <td><strong>Rule-based</strong></td>
      <td>å…³é”®è¯/é•¿åº¦/æ­£åˆ™</td>
      <td>~0ms</td>
      <td>60-75%</td>
      <td>ç®€å•åˆ†ç±»ã€MVP</td>
    </tr>
    <tr>
      <td><strong>Classifier-based</strong></td>
      <td>è®­ç»ƒå°åˆ†ç±»å™¨é¢„æµ‹éš¾åº¦</td>
      <td>5-20ms</td>
      <td>85-92%</td>
      <td>ç”Ÿäº§ç¯å¢ƒä¸»æµ</td>
    </tr>
    <tr>
      <td><strong>LLM-based</strong></td>
      <td>ç”¨å° LLM è¯„ä¼°å¤æ‚åº¦</td>
      <td>100-500ms</td>
      <td>90-95%</td>
      <td>å¤æ‚åº¦é«˜/å¤šç»´è¯„ä¼°</td>
    </tr>
  </table>

  <pre><code>// Production Model Router
interface RouterConfig {
  models: ModelTier[];
  classifier: ComplexityClassifier;
  fallbackModel: string;
}

interface ModelTier {
  name: string;           // e.g., "gpt-4o-mini"
  costPer1kTokens: number; // e.g., 0.00015
  maxComplexity: number;   // 0-1 score threshold
  capabilities: string[];  // ["chat", "code", "reasoning"]
}

class ModelRouter {
  private tiers: ModelTier[] = [
    { name: 'gpt-4o-mini',     costPer1kTokens: 0.00015, maxComplexity: 0.3,
      capabilities: ['chat', 'classify', 'extract'] },
    { name: 'claude-3.5-haiku', costPer1kTokens: 0.001,  maxComplexity: 0.6,
      capabilities: ['chat', 'code', 'summarize'] },
    { name: 'claude-sonnet-4',  costPer1kTokens: 0.003,  maxComplexity: 0.85,
      capabilities: ['chat', 'code', 'reasoning', 'analysis'] },
    { name: 'claude-opus-4',    costPer1kTokens: 0.015,  maxComplexity: 1.0,
      capabilities: ['chat', 'code', 'reasoning', 'creative', 'complex'] },
  ];

  async route(request: LLMRequest): Promise<string> {
    // Step 1: Check capability requirements
    const requiredCaps = this.detectCapabilities(request);
    
    // Step 2: Estimate complexity (tiny classifier, <5ms)
    const complexity = await this.classifier.predict(request.messages);
    
    // Step 3: Find cheapest model that meets both requirements
    for (const tier of this.tiers) {
      if (complexity <= tier.maxComplexity &&
          requiredCaps.every(c => tier.capabilities.includes(c))) {
        metrics.record('router.model', tier.name);
        return tier.name;
      }
    }
    
    return this.fallbackModel; // Always have a fallback
  }
  
  private detectCapabilities(req: LLMRequest): string[] {
    const text = req.messages.map(m => m.content).join(' ');
    const caps: string[] = ['chat'];
    
    if (/code|function|implement|debug|```/.test(text)) caps.push('code');
    if (/why|explain|analyze|compare|reason/.test(text)) caps.push('reasoning');
    if (/write.*story|creative|poem|essay/.test(text)) caps.push('creative');
    if (text.length > 2000 || /complex|detailed|thorough/.test(text)) caps.push('complex');
    
    return caps;
  }
}</code></pre>

  <div class="highlight-blue">
    <h4>ğŸ’¡ Router è®­ç»ƒæ•°æ®ä»å“ªæ¥ï¼Ÿ</h4>
    <p><strong>Bootstrap æ–¹æ³•</strong>ï¼šå…ˆè®©æ‰€æœ‰è¯·æ±‚éƒ½èµ°å¤§æ¨¡å‹ï¼ŒåŒæ—¶è®°å½• (prompt, quality_score)ã€‚ç§¯ç´¯è¶³å¤Ÿæ•°æ®åï¼Œè®­ç»ƒ classifier é¢„æµ‹"è¿™ä¸ª prompt ç”¨å°æ¨¡å‹èƒ½å¦è¾¾åˆ° 90+ è´¨é‡"ã€‚</p>
    <p><strong>æŒç»­å­¦ä¹ </strong>ï¼šä¸Šçº¿åï¼Œå¯¹å°æ¨¡å‹çš„è¾“å‡ºé‡‡æ ·è´¨æ£€ï¼ˆäººå·¥æˆ– LLM-as-Judgeï¼‰ï¼Œä¸æ–­æ›´æ–°åˆ†ç±»å™¨ã€‚</p>
  </div>

  <h2>ğŸ“‰ Layer 5: Model Cascade â€” çº§è”ç­–ç•¥</h2>

  <div class="bilingual">
    <div class="zh">
      <p><strong>ä¸ Router çš„åŒºåˆ«</strong>ï¼šRouter æ˜¯"å…ˆåˆ¤æ–­å†æ‰§è¡Œ"ï¼ˆä¸€æ¬¡é€‰æ‹©ï¼‰ï¼ŒCascade æ˜¯"å…ˆå°è¯•å†å‡çº§"ï¼ˆæ¸è¿›å¼ï¼‰ã€‚</p>
      <p><strong>æ ¸å¿ƒæµç¨‹</strong>ï¼š</p>
      <ol>
        <li>å…ˆç”¨å°æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ</li>
        <li>ç”¨ <strong>Verifier</strong> è¯„ä¼°ç­”æ¡ˆè´¨é‡/ç½®ä¿¡åº¦</li>
        <li>å¦‚æœä¸å¤Ÿå¥½ï¼Œå‡çº§åˆ°æ›´å¤§æ¨¡å‹</li>
      </ol>
      <p><strong>ä¼˜åŠ¿</strong>ï¼šä¸éœ€è¦è®­ç»ƒ Routerï¼Œå¤©ç„¶è‡ªé€‚åº”ã€‚</p>
      <p><strong>åŠ£åŠ¿</strong>ï¼šworst case å»¶è¿Ÿç¿»å€ï¼ˆä¸¤æ¬¡è°ƒç”¨ï¼‰ï¼Œéœ€è¦å¥½çš„ Verifierã€‚</p>
    </div>
    <div class="en">
      <p><strong>Difference from Router</strong>: Router is "decide then execute" (one choice). Cascade is "try then upgrade" (progressive).</p>
      <p><strong>Core flow</strong>:</p>
      <ol>
        <li>Generate answer with small model first</li>
        <li>Use <strong>Verifier</strong> to assess answer quality/confidence</li>
        <li>If insufficient, escalate to larger model</li>
      </ol>
      <p><strong>Pro</strong>: No router training needed, naturally adaptive.</p>
      <p><strong>Con</strong>: Worst-case latency doubles (two calls), needs good Verifier.</p>
    </div>
  </div>

  <pre><code>// Model Cascade with Verification
class ModelCascade {
  private models = ['gpt-4o-mini', 'claude-3.5-haiku', 'claude-sonnet-4'];
  
  async generate(request: LLMRequest): Promise<CascadeResult> {
    for (let i = 0; i < this.models.length; i++) {
      const model = this.models[i];
      const response = await this.llm.call({ ...request, model });
      
      // Last model = always accept
      if (i === this.models.length - 1) {
        return { response, model, escalated: i > 0 };
      }
      
      // Verify quality
      const verdict = await this.verify(request, response);
      
      if (verdict.confident && verdict.quality >= 0.85) {
        metrics.record('cascade.accepted_at', model);
        return { response, model, escalated: false };
      }
      
      metrics.record('cascade.escalated_from', model);
      // Continue to next tier...
    }
  }
  
  private async verify(req: LLMRequest, resp: LLMResponse): Promise<Verdict> {
    // Strategy 1: Self-consistency â€” ask same question twice, compare
    // Strategy 2: LLM-as-Judge â€” small model evaluates the answer
    // Strategy 3: Heuristic â€” check format, length, keywords
    // Strategy 4: Task-specific â€” for code: parse/compile, for math: verify
    
    // Production combo: heuristic first (free), then LLM judge if needed
    const heuristic = this.heuristicCheck(req, resp);
    if (heuristic.confident) return heuristic;
    
    return this.llmJudge(req, resp); // Costs ~1/10 of generation
  }
}</code></pre>

  <div class="highlight-green">
    <h4>ğŸ¯ Routing vs Cascade: ä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ</h4>
    <table>
      <tr><th>ç»´åº¦</th><th>Model Router</th><th>Model Cascade</th></tr>
      <tr><td>å»¶è¿Ÿè¦æ±‚</td><td>âœ… ä½å»¶è¿Ÿï¼ˆä¸€æ¬¡è°ƒç”¨ï¼‰</td><td>âš ï¸ å¯èƒ½é«˜å»¶è¿Ÿ</td></tr>
      <tr><td>å®ç°å¤æ‚åº¦</td><td>âš ï¸ éœ€è¦è®­ç»ƒæ•°æ®</td><td>âœ… ç›¸å¯¹ç®€å•</td></tr>
      <tr><td>è´¨é‡ä¿è¯</td><td>âš ï¸ ä¾èµ– Router å‡†ç¡®ç‡</td><td>âœ… æœ‰ Verifier å…œåº•</td></tr>
      <tr><td>æœ€ä½³åœºæ™¯</td><td>æˆç†Ÿç³»ç»Ÿã€é«˜ QPS</td><td>æ–°ç³»ç»Ÿã€è´¨é‡ä¼˜å…ˆ</td></tr>
    </table>
    <p><strong>æœ€ä½³å®è·µ</strong>ï¼šå…ˆç”¨ Cascade ä¸Šçº¿æ”¶é›†æ•°æ®ï¼Œç„¶åè®­ç»ƒ Router æ›¿æ¢ã€‚ä¸¤è€…ä¹Ÿå¯ä»¥ç»„åˆï¼šRouter åšç²—åˆ†ï¼Œå¯¹è¾¹ç•Œ case ç”¨ Cascadeã€‚</p>
  </div>

  <h2>ğŸ”§ Layer 3 & 6: å…¶ä»–ä¼˜åŒ–æ‰‹æ®µ</h2>

  <h3>Prompt ä¼˜åŒ– â€” ç«‹ç«¿è§å½±</h3>
  <table>
    <tr>
      <th>æŠ€æœ¯</th>
      <th>æ•ˆæœ</th>
      <th>å®ç°</th>
    </tr>
    <tr>
      <td><strong>System Prompt å‹ç¼©</strong></td>
      <td>Token -40-60%</td>
      <td>å»å†—ä½™æŒ‡ä»¤ï¼Œç”¨ç®€æ´æ ¼å¼</td>
    </tr>
    <tr>
      <td><strong>Few-shot â†’ Zero-shot</strong></td>
      <td>Token -60-80%</td>
      <td>å¼ºæ¨¡å‹é€šå¸¸ zero-shot å°±å¤Ÿ</td>
    </tr>
    <tr>
      <td><strong>è¾“å‡ºæ ¼å¼çº¦æŸ</strong></td>
      <td>Output token -50%</td>
      <td><code>max_tokens</code> + ç»“æ„åŒ–è¾“å‡º</td>
    </tr>
    <tr>
      <td><strong>LLMLingua å‹ç¼©</strong></td>
      <td>Token -70-90%</td>
      <td>å°æ¨¡å‹åˆ é™¤ä½ä¿¡æ¯é‡ token</td>
    </tr>
  </table>

  <h3>æ‰¹å¤„ç†ä¸å¼‚æ­¥ â€” ååä¼˜åŒ–</h3>
  <div class="bilingual">
    <div class="zh">
      <ul>
        <li><strong>Batch API</strong>ï¼šOpenAI æä¾› 50% æŠ˜æ‰£çš„æ‰¹å¤„ç† APIï¼Œå»¶è¿Ÿæ”¾å®½åˆ° 24h</li>
        <li><strong>è¯·æ±‚åˆå¹¶</strong>ï¼šå¤šä¸ªç›¸ä¼¼è¯·æ±‚åˆå¹¶ä¸ºä¸€ä¸ªï¼ˆå¦‚"ç¿»è¯‘è¿™ 10 æ®µæ–‡å­—"è€Œé 10 æ¬¡è°ƒç”¨ï¼‰</li>
        <li><strong>ä¼˜å…ˆçº§é˜Ÿåˆ—</strong>ï¼šç´§æ€¥è¯·æ±‚å®æ—¶å¤„ç†ï¼Œéç´§æ€¥æ’é˜Ÿç­‰æ‰¹å¤„ç†</li>
        <li><strong>é¢„è®¡ç®—</strong>ï¼šå¯é¢„çŸ¥çš„è¯·æ±‚æå‰åœ¨ä½å³°æœŸç”Ÿæˆç¼“å­˜</li>
      </ul>
    </div>
    <div class="en">
      <ul>
        <li><strong>Batch API</strong>: OpenAI offers 50% discount batch API with 24h latency SLA</li>
        <li><strong>Request merging</strong>: Combine similar requests ("translate these 10 paragraphs" vs 10 calls)</li>
        <li><strong>Priority queues</strong>: Urgent requests go real-time, rest queue for batch</li>
        <li><strong>Pre-compute</strong>: Generate cache during off-peak for predictable requests</li>
      </ul>
    </div>
  </div>

  <h2>ğŸ“Š ç”Ÿäº§çº§æˆæœ¬ä¼˜åŒ–ç»„åˆæ–¹æ¡ˆ</h2>
  
  <div class="diagram">
<pre style="background: #263238; color: #eeffff; font-size: 12px;">
                          Example: Customer Support Bot
                          100K requests/day â†’ Target: 80% cost reduction

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                                 â”‚
  â”‚  Request â”€â”€â†’ Exact Cache â”€â”€hit(20%)â”€â”€â†’ Return                  â”‚
  â”‚                   â”‚ miss                                        â”‚
  â”‚                   â–¼                                             â”‚
  â”‚            Semantic Cache â”€â”€hit(15%)â”€â”€â†’ Return                  â”‚
  â”‚                   â”‚ miss                                        â”‚
  â”‚                   â–¼                                             â”‚
  â”‚          Prompt Compress (LLMLingua, -50% tokens)              â”‚
  â”‚                   â”‚                                             â”‚
  â”‚                   â–¼                                             â”‚
  â”‚            Model Router                                        â”‚
  â”‚           â•±       â”‚       â•²                                    â”‚
  â”‚     Simple(60%) Medium(25%) Complex(15%)                       â”‚
  â”‚        â”‚          â”‚          â”‚                                  â”‚
  â”‚    gpt-4o-mini  haiku    sonnet                                â”‚
  â”‚    $0.00015/1k  $0.001   $0.003                                â”‚
  â”‚        â”‚          â”‚          â”‚                                  â”‚
  â”‚        â–¼          â–¼          â–¼                                  â”‚
  â”‚              Response + Update Cache                            â”‚
  â”‚                                                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Cost Breakdown (per day):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Before: 100K Ã— ~800 tokens Ã— $0.003/1k = $240/day          â”‚
  â”‚                                                              â”‚
  â”‚ After:                                                       â”‚
  â”‚   Cache hits (35K):      $0          (saved $84)            â”‚
  â”‚   Simple (39K Ã— 400tk):  $2.34       (was $93.60)           â”‚
  â”‚   Medium (16K Ã— 400tk):  $6.40       (was $38.40)           â”‚
  â”‚   Complex (10K Ã— 400tk): $12.00      (was $24.00)           â”‚
  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
  â”‚   Total: ~$21/day  (was $240)  â†’  91% savings ğŸ‰            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
  </div>

  <h2>ğŸ“ˆ æˆæœ¬ç›‘æ§ä¸å‘Šè­¦</h2>
  <div class="bilingual">
    <div class="zh">
      <p>ä¼˜åŒ–ä¹‹åï¼Œ<strong>æŒç»­ç›‘æ§</strong>åŒæ ·é‡è¦ï¼š</p>
      <ul>
        <li><strong>å…³é”®æŒ‡æ ‡</strong>ï¼šcost_per_request, cache_hit_rate, route_distribution, quality_score</li>
        <li><strong>å‘Šè­¦è§„åˆ™</strong>ï¼šcache hit ä¸‹é™ 10%+ã€å•è¯·æ±‚æˆæœ¬å¼‚å¸¸ã€quality ä¸‹é™</li>
        <li><strong>Dashboard</strong>ï¼šæŒ‰æ¨¡å‹/ä»»åŠ¡ç±»å‹/æ—¶é—´æ®µçœ‹æˆæœ¬åˆ†å¸ƒ</li>
        <li><strong>é¢„ç®—å¡æ§</strong>ï¼šè®¾ç½®æ¯å°æ—¶/æ¯å¤©ä¸Šé™ï¼Œè¶…é™é™çº§æˆ–æ‹’ç»</li>
      </ul>
    </div>
    <div class="en">
      <ul>
        <li><strong>Key metrics</strong>: cost_per_request, cache_hit_rate, route_distribution, quality_score</li>
        <li><strong>Alert rules</strong>: cache hit drops 10%+, abnormal per-request cost, quality degradation</li>
        <li><strong>Dashboard</strong>: Cost breakdown by model/task type/time period</li>
        <li><strong>Budget guardrails</strong>: Hourly/daily caps; degrade or reject when exceeded</li>
      </ul>
    </div>
  </div>

  <pre><code>// Cost Monitoring Middleware
class CostTracker {
  async trackRequest(req: LLMRequest, resp: LLMResponse, model: string) {
    const inputTokens = resp.usage.prompt_tokens;
    const outputTokens = resp.usage.completion_tokens;
    const cost = this.calculateCost(model, inputTokens, outputTokens);
    
    // Record metrics
    await Promise.all([
      this.metrics.record('llm.cost', cost, { model, task: req.taskType }),
      this.metrics.record('llm.tokens.input', inputTokens, { model }),
      this.metrics.record('llm.tokens.output', outputTokens, { model }),
    ]);
    
    // Budget check
    const hourlySpend = await this.getHourlySpend();
    if (hourlySpend > this.budget.hourlyLimit * 0.8) {
      await this.alert('Budget warning: 80% of hourly limit reached');
    }
    if (hourlySpend > this.budget.hourlyLimit) {
      this.enableDegradedMode(); // Force smaller models
    }
  }
}</code></pre>

  <h2>ğŸ¤ é¢è¯•é«˜é¢‘é—®é¢˜</h2>

  <div class="highlight">
    <h4>Q1: ä½ çš„ LLM ç³»ç»Ÿæ—¥å‡ $5000 API æˆæœ¬ï¼Œå¦‚ä½•ä¼˜åŒ–åˆ° $1000 ä»¥å†…ï¼Ÿ</h4>
    <p><strong>ç­”é¢˜æ¡†æ¶</strong>ï¼š</p>
    <ol>
      <li><strong>å…ˆé‡åŒ–</strong>ï¼šåˆ†æ token æ¶ˆè€—åˆ†å¸ƒï¼ˆinput vs output, by task typeï¼‰</li>
      <li><strong>å¿«èµ¢</strong>ï¼šExact Cache (20% hit) + Prompt å‹ç¼© (-40% tokens) â†’ ç«‹çœ ~50%</li>
      <li><strong>æ ¸å¿ƒ</strong>ï¼šModel Routerï¼Œ60-70% è¯·æ±‚ç”¨ mini æ¨¡å‹ â†’ å†çœ 50%+</li>
      <li><strong>æŒç»­</strong>ï¼šSemantic Cache + æ‰¹å¤„ç† + ç›‘æ§å‘Šè­¦</li>
    </ol>
    <p>æŒ‰æ­¤ç»„åˆï¼š$5000 â†’ $2500 (cache+å‹ç¼©) â†’ $1000 (routing) âœ…</p>
  </div>

  <div class="highlight">
    <h4>Q2: Model Routing å¦‚ä½•ä¿è¯è´¨é‡ä¸ä¸‹é™ï¼Ÿ</h4>
    <p><strong>ä¸‰å±‚ä¿éšœ</strong>ï¼š</p>
    <ol>
      <li><strong>Offline eval</strong>ï¼šåœ¨å†å²æ•°æ®ä¸ŠéªŒè¯ router å‡†ç¡®ç‡ï¼Œç¡®ä¿ 95%+</li>
      <li><strong>Online monitoring</strong>ï¼šé‡‡æ · 5% è¯·æ±‚ç”¨å¤§æ¨¡å‹äºŒæ¬¡è¯„ä¼°ï¼Œå¯¹æ¯”è´¨é‡</li>
      <li><strong>Fallback mechanism</strong>ï¼šç”¨æˆ·åé¦ˆ"ä¸æ»¡æ„"æ—¶è‡ªåŠ¨å‡çº§æ¨¡å‹é‡è¯•</li>
    </ol>
  </div>

  <div class="highlight">
    <h4>Q3: Semantic Cache çš„ç›¸ä¼¼åº¦é˜ˆå€¼æ€ä¹ˆå®šï¼Ÿ</h4>
    <p><strong>ç­”</strong>ï¼šä¸èƒ½æ‹è„‘è¢‹ã€‚éœ€è¦ç”¨æ ‡æ³¨æ•°æ®æ ¡å‡†ï¼šæ”¶é›† 1000 å¯¹ (query_a, query_b, same_intent)ï¼Œç”» precision-recall æ›²çº¿ã€‚</p>
    <ul>
      <li><strong>é«˜ç²¾åº¦åœºæ™¯</strong>ï¼ˆé‡‘è/åŒ»ç–—ï¼‰ï¼šé€‰ precision=99% çš„é˜ˆå€¼ï¼ˆé€šå¸¸ 0.96+ï¼‰</li>
      <li><strong>é«˜å‘½ä¸­åœºæ™¯</strong>ï¼ˆé—²èŠ/FAQï¼‰ï¼šé€‰ F1 æœ€ä¼˜ç‚¹ï¼ˆé€šå¸¸ 0.90-0.93ï¼‰</li>
      <li>éƒ¨ç½²åæŒç»­ A/B test å¾®è°ƒ</li>
    </ul>
  </div>

  <div class="highlight">
    <h4>Q4: å¦‚æœä½ åœ¨è®¾è®¡ä¸€ä¸ªå¤šç§Ÿæˆ· LLM å¹³å°ï¼Œå¦‚ä½•åšæˆæœ¬éš”ç¦»å’Œå…¬å¹³è®¡è´¹ï¼Ÿ</h4>
    <p><strong>å…³é”®è®¾è®¡</strong>ï¼š</p>
    <ul>
      <li><strong>Token è®¡é‡</strong>ï¼šæ¯ä¸ªç§Ÿæˆ·ç‹¬ç«‹ meterï¼ŒæŒ‰å®é™… token æ¶ˆè€—è®¡è´¹</li>
      <li><strong>é…é¢ç®¡ç†</strong>ï¼šper-tenant rate limit + æ¯æœˆé…é¢ + è¶…é¢è‡ªåŠ¨é™çº§</li>
      <li><strong>æ¨¡å‹æƒé™</strong>ï¼šä¸åŒ tier çš„ç§Ÿæˆ·å¯ç”¨ä¸åŒæ¨¡å‹ï¼ˆFree â†’ mini only, Pro â†’ allï¼‰</li>
      <li><strong>å…±äº«ç¼“å­˜</strong>ï¼šå…¨å±€ Exact Cache å¯ä»¥è·¨ç§Ÿæˆ·å…±äº«ï¼ˆåŒé—®é¢˜åŒç­”æ¡ˆï¼‰ï¼Œä½†éœ€æ³¨æ„æ•°æ®éš”ç¦»</li>
      <li><strong>æˆæœ¬åˆ†æ‘Š</strong>ï¼šå…±äº«åŸºç¡€è®¾æ–½æˆæœ¬æŒ‰è¯·æ±‚æ¯”ä¾‹åˆ†æ‘Š</li>
    </ul>
  </div>

  <h2>ğŸ”— ä¸å‰åå†…å®¹çš„å…³è”</h2>
  <div class="series-nav">
    <p><strong>â† Day 1 Token é™åˆ¶å¤„ç†</strong>ï¼šPrompt å‹ç¼©æ˜¯æˆæœ¬ä¼˜åŒ–çš„å­é›†ï¼Œä»Šå¤©æŠŠå®ƒæ”¾åˆ°æ›´å¤§çš„ä¼˜åŒ–æ¡†æ¶ä¸­ã€‚</p>
    <p><strong>â†’ Day 3 Hallucination æ£€æµ‹</strong>ï¼šCascade ä¸­çš„ Verifier ä¸å¹»è§‰æ£€æµ‹ç›¸å…³â€”â€”å¦‚ä½•åˆ¤æ–­å°æ¨¡å‹çš„ç­”æ¡ˆæ˜¯å¦å¯é ï¼Ÿ</p>
    <p><strong>å›é¡¾ Phase 2.1</strong>ï¼šè®¾è®¡ ChatGPT (Day 1) ä¸­çš„ Model Router å’Œä»˜è´¹åˆ†å±‚ï¼Œä»Šå¤©æ˜¯æ·±å…¥ç‰ˆã€‚</p>
  </div>

  <div class="follow-up">
    <h3>ğŸ’¬ åç»­è®¨è®º / Follow-up Discussion</h3>
    <p>å…³äºæˆæœ¬ä¼˜åŒ–ï¼Œæ¬¢è¿è®¨è®ºï¼š</p>
    <ul>
      <li>ä½ åœ¨å®é™…é¡¹ç›®ä¸­é‡åˆ°çš„ LLM æˆæœ¬é—®é¢˜</li>
      <li>OpenClaw çš„æ¨¡å‹è·¯ç”±æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ</li>
      <li>å¦‚ä½•è¯„ä¼°"è´¨é‡æ²¡æœ‰ä¸‹é™"â€”â€” LLM-as-Judge é è°±å—ï¼Ÿ</li>
    </ul>
  </div>

  <hr>
  <p style="color: #999; font-size: 12px;">
    Generated by OpenClaw Knowledge Push Â· LLM ç‰¹æœ‰é—®é¢˜ç³»åˆ— Day 2/5 Â· 2026-02-21
  </p>
</body>
</html>
