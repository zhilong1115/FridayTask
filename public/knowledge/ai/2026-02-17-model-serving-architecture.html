<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-17 - Model Serving æ¶æ„ï¼šä»æ¨ç†å¼•æ“åˆ°ç”Ÿäº§éƒ¨ç½²</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.8; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; }
    h2 { color: #1a73e8; border-bottom: 2px solid #e8eaed; padding-bottom: 8px; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; font-family: monospace; white-space: pre; line-height: 1.4; font-size: 13px; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; }
    code { font-size: 14px; }
    .highlight { background: #fff3cd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight h4 { margin-top: 0; }
    .compare-table { width: 100%; border-collapse: collapse; margin: 15px 0; font-size: 14px; }
    .compare-table th, .compare-table td { border: 1px solid #dadce0; padding: 10px; text-align: left; }
    .compare-table th { background: #f8f9fa; }
    .interview { background: #e8f5e9; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .interview h4 { color: #2e7d32; margin-top: 0; }
    .pattern { background: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .warning { background: #fce4ec; padding: 12px; border-radius: 8px; margin: 10px 0; }
    .series-nav { background: #f3e5f5; padding: 12px; border-radius: 8px; margin: 20px 0; }
  </style>
</head>
<body>

<h1>ğŸš€ Model Serving æ¶æ„ï¼šä»æ¨ç†å¼•æ“åˆ°ç”Ÿäº§éƒ¨ç½²</h1>
<p>Phase 2.2 ML System Design è¦ç‚¹ Â· Day 3 | 2026-02-17</p>

<div class="series-nav">
  ğŸ“š <strong>ç³»åˆ—å¯¼èˆª</strong>ï¼š
  <a href="2026-02-15-ml-system-design-patterns.html">Day 1: 7å¤§é€šç”¨Pattern</a> â†’
  <a href="2026-02-16-feature-store-design.html">Day 2: Feature Store</a> â†’
  <strong>Day 3: Model Serving (æœ¬ç¯‡)</strong>
</div>

<h2>ğŸ“– ä¸ºä»€ä¹ˆ Model Serving æ˜¯é¢è¯•é‡ç‚¹ï¼Ÿ</h2>

<div class="bilingual">
  <div class="zh">
    <p>åœ¨ ML ç³»ç»Ÿè®¾è®¡é¢è¯•ä¸­ï¼Œã€Œä½ çš„æ¨¡å‹æ€ä¹ˆä¸Šçº¿ã€æ˜¯å¿…é—®é¢˜ã€‚è®­ç»ƒä¸€ä¸ªå¥½æ¨¡å‹åªæ˜¯æ•…äº‹çš„ä¸€åŠâ€”â€”å¦‚ä½•ä»¥ä½å»¶è¿Ÿã€é«˜ååã€ä½æˆæœ¬æŠŠæ¨ç†ç»“æœé€åˆ°ç”¨æˆ·æ‰‹é‡Œï¼Œæ‰æ˜¯çœŸæ­£çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚</p>
    <p>Model Serving æ˜¯ Day 1 ä¸ƒå¤§ Pattern ä¸­ <strong>Offline/Online Split</strong> å’Œ <strong>Multi-Stage Pipeline</strong> çš„è½åœ°ç‚¹ï¼Œä¹Ÿæ˜¯ Day 2 Feature Store çš„ä¸‹æ¸¸æ¶ˆè´¹è€…ã€‚</p>
  </div>
  <div class="en">
    <p>"How do you serve your model?" is a must-ask in ML system design interviews. Training a good model is only half the story â€” delivering predictions at low latency, high throughput, and low cost is the real engineering challenge.</p>
    <p>Model Serving is where Day 1's Offline/Online Split and Multi-Stage Pipeline patterns land, and it's the downstream consumer of Day 2's Feature Store.</p>
  </div>
</div>

<h2>ğŸ—ï¸ ä¸‰å±‚æ¶æ„ / The Three-Layer Stack</h2>

<div class="architecture">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Layer 3: Inference Gateway                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Load     â”‚  â”‚ A/B Test â”‚  â”‚ Rate     â”‚  â”‚ Auth &        â”‚  â”‚
â”‚  â”‚ Balancer â”‚  â”‚ Router   â”‚  â”‚ Limiter  â”‚  â”‚ Observability â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Layer 2: Serving Framework                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Request  â”‚  â”‚ Batching â”‚  â”‚ Model    â”‚  â”‚ Health   â”‚      â”‚
â”‚  â”‚ Queue    â”‚  â”‚ Engine   â”‚  â”‚ Registry â”‚  â”‚ Checks   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Layer 1: Model Runtime                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ KV Cache â”‚  â”‚ Quantize â”‚  â”‚ Attentionâ”‚  â”‚ Tensor   â”‚      â”‚
â”‚  â”‚ Manager  â”‚  â”‚ Engine   â”‚  â”‚ Kernels  â”‚  â”‚ Parallel â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         GPU Memory (HBM)              CPU / System Memory
</div>

<div class="bilingual">
  <div class="zh">
    <p><strong>Layer 1 - Model Runtime</strong>ï¼šæœ€åº•å±‚ï¼Œç®¡ç† GPU èµ„æºã€‚è´Ÿè´£ KV Cache åˆ†é…ï¼ˆå¦‚ vLLM çš„ PagedAttentionï¼‰ã€é‡åŒ–æ¨ç†ã€æ³¨æ„åŠ› kernel ä¼˜åŒ–ã€å¤š GPU å¼ é‡å¹¶è¡Œã€‚è¿™æ˜¯ã€Œå•å¡è·‘å¾—å¿«ä¸å¿«ã€çš„é—®é¢˜ã€‚</p>
    <p><strong>Layer 2 - Serving Framework</strong>ï¼šä¸­é—´å±‚ï¼Œç®¡ç†è¯·æ±‚ç”Ÿå‘½å‘¨æœŸã€‚è´Ÿè´£ Continuous Batchingã€è¯·æ±‚é˜Ÿåˆ—ç®¡ç†ã€æ¨¡å‹çƒ­åŠ è½½/å¸è½½ã€å¥åº·æ£€æŸ¥ã€‚è¿™æ˜¯ã€Œå¤šä¸ªè¯·æ±‚æ€ä¹ˆé«˜æ•ˆå¤„ç†ã€çš„é—®é¢˜ã€‚</p>
    <p><strong>Layer 3 - Inference Gateway</strong>ï¼šæœ€ä¸Šå±‚ï¼Œç®¡ç†æµé‡å’Œç­–ç•¥ã€‚è´Ÿè½½å‡è¡¡ã€A/B æµ‹è¯•è·¯ç”±ã€é™æµã€è®¤è¯ã€ç›‘æ§ã€‚è¿™æ˜¯ã€Œé¢å‘ç”¨æˆ·çš„å¯é æ€§ã€çš„é—®é¢˜ã€‚</p>
  </div>
  <div class="en">
    <p><strong>Layer 1 - Model Runtime</strong>: Manages GPU resources. KV Cache allocation (e.g., vLLM's PagedAttention), quantized inference, attention kernel optimization, tensor parallelism. This answers "how fast can one GPU run?"</p>
    <p><strong>Layer 2 - Serving Framework</strong>: Manages request lifecycle. Continuous Batching, request queuing, hot model loading/unloading, health checks. This answers "how to handle many requests efficiently?"</p>
    <p><strong>Layer 3 - Inference Gateway</strong>: Manages traffic and policies. Load balancing, A/B routing, rate limiting, auth, observability. This answers "how to be reliable for users?"</p>
  </div>
</div>

<h2>âš”ï¸ æ¡†æ¶é€‰å‹ / Framework Comparison</h2>

<table class="compare-table">
<tr>
  <th>ç»´åº¦</th>
  <th>vLLM</th>
  <th>TGI (HuggingFace)</th>
  <th>TensorRT-LLM + Triton</th>
  <th>Ollama / llama.cpp</th>
</tr>
<tr>
  <td><strong>å®šä½</strong></td>
  <td>é«˜åå LLM serving</td>
  <td>HF ç”Ÿæ€ä¸€ç«™å¼</td>
  <td>æè‡´æ€§èƒ½</td>
  <td>æœ¬åœ°å¼€å‘/è¾¹ç¼˜</td>
</tr>
<tr>
  <td><strong>æ ¸å¿ƒæŠ€æœ¯</strong></td>
  <td>PagedAttention, Continuous Batching</td>
  <td>Flash Attention, Token Streaming</td>
  <td>FP8/INT4 ç¼–è¯‘ä¼˜åŒ–, In-flight Batching</td>
  <td>GGUF é‡åŒ–, CPU/Metal æ¨ç†</td>
</tr>
<tr>
  <td><strong>ååé‡</strong></td>
  <td>â­â­â­â­</td>
  <td>â­â­â­</td>
  <td>â­â­â­â­â­</td>
  <td>â­â­</td>
</tr>
<tr>
  <td><strong>æ˜“ç”¨æ€§</strong></td>
  <td>â­â­â­â­</td>
  <td>â­â­â­â­â­</td>
  <td>â­â­</td>
  <td>â­â­â­â­â­</td>
</tr>
<tr>
  <td><strong>Multi-GPU</strong></td>
  <td>Tensor + Pipeline å¹¶è¡Œ</td>
  <td>Tensor å¹¶è¡Œ</td>
  <td>å…¨å¹¶è¡Œæ¨¡å¼</td>
  <td>æœ‰é™æ”¯æŒ</td>
</tr>
<tr>
  <td><strong>Multi-Model</strong></td>
  <td>å•è¿›ç¨‹å•æ¨¡å‹</td>
  <td>å•è¿›ç¨‹å•æ¨¡å‹</td>
  <td>Triton æ”¯æŒå¤šæ¨¡å‹</td>
  <td>æŒ‰éœ€åŠ è½½/å¸è½½</td>
</tr>
<tr>
  <td><strong>é€‚ç”¨åœºæ™¯</strong></td>
  <td>ç”Ÿäº§ LLM API</td>
  <td>HF æ¨¡å‹å¿«é€Ÿä¸Šçº¿</td>
  <td>å¤§è§„æ¨¡æ¨ç†é›†ç¾¤</td>
  <td>åŸå‹/è¾¹ç¼˜/æœ¬åœ°</td>
</tr>
</table>

<div class="highlight">
  <h4>ğŸ’¡ é¢è¯•é€‰å‹å…¬å¼ / Interview Selection Formula</h4>
  <p><strong>åŸå‹ â†’ Ollama/TGI â†’ ç”Ÿäº§ â†’ vLLM â†’ æè‡´ä¼˜åŒ– â†’ TensorRT-LLM + Triton</strong></p>
  <p>é¢è¯•æ—¶æŒ‰åœºæ™¯é€’è¿›åœ°è®²ï¼Œæ¯”å•è¯´ä¸€ä¸ªæ¡†æ¶å¥½å¾—å¤šã€‚å±•ç¤ºä½ ç†è§£ä¸åŒé˜¶æ®µçš„éœ€æ±‚ã€‚</p>
</div>

<h2>ğŸ”„ å››ç§éƒ¨ç½²ç­–ç•¥ / Four Deployment Strategies</h2>

<div class="architecture">
Strategy 1: Blue-Green              Strategy 2: Canary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  100%â†’0%              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  95% â”€â”€â†’ 0%
â”‚ Model v1 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚ Model v1 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (Blue)   â”‚           â”‚ switch    â”‚ (Stable) â”‚          â”‚ gradual
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â–¼           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  0%â†’100%             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  5% â”€â”€â†’ 100%
â”‚ Model v2 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚ Model v2 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ (Green)  â”‚                       â”‚ (Canary) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Strategy 3: Shadow                  Strategy 4: Multi-Armed Bandit
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  100% (serve)         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â† dynamic %
â”‚ Model v1 â”‚ â†’ Response            â”‚ Model A  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â† dynamic %
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  100% (log only)      â”‚ Model B  â”‚
â”‚ Model v2 â”‚ â†’ /dev/null           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â† dynamic %
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (compare offline)    â”‚ Model C  â”‚  reward-based
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  allocation
</div>

<div class="bilingual">
  <div class="zh">
    <h4>1. Blue-Greenï¼ˆè“ç»¿éƒ¨ç½²ï¼‰</h4>
    <p>æœ€ç®€å•ç²—æš´ï¼šä¸¤å¥—å®Œæ•´ç¯å¢ƒï¼Œä¸€é”®åˆ‡æ¢ã€‚ä¼˜ç‚¹æ˜¯å›æ»šç§’çº§ï¼›ç¼ºç‚¹æ˜¯è¦ 2x èµ„æºï¼ˆGPU å¾ˆè´µï¼ï¼‰ã€‚é€‚åˆä½é¢‘æ›´æ–°ã€ä¸èƒ½å‡ºé”™çš„åœºæ™¯ã€‚</p>
    
    <h4>2. Canaryï¼ˆé‡‘ä¸é›€å‘å¸ƒï¼‰â­ æœ€å¸¸ç”¨</h4>
    <p>é€æ­¥æ”¾é‡ï¼š5% â†’ 25% â†’ 50% â†’ 100%ã€‚æ¯æ­¥ç›‘æ§å…³é”®æŒ‡æ ‡ï¼ˆå»¶è¿Ÿ P99ã€é”™è¯¯ç‡ã€ä¸šåŠ¡æŒ‡æ ‡ï¼‰ã€‚è‡ªåŠ¨å›æ»šæ¡ä»¶è§¦å‘ = å®‰å…¨ç½‘ã€‚<strong>é¢è¯•å¿…è®²è¿™ä¸ªã€‚</strong></p>
    
    <h4>3. Shadowï¼ˆå½±å­æ¨¡å¼ï¼‰</h4>
    <p>æ–°æ¨¡å‹æ¥æ”¶çœŸå®æµé‡ä½†ä¸è¿”å›ç»“æœï¼Œåªè®°å½•æ—¥å¿—åšç¦»çº¿å¯¹æ¯”ã€‚é›¶ç”¨æˆ·å½±å“ï¼Œä½† 2x è®¡ç®—æˆæœ¬ã€‚é€‚åˆé«˜é£é™©åœºæ™¯ï¼ˆé‡‘èã€åŒ»ç–—ï¼‰ã€‚</p>
    
    <h4>4. Multi-Armed Banditï¼ˆå¤šè‡‚è€è™æœºï¼‰</h4>
    <p>åŠ¨æ€åˆ†é…æµé‡ï¼šæ ¹æ®å®æ—¶æŒ‡æ ‡è‡ªåŠ¨æŠŠæ›´å¤šæµé‡å¯¼å‘è¡¨ç°å¥½çš„æ¨¡å‹ã€‚æ¯”å›ºå®š A/B æµ‹è¯•æ”¶æ•›æ›´å¿«ï¼Œä½†å®ç°å¤æ‚ã€‚é€‚åˆæ¨èç³»ç»Ÿç­‰æœ‰æ˜ç¡® reward çš„åœºæ™¯ã€‚</p>
  </div>
  <div class="en">
    <h4>1. Blue-Green</h4>
    <p>Simplest: two full environments, instant switch. Rollback in seconds, but requires 2x resources (GPUs are expensive!). For low-frequency, zero-tolerance updates.</p>
    
    <h4>2. Canary â­ Most Common</h4>
    <p>Gradual rollout: 5% â†’ 25% â†’ 50% â†’ 100%. Monitor key metrics (P99 latency, error rate, business KPIs) at each step. Auto-rollback as safety net. <strong>Must-discuss in interviews.</strong></p>
    
    <h4>3. Shadow</h4>
    <p>New model receives real traffic but doesn't serve responses â€” logs only for offline comparison. Zero user impact, but 2x compute. For high-risk domains (finance, healthcare).</p>
    
    <h4>4. Multi-Armed Bandit</h4>
    <p>Dynamic allocation: automatically routes more traffic to better-performing models based on real-time rewards. Converges faster than fixed A/B, but complex to implement. Great for recommendation systems with clear reward signals.</p>
  </div>
</div>

<h2>ğŸ“Š Auto-Scaling ç­–ç•¥ / GPU-Aware Scaling</h2>

<div class="highlight">
  <h4>æ ¸å¿ƒéš¾ç‚¹ï¼šGPU â‰  CPU</h4>
  <p>CPU auto-scaling çœ‹ CPU åˆ©ç”¨ç‡å°±è¡Œã€‚GPU ä¸ä¸€æ ·â€”â€”æ¨¡å‹åŠ è½½åˆ°æ˜¾å­˜éœ€è¦ 30-120 ç§’ï¼ˆå†·å¯åŠ¨ï¼‰ï¼Œè€Œä¸” GPU åˆ©ç”¨ç‡ä¸èƒ½ç®€å•å¯¹åº”è¯·æ±‚é‡ã€‚éœ€è¦æ›´æ™ºèƒ½çš„æŒ‡æ ‡ã€‚</p>
</div>

<pre><code># ç”Ÿäº§çº§ Auto-Scaling æŒ‡æ ‡ç»„åˆ
scaling_config:
  metrics:
    # ä¸»æŒ‡æ ‡ï¼šè¯·æ±‚é˜Ÿåˆ—æ·±åº¦ï¼ˆæœ€ç›´æ¥åæ˜ ä¾›éœ€å…³ç³»ï¼‰
    - name: pending_requests
      target: 10          # æ¯ä¸ª replica çš„é˜Ÿåˆ—ä¸Šé™
      type: AverageValue
    
    # è¾…åŠ©æŒ‡æ ‡ï¼šGPU æ˜¾å­˜åˆ©ç”¨ç‡
    - name: gpu_memory_utilization
      target: 85%         # KV Cache æ¥è¿‘æ»¡ = éœ€è¦æ‰©å®¹
      type: Utilization
    
    # è¾…åŠ©æŒ‡æ ‡ï¼šTTFT (Time to First Token)
      - name: p95_time_to_first_token_ms
      target: 500         # è¶…è¿‡ 500ms = ç”¨æˆ·ä½“éªŒå˜å·®
      type: AverageValue

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60   # 1åˆ†é’Ÿå†…æŒç»­å‹åŠ›æ‰æ‰©
      policies:
        - type: Pods
          value: 2                     # æ¯æ¬¡æœ€å¤šåŠ  2 ä¸ª pod
          periodSeconds: 120
    scaleDown:
      stabilizationWindowSeconds: 300  # 5åˆ†é’Ÿå†·å´æœŸï¼ˆGPU è´µï¼Œåˆ«æŠ–åŠ¨ï¼‰
      policies:
        - type: Pods
          value: 1                     # æ¯æ¬¡åªå‡ 1 ä¸ª pod
          periodSeconds: 300

  # é¢„çƒ­ç­–ç•¥ï¼šå¯¹æŠ—å†·å¯åŠ¨
  warmup:
    modelPreload: true           # Pod å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹
    healthCheckDelay: 120s       # ç­‰æ¨¡å‹åŠ è½½å®Œæ‰æ¥æµé‡
    warmupRequests: 10           # å‘ 10 ä¸ªæµ‹è¯•è¯·æ±‚é¢„çƒ­ KV Cache</code></pre>

<div class="pattern">
  <h4>ğŸ¯ é¢è¯•å…³é”®ç‚¹ / Interview Key Points</h4>
  <ul>
    <li><strong>å†·å¯åŠ¨å¯¹ç­–</strong>ï¼šModel Preloading + Warm Poolï¼ˆä¿æŒ N ä¸ªå¾…å‘½å®ä¾‹ï¼‰+ é¢„æµ‹æ€§æ‰©å®¹ï¼ˆæ ¹æ®å†å²æµé‡æå‰æ‰©ï¼‰</li>
    <li><strong>ç¼©å®¹è¦è°¨æ…</strong>ï¼šGPU å®ä¾‹è´µ + å†·å¯åŠ¨æ…¢ â†’ ç¼©å®¹çª—å£å¿…é¡»é•¿ï¼ˆ5-15åˆ†é’Ÿï¼‰</li>
    <li><strong>å¤šæ¨¡å‹å…±äº« GPU</strong>ï¼šç”¨ Triton çš„ Model Ensemble æˆ– NVIDIA MPS è®©å°æ¨¡å‹å…±äº«ä¸€å¼ å¡</li>
  </ul>
</div>

<h2>ğŸ›¡ï¸ ç”Ÿäº§åŒ–å¿…å¤‡ / Production Essentials</h2>

<h3>1. å¥åº·æ£€æŸ¥ä¸‰çº§ä½“ç³»</h3>

<pre><code># Level 1: Liveness â€” è¿›ç¨‹è¿˜æ´»ç€å—ï¼Ÿ
GET /health â†’ 200 OK

# Level 2: Readiness â€” æ¨¡å‹åŠ è½½å®Œäº†å—ï¼Ÿèƒ½æ¥è¯·æ±‚å—ï¼Ÿ
GET /ready â†’ 200 OK (model loaded, GPU memory allocated)
           â†’ 503 (still loading / OOM recovery)

# Level 3: Inference Health â€” æ¨ç†è´¨é‡æ­£å¸¸å—ï¼Ÿ
POST /health/inference
  â†’ Run canary prompt, check latency < threshold
  â†’ Detect: GPU error (NaN outputs), memory leak, KV cache corruption</code></pre>

<h3>2. Graceful Degradation ä¼˜é›…é™çº§</h3>

<div class="architecture">
æ­£å¸¸æ¨¡å¼:            é™çº§æ¨¡å¼ (è¿‡è½½æ—¶):         ç†”æ–­æ¨¡å¼ (æ•…éšœæ—¶):
User â†’ GPT-4o       User â†’ GPT-4o-mini       User â†’ Cached Response
     â†’ Full ctx          â†’ Truncated ctx           â†’ Fallback message
     â†’ Streaming         â†’ Non-streaming            â†’ "ç¨åé‡è¯•"
     
è§¦å‘æ¡ä»¶:            è§¦å‘æ¡ä»¶:                  è§¦å‘æ¡ä»¶:
QPS < 1000           QPS > 1000 or P99 > 2s    Error rate > 10%
GPU util < 80%       GPU util > 90%             GPU unreachable
</div>

<h3>3. å…³é”®ç›‘æ§æŒ‡æ ‡ / Key Metrics</h3>

<table class="compare-table">
<tr>
  <th>å±‚çº§</th>
  <th>æŒ‡æ ‡</th>
  <th>å‘Šè­¦é˜ˆå€¼</th>
  <th>å«ä¹‰</th>
</tr>
<tr>
  <td>ç”¨æˆ·ä½“éªŒ</td>
  <td>TTFT (Time to First Token)</td>
  <td>P95 > 1s</td>
  <td>ç”¨æˆ·ç­‰ç¬¬ä¸€ä¸ªå­—çš„æ—¶é—´</td>
</tr>
<tr>
  <td>ç”¨æˆ·ä½“éªŒ</td>
  <td>TPS (Tokens Per Second)</td>
  <td>&lt; 20 tok/s</td>
  <td>ç”Ÿæˆé€Ÿåº¦ï¼Œå½±å“æµå¼ä½“éªŒ</td>
</tr>
<tr>
  <td>ç³»ç»Ÿå¥åº·</td>
  <td>KV Cache åˆ©ç”¨ç‡</td>
  <td>> 90%</td>
  <td>æ»¡äº†ä¼šæ‹’ç»æ–°è¯·æ±‚</td>
</tr>
<tr>
  <td>ç³»ç»Ÿå¥åº·</td>
  <td>Batch Size (Running)</td>
  <td>æŒç»­ max</td>
  <td>GPU æ»¡è½½ï¼Œå»¶è¿Ÿä¼šä¸Šå‡</td>
</tr>
<tr>
  <td>æˆæœ¬</td>
  <td>GPU åˆ©ç”¨ç‡</td>
  <td>&lt; 30%</td>
  <td>æµªè´¹é’±ï¼Œåº”è¯¥ç¼©å®¹æˆ–å…±äº«</td>
</tr>
<tr>
  <td>æˆæœ¬</td>
  <td>$/1M tokens</td>
  <td>æŒç»­ä¸Šå‡</td>
  <td>æ•ˆç‡ä¸‹é™ï¼Œæ£€æŸ¥ batching</td>
</tr>
</table>

<h2>ğŸ’» ç«¯åˆ°ç«¯è®¾è®¡ï¼šé¢è¯•å®Œæ•´å›ç­” / Complete Interview Answer</h2>

<div class="architecture">
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   API Gateway   â”‚ â† Auth, Rate Limit, Logging
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Model Router   â”‚ â† A/B Test, Canary, Fallback
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ vLLM   â”‚ â”‚ vLLM   â”‚ â”‚ Triton  â”‚ â† Serving Layer
              â”‚ GPT-4o â”‚ â”‚ GPT-4m â”‚ â”‚ Embed   â”‚
              â”‚ (4Ã—H100â”‚ â”‚(1Ã—H100)â”‚ â”‚(1Ã—A100) â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   â”‚         â”‚           â”‚
              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
              â”‚        Feature Store          â”‚ â† Day 2 çš„æ•°æ®å±‚
              â”‚   (User context, history)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      Monitoring & Alerting    â”‚
              â”‚  TTFT, TPS, GPU%, Error Rate  â”‚
              â”‚  Prometheus â†’ Grafana â†’ PD    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>

<div class="highlight">
  <h4>ğŸ—£ï¸ é¢è¯•è¯æœ¯ / Interview Talking Points</h4>
  <ol>
    <li><strong>åˆ†å±‚è®²</strong>ï¼šå…ˆç”»ä¸‰å±‚æ¶æ„ï¼ˆRuntime â†’ Serving â†’ Gatewayï¼‰ï¼Œå±•ç¤ºä½ ç†è§£ä¸åŒæŠ½è±¡å±‚çš„èŒè´£</li>
    <li><strong>é€‰å‹è®²æƒè¡¡</strong>ï¼šä¸è¦åªè¯´ã€Œç”¨ vLLMã€ï¼Œè¦è¯´ã€ŒåŸå‹ç”¨ TGI å¿«é€ŸéªŒè¯ï¼Œç”Ÿäº§åˆ‡ vLLM å› ä¸º PagedAttention æå‡ 2-4x ååã€</li>
    <li><strong>éƒ¨ç½²ç­–ç•¥</strong>ï¼šå¿…è®² Canary + Shadowï¼Œä½“ç°ä½ é‡è§†å®‰å…¨ä¸Šçº¿</li>
    <li><strong>Scale è®²æŒ‡æ ‡</strong>ï¼šä¸èƒ½åªè¯´ HPAï¼Œè¦è®² GPU-specific çš„ scaling signalsï¼ˆé˜Ÿåˆ—æ·±åº¦ > åˆ©ç”¨ç‡ï¼‰</li>
    <li><strong>æˆæœ¬æ„è¯†</strong>ï¼šGPU æ˜¯æœ€å¤§æˆæœ¬é¡¹ï¼Œè®² Model Routingï¼ˆç®€å•è¯·æ±‚ç”¨å°æ¨¡å‹ï¼‰å’Œ Multi-Model GPU Sharing</li>
  </ol>
</div>

<h2>ğŸ“ å®¹é‡ä¼°ç®— / Capacity Estimation</h2>

<div class="pattern">
  <h4>åœºæ™¯ï¼šæ—¥æ´» 1M çš„ ChatBot æœåŠ¡</h4>
  <p><strong>å‡è®¾</strong>ï¼šæ¯ç”¨æˆ· 5 æ¬¡å¯¹è¯/å¤©ï¼Œæ¯æ¬¡å¹³å‡ 500 input + 300 output tokens</p>
  <ul>
    <li>æ€»è¯·æ±‚ï¼š1M Ã— 5 = 5M req/day = ~58 QPS (avg), ~290 QPS (peak 5x)</li>
    <li>Token ååï¼š290 Ã— 300 = 87K output tokens/s (peak)</li>
    <li>å• H100 vLLM (Llama-70B FP16)ï¼š~3000 output tokens/s</li>
    <li>â†’ éœ€è¦ 87K / 3K â‰ˆ <strong>29 å¼  H100</strong>ï¼ˆå³°å€¼ï¼‰+ ç¼“å†² = 35 å¼ </li>
    <li>æˆæœ¬ï¼š35 Ã— $2/hr â‰ˆ <strong>$1,680/å¤© â‰ˆ $50K/æœˆ</strong>ï¼ˆæŒ‰éœ€ä»·æ ¼ï¼‰</li>
    <li>ä¼˜åŒ–åï¼ˆFP8 é‡åŒ– + Model Routing 50% å°æ¨¡å‹ï¼‰ï¼šâ†’ ~$20-25K/æœˆ</li>
  </ul>
</div>

<h2>ğŸ”— ä¸å‰ä¸¤å¤©çš„å…³è” / Series Connections</h2>

<table class="compare-table">
<tr>
  <th>Day 1 Pattern</th>
  <th>åœ¨ Model Serving ä¸­çš„ä½“ç°</th>
</tr>
<tr>
  <td>Multi-Stage Pipeline</td>
  <td>Embed â†’ Retrieve â†’ Rank â†’ Generateï¼Œæ¯æ®µç”¨ä¸åŒæ¨¡å‹/ç¡¬ä»¶</td>
</tr>
<tr>
  <td>Offline/Online Split</td>
  <td>æ¨¡å‹è®­ç»ƒ (offline) vs æ¨ç†æœåŠ¡ (online)ï¼ŒFeature Store æ¡¥æ¥</td>
</tr>
<tr>
  <td>Model Routing</td>
  <td>Gateway å±‚æŒ‰ complexity/cost è·¯ç”±åˆ°ä¸åŒæ¨¡å‹å®ä¾‹</td>
</tr>
<tr>
  <td>Multi-Layer Caching</td>
  <td>KV Cache (GPU å†…) + Prompt Cache (Redis) + Response Cache (CDN)</td>
</tr>
<tr>
  <td>Guardrails</td>
  <td>Health Check ä¸‰çº§ + Graceful Degradation + Auto-rollback</td>
</tr>
</table>

<h2>ğŸ¤ é¢è¯•é«˜é¢‘é—®é¢˜ / Interview Questions</h2>

<div class="interview">
  <h4>Q1: vLLM çš„ Continuous Batching å’Œä¼ ç»Ÿ Static Batching æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</h4>
  <p><strong>ç­”</strong>ï¼šStatic Batching ç­‰æ‰€æœ‰è¯·æ±‚éƒ½ç”Ÿæˆå®Œæ‰å¤„ç†ä¸‹ä¸€æ‰¹ â†’ çŸ­è¯·æ±‚è¢«é•¿è¯·æ±‚æ‹–æ…¢ã€‚Continuous Batching å…è®¸è¯·æ±‚åœ¨ä»»æ„ iteration åŠ å…¥/é€€å‡º batch â†’ çŸ­è¯·æ±‚åŠæ—¶è¿”å›ï¼ŒGPU åˆ©ç”¨ç‡ä» ~30% æå‡åˆ° ~80%+ã€‚è¿™å°±åƒé¤å…ä»ã€Œä¸€æ¡Œå…¨ç‚¹å®Œæ‰ä¸Šèœã€å˜æˆã€Œåšå¥½ä¸€é“ä¸Šä¸€é“ã€ã€‚</p>
</div>

<div class="interview">
  <h4>Q2: å¦‚ä½•è®¾è®¡ Canary å‘å¸ƒçš„è‡ªåŠ¨å›æ»šæœºåˆ¶ï¼Ÿ</h4>
  <p><strong>ç­”</strong>ï¼šä¸‰æ­¥ï¼šâ‘ å®šä¹‰ SLO æŒ‡æ ‡ï¼ˆP99 å»¶è¿Ÿ < 2s, é”™è¯¯ç‡ < 1%, ä¸šåŠ¡æŒ‡æ ‡ä¸é™ > 5%ï¼‰â‘¡ç”¨ Istio/Envoy åšæµé‡åˆ†å‰²ï¼Œä» 5% å¼€å§‹é€æ­¥æ”¾é‡ â‘¢è‡ªåŠ¨åŒ–æ§åˆ¶å™¨ï¼ˆArgo Rollouts / Flaggerï¼‰æ¯ 2 åˆ†é’Ÿæ£€æŸ¥æŒ‡æ ‡ï¼Œè¿åä»»ä¸€ SLO ç«‹å³å›æ»šã€‚å…³é”®æ˜¯ï¼š<strong>å›æ»šå¿…é¡»æ˜¯è‡ªåŠ¨çš„ã€ç§’çº§çš„</strong>ï¼Œä¸èƒ½é äººå€¼ç­ã€‚</p>
</div>

<div class="interview">
  <h4>Q3: GPU èµ„æºå¤ªè´µï¼Œå¦‚ä½•ä¼˜åŒ–æ¨ç†æˆæœ¬ï¼Ÿ</h4>
  <p><strong>ç­”</strong>ï¼šå››å±‚ä¼˜åŒ–ï¼šâ‘  <strong>Model Routing</strong> â€” ç®€å•è¯·æ±‚ç”¨ 7B æ¨¡å‹ï¼ˆä¾¿å®œ 10xï¼‰ï¼Œå¤æ‚è¯·æ±‚æ‰ç”¨ 70B â‘¡ <strong>é‡åŒ–</strong> â€” FP8/INT4 å‡å°‘æ˜¾å­˜å ç”¨ï¼ŒåŒå¡æœåŠ¡æ›´å¤šè¯·æ±‚ â‘¢ <strong>ç¼“å­˜</strong> â€” Semantic Cache é¿å…é‡å¤æ¨ç†ï¼ˆå‘½ä¸­ç‡å¯è¾¾ 20-40%ï¼‰ â‘£ <strong>å…±äº«</strong> â€” ä½æµé‡æ¨¡å‹ç”¨ MPS/Time-slicing å…±äº« GPUã€‚ç»„åˆä½¿ç”¨å¯é™æˆæœ¬ 50-70%ã€‚</p>
</div>

<div class="interview">
  <h4>Q4: Shadow Deployment å’Œ A/B Test åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹é€‰å“ªä¸ªï¼Ÿ</h4>
  <p><strong>ç­”</strong>ï¼š<strong>Shadow</strong> = æ–°æ¨¡å‹çš„å®‰å…¨éªŒè¯é˜¶æ®µï¼Œé›¶ç”¨æˆ·é£é™©ä½†æ— æ³•è¡¡é‡çœŸå®ç”¨æˆ·åé¦ˆï¼ˆå› ä¸ºç”¨æˆ·çœ‹ä¸åˆ°æ–°ç»“æœï¼‰ã€‚é€‚åˆä¸Šçº¿å‰éªŒè¯å»¶è¿Ÿ/é”™è¯¯/è¾“å‡ºè´¨é‡ã€‚<strong>A/B Test</strong> = å·²ç»éªŒè¯å®‰å…¨åï¼Œæµ‹é‡ç”¨æˆ·è¡Œä¸ºå·®å¼‚ï¼ˆCTR, æ»¡æ„åº¦ï¼‰ã€‚æ­£ç¡®æµç¨‹æ˜¯ï¼šShadow å…ˆ â†’ ç¡®è®¤æ—  regression â†’ Canary/A/B â†’ å…¨é‡ã€‚ä¸¤è€…æ˜¯é¡ºåºå…³ç³»ï¼Œä¸æ˜¯æ›¿ä»£å…³ç³»ã€‚</p>
</div>

<h2>ğŸ’¬ æ€è€ƒé¢˜ / Think About This</h2>

<div class="warning">
  <p>ğŸ¤” å¦‚æœä½ çš„æœåŠ¡éœ€è¦åŒæ—¶è¿è¡Œ 50 ä¸ªä¸åŒçš„ fine-tuned æ¨¡å‹ï¼ˆä¾‹å¦‚æ¯ä¸ªä¼ä¸šå®¢æˆ·ä¸€ä¸ªï¼‰ï¼Œä½†æ¯ä¸ªæ¨¡å‹çš„è¯·æ±‚é‡éƒ½å¾ˆä½ï¼ˆå¹³å‡ 1 QPSï¼‰ï¼Œä½ ä¼šæ€ä¹ˆè®¾è®¡ serving æ¶æ„æ¥æ—¢ä¿è¯å»¶è¿Ÿåˆæ§åˆ¶æˆæœ¬ï¼Ÿ</p>
  <p><em>æç¤ºï¼šæƒ³æƒ³ LoRA adapter çš„ç‰¹æ€§ã€æ¨¡å‹çƒ­åŠ è½½ç­–ç•¥ã€ä»¥åŠ Serverless GPU çš„å¯èƒ½æ€§â€¦</em></p>
</div>

<div class="series-nav">
  ğŸ“š <strong>ä¸‹ä¸€ç¯‡é¢„å‘Š</strong>ï¼šDay 4 â€” A/B Testing for MLï¼šå¦‚ä½•ç§‘å­¦åœ°éªŒè¯æ¨¡å‹æ”¹è¿›ï¼Ÿç»Ÿè®¡æ˜¾è‘—æ€§ã€åœ¨çº¿æŒ‡æ ‡è®¾è®¡ã€å®éªŒå¹³å°æ¶æ„ã€‚
</div>

</body>
</html>
