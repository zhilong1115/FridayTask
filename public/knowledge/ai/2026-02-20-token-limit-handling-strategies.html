<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-20 - Token é™åˆ¶å¤„ç†ç­–ç•¥ï¼šLLM Context Window çš„å…­å¤§ç ´è§£ä¹‹é“</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.8; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #1a73e8; border-bottom: 2px solid #1a73e8; padding-bottom: 10px; }
    h2 { color: #3c4043; margin-top: 40px; }
    h3 { color: #5f6368; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 14px; }
    code { font-family: 'SF Mono', 'Fira Code', monospace; }
    .highlight { background: #fff3cd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-blue { background: #d1ecf1; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-green { background: #d4edda; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-red { background: #f8d7da; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .diagram { background: #f0f0f0; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: monospace; white-space: pre; font-size: 13px; line-height: 1.4; overflow-x: auto; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    th, td { border: 1px solid #ddd; padding: 10px; text-align: left; }
    th { background: #f0f0f0; }
    .interview { background: #e8eaf6; padding: 20px; border-radius: 8px; margin: 20px 0; border-left: 4px solid #3f51b5; }
    .series-nav { background: #e3f2fd; padding: 15px; border-radius: 8px; margin: 20px 0; }
    .tag { display: inline-block; background: #e0e0e0; padding: 2px 8px; border-radius: 4px; font-size: 12px; margin: 2px; }
    .strategy-card { border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 15px 0; }
    .strategy-card h4 { margin-top: 0; color: #1a73e8; }
  </style>
</head>
<body>

<div class="series-nav">
  ğŸ“˜ <strong>Phase 2.3: LLM ç‰¹æœ‰é—®é¢˜</strong> â€” Day 1 of 5<br>
  Day 1: Token é™åˆ¶å¤„ç†ç­–ç•¥ (æœ¬ç¯‡) â†’ Day 2: æˆæœ¬ä¼˜åŒ– â†’ Day 3: Hallucination æ£€æµ‹ â†’ Day 4: æµå¼è¾“å‡ºæ¶æ„ â†’ Day 5: Multi-turn å¯¹è¯çŠ¶æ€ç®¡ç†
</div>

<h1>ğŸ”‘ Token é™åˆ¶å¤„ç†ç­–ç•¥ï¼šLLM Context Window çš„å…­å¤§ç ´è§£ä¹‹é“</h1>
<p>
  <span class="tag">Phase 2.3</span>
  <span class="tag">LLM</span>
  <span class="tag">Context Window</span>
  <span class="tag">é¢è¯•é«˜é¢‘</span>
  ğŸ“… 2026-02-20
</p>

<h2>ğŸ“– ä¸ºä»€ä¹ˆ Token é™åˆ¶æ˜¯æ ¸å¿ƒé—®é¢˜ï¼Ÿ</h2>

<div class="bilingual">
  <div class="zh">
    <p>æ¯ä¸ª LLM éƒ½æœ‰ä¸€ä¸ª <strong>context window</strong>â€”â€”å®ƒèƒ½ä¸€æ¬¡å¤„ç†çš„æœ€å¤§ token æ•°ã€‚è¿™ä¸ä»…ä»…æ˜¯è¾“å…¥é™åˆ¶ï¼Œå®ƒç›´æ¥å†³å®šäº†ç³»ç»Ÿè®¾è®¡çš„æ–¹æ–¹é¢é¢ï¼š</p>
    <ul>
      <li><strong>æˆæœ¬</strong>ï¼štoken è¶Šå¤šï¼ŒAPI è´¹ç”¨è¶Šé«˜ï¼ˆé€šå¸¸æ˜¯çº¿æ€§å…³ç³»ï¼‰</li>
      <li><strong>å»¶è¿Ÿ</strong>ï¼šAttention è®¡ç®—æ˜¯ O(nÂ²)ï¼Œcontext ç¿»å€å»¶è¿Ÿå¯èƒ½ç¿» 4 å€</li>
      <li><strong>è´¨é‡</strong>ï¼š"Lost in the Middle" ç°è±¡â€”â€”æ¨¡å‹æ›´å…³æ³¨å¼€å¤´å’Œç»“å°¾çš„ä¿¡æ¯</li>
      <li><strong>æ¶æ„</strong>ï¼šå†³å®šäº†ä½ ç”¨ RAG è¿˜æ˜¯ç›´æ¥å¡è¿› prompt</li>
    </ul>
  </div>
  <div class="en">
    <p>Every LLM has a <strong>context window</strong>â€”the maximum tokens it can process at once. This isn't just an input limit; it shapes every aspect of system design:</p>
    <ul>
      <li><strong>Cost</strong>: More tokens = higher API costs (typically linear)</li>
      <li><strong>Latency</strong>: Attention is O(nÂ²); doubling context can quadruple latency</li>
      <li><strong>Quality</strong>: "Lost in the Middle"â€”models attend more to start and end</li>
      <li><strong>Architecture</strong>: Determines whether you use RAG or stuff everything in prompt</li>
    </ul>
  </div>
</div>

<div class="highlight">
  ğŸ’¡ <strong>é¢è¯•è§†è§’</strong>ï¼šè¢«é—®åˆ° "ä½ çš„ç³»ç»Ÿå¦‚ä½•å¤„ç†è¶…é•¿æ–‡æ¡£?" æ—¶ï¼Œä¸è¦åªå›ç­”ä¸€ç§ç­–ç•¥ã€‚é¢è¯•å®˜æƒ³çœ‹åˆ°ä½ ç†è§£<strong>å¤šç§æ–¹æ¡ˆçš„ tradeoff</strong>ï¼Œå¹¶èƒ½æ ¹æ®åœºæ™¯é€‰æ‹©æœ€ä¼˜ç»„åˆã€‚
</div>

<div class="diagram">
Context Window ç°çŠ¶ (2026)ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model              â”‚ Context Window â”‚ â‰ˆ è‹±æ–‡å­—æ•°        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4o             â”‚ 128K tokens    â”‚ ~96K words        â”‚
â”‚ Claude Opus/Sonnet â”‚ 200K tokens    â”‚ ~150K words       â”‚
â”‚ Gemini 2.0         â”‚ 2M tokens      â”‚ ~1.5M words       â”‚
â”‚ GPT-4 (original)   â”‚ 8K tokens      â”‚ ~6K words         â”‚
â”‚ Llama 3.1          â”‚ 128K tokens    â”‚ ~96K words        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½†ï¼window å¤§ â‰  åº”è¯¥å…¨å¡æ»¡ï¼š
- 128K context çš„ API call â‰ˆ $0.40-2.00/æ¬¡
- å»¶è¿Ÿå¯èƒ½ 30-60 ç§’
- è´¨é‡åœ¨ >60K tokens åæ˜æ˜¾ä¸‹é™
</div>

<h2>ğŸ—ï¸ å…­å¤§ç­–ç•¥æ€»è§ˆ</h2>

<div class="diagram">
ç­–ç•¥é€‰æ‹©å†³ç­–æ ‘ï¼š

                     è¾“å…¥è¶…è¿‡ context windowï¼Ÿ
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚           â”‚           â”‚
           åˆšå¥½è¶…      å¤§å¹…è¶…å‡º     åŠ¨æ€å¢é•¿
         (1.5-2x)     (>10x)     (å¯¹è¯/æµå¼)
              â”‚           â”‚           â”‚
        â”Œâ”€â”€â”€â”€â”€â”˜     â”Œâ”€â”€â”€â”€â”€â”˜     â”Œâ”€â”€â”€â”€â”€â”˜
        â–¼           â–¼           â–¼
   â‘  Truncation  â‘¢ RAG      â‘¤ Sliding Window
   â‘¡ Compression â‘£ MapReduce â‘¥ Hierarchical
                                 Summarization

è·¨å±‚ä¼˜åŒ–ï¼šæ‰€æœ‰ç­–ç•¥éƒ½å¯æ­é… Prompt Compression è¿›ä¸€æ­¥å‡å°‘ token
</div>

<h2>âœ¨ ç­–ç•¥ä¸€ï¼šTruncation & Priority Ordering</h2>

<div class="strategy-card">
  <h4>æ ¸å¿ƒæ€æƒ³ï¼šæœ€ç®€å•ä½†æœ€å®¹æ˜“å‡ºé”™çš„æ–¹æ¡ˆ</h4>
  
  <div class="bilingual">
    <div class="zh">
      <p>ç›´æ¥æˆªæ–­è¶…å‡º window çš„å†…å®¹ã€‚å…³é”®æ˜¯<strong>æˆªä»€ä¹ˆã€ä¿ç•™ä»€ä¹ˆ</strong>ï¼š</p>
      <ul>
        <li><strong>å¤´éƒ¨æˆªæ–­</strong>ï¼šä¿ç•™æœ€æ–°å†…å®¹ï¼ˆå¯¹è¯åœºæ™¯ï¼‰</li>
        <li><strong>å°¾éƒ¨æˆªæ–­</strong>ï¼šä¿ç•™å¼€å¤´ï¼ˆæ–‡æ¡£åˆ†æï¼‰</li>
        <li><strong>ä¸­é—´æˆªæ–­</strong>ï¼šä¿ç•™å¤´å°¾ï¼ˆåˆ©ç”¨ Lost-in-the-Middle ç ”ç©¶ç»“è®ºï¼‰</li>
        <li><strong>ä¼˜å…ˆçº§æˆªæ–­</strong>ï¼šæŒ‰é‡è¦æ€§æ’åºåæˆªæ–­</li>
      </ul>
    </div>
    <div class="en">
      <p>Simply truncate what exceeds the window. The key is <strong>what to cut vs. keep</strong>:</p>
      <ul>
        <li><strong>Head truncation</strong>: Keep recent content (chat)</li>
        <li><strong>Tail truncation</strong>: Keep beginning (document analysis)</li>
        <li><strong>Middle truncation</strong>: Keep head+tail (leveraging Lost-in-the-Middle findings)</li>
        <li><strong>Priority truncation</strong>: Rank by importance, then cut</li>
      </ul>
    </div>
  </div>
</div>

<pre><code>// Priority-based truncation for chat context
interface Message { role: string; content: string; tokens: number; priority: number; }

function truncateContext(messages: Message[], maxTokens: number): Message[] {
  // 1. System prompt æ°¸è¿œä¿ç•™ (priority = Infinity)
  // 2. æœ€è¿‘ 2 è½®å¯¹è¯ä¿ç•™ (priority = 100)
  // 3. åŒ…å« tool results çš„ä¿ç•™ (priority = 80)
  // 4. å…¶ä½™æŒ‰æ—¶é—´å€’åºé€’å‡
  
  const systemMsg = messages.filter(m => m.role === 'system');
  const recentMsgs = messages.slice(-4); // æœ€è¿‘ 2 è½®
  const toolMsgs = messages.filter(m => m.role === 'tool');
  
  // è®¡ç®—å·²ç”¨ token
  let usedTokens = [...systemMsg, ...recentMsgs, ...toolMsgs]
    .reduce((sum, m) => sum + m.tokens, 0);
  
  // å‰©ä½™ç©ºé—´æŒ‰ä¼˜å…ˆçº§å¡«å……
  const remaining = messages
    .filter(m => !systemMsg.includes(m) && !recentMsgs.includes(m))
    .sort((a, b) => b.priority - a.priority);
  
  const result = [...systemMsg];
  for (const msg of remaining) {
    if (usedTokens + msg.tokens <= maxTokens) {
      result.push(msg);
      usedTokens += msg.tokens;
    }
  }
  result.push(...recentMsgs);
  return result;
}</code></pre>

<div class="highlight-red">
  âš ï¸ <strong>é™·é˜±</strong>ï¼šå¾ˆå¤šç³»ç»Ÿç›´æ¥æŒ‰ character count ä¼°ç®— token æ•°ï¼Œä½†ä¸­æ–‡ 1 å­— â‰ˆ 2-3 tokensï¼Œè‹±æ–‡ 1 word â‰ˆ 1.3 tokensã€‚ä¸€å®šè¦ç”¨ tokenizerï¼ˆå¦‚ tiktokenï¼‰ç²¾ç¡®è®¡ç®—ã€‚
</div>

<h2>âœ¨ ç­–ç•¥äºŒï¼šPrompt Compression (LLMLingua)</h2>

<div class="strategy-card">
  <h4>æ ¸å¿ƒæ€æƒ³ï¼šç”¨å°æ¨¡å‹å‹ç¼© promptï¼Œå¤§æ¨¡å‹ç…§æ ·ç†è§£</h4>
  
  <div class="bilingual">
    <div class="zh">
      <p><strong>LLMLingua</strong> (å¾®è½¯ç ”ç©¶é™¢) çš„æ ¸å¿ƒæ´å¯Ÿï¼šprompt ä¸­å¤§é‡ token æ˜¯å†—ä½™çš„ã€‚ç”¨ä¸€ä¸ªå° LM è®¡ç®—æ¯ä¸ª token çš„ perplexityï¼Œä½å›°æƒ‘åº¦çš„ tokenï¼ˆå¯é¢„æµ‹çš„ï¼‰å¯ä»¥å®‰å…¨åˆ é™¤ã€‚</p>
      <p>ä¸‰ä¸ªæ¨¡å—åä½œï¼š</p>
      <ol>
        <li><strong>Budget Controller</strong>ï¼šåˆ†é…æ¯ä¸ªéƒ¨åˆ†çš„å‹ç¼©é¢„ç®—ï¼ˆdemo éƒ¨åˆ†å‹æ›´å¤šï¼Œinstruction éƒ¨åˆ†å‹æ›´å°‘ï¼‰</li>
        <li><strong>Iterative Token Compression</strong>ï¼šé€ token è¯„ä¼°ï¼Œåˆ é™¤ä½ä¿¡æ¯é‡ token</li>
        <li><strong>Distribution Alignment</strong>ï¼šç¡®ä¿å‹ç¼©åçš„åˆ†å¸ƒä¸åŸå§‹å¯¹é½</li>
      </ol>
    </div>
    <div class="en">
      <p><strong>LLMLingua</strong> (Microsoft Research) key insight: most prompt tokens are redundant. A small LM scores each token's perplexity; low-perplexity (predictable) tokens are safely removed.</p>
      <p>Three modules cooperate:</p>
      <ol>
        <li><strong>Budget Controller</strong>: Allocate compression budget per section</li>
        <li><strong>Iterative Token Compression</strong>: Evaluate token-by-token, remove low-information ones</li>
        <li><strong>Distribution Alignment</strong>: Ensure compressed distribution aligns with original</li>
      </ol>
    </div>
  </div>
</div>

<pre><code># LLMLingua-2 ä½¿ç”¨ç¤ºä¾‹
from llmlingua import PromptCompressor

compressor = PromptCompressor(
    model_name="microsoft/llmlingua-2-xlm-roberta-large-meetingbank",
    device_map="cpu"  # å°æ¨¡å‹ï¼ŒCPU å°±å¤Ÿ
)

# åŸå§‹ prompt: 5000 tokens
original_prompt = """
[Very long system prompt with examples, context docs, user history...]
"""

# å‹ç¼©åˆ° ~2000 tokens (60% reduction)
compressed = compressor.compress_prompt(
    original_prompt,
    rate=0.4,                    # ä¿ç•™ 40% çš„ token
    force_tokens=["\\n", "?"],   # å¼ºåˆ¶ä¿ç•™çš„ token
    chunk_end_tokens=[".", "\\n"] # chunk è¾¹ç•Œ
)

print(f"Original: {compressed['origin_tokens']} tokens")
print(f"Compressed: {compressed['compressed_tokens']} tokens")
print(f"Ratio: {compressed['ratio']:.1%}")
# Output: Original: 5000 tokens â†’ Compressed: 2000 tokens â†’ Ratio: 40.0%</code></pre>

<div class="diagram">
LLMLingua å‹ç¼©æ•ˆæœå¯¹ç…§ï¼š

åŸå§‹ prompt (10 tokens):
  "The weather in San Francisco is usually foggy and cold"

å‹ç¼©å (6 tokens, rate=0.6):
  "weather San Francisco usually foggy cold"

å¤§æ¨¡å‹ç†è§£ï¼šâœ… å‡ ä¹æ— æŸ
æˆæœ¬èŠ‚çœï¼š40%
å»¶è¿Ÿå‡å°‘ï¼š~35%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compression Rate â”‚ Quality Loss â”‚ Savings â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0.8 (ä¿ç•™80%)    â”‚ < 1%         â”‚ 20%     â”‚
â”‚ 0.6 (ä¿ç•™60%)    â”‚ 2-3%         â”‚ 40%     â”‚
â”‚ 0.4 (ä¿ç•™40%)    â”‚ 5-8%         â”‚ 60%     â”‚
â”‚ 0.2 (ä¿ç•™20%)    â”‚ 15-25%       â”‚ 80%     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Sweet spot: 0.5-0.6 (ä¿ç•™ 50-60%)
</div>

<h2>âœ¨ ç­–ç•¥ä¸‰ï¼šRAG (Retrieval-Augmented Generation)</h2>

<div class="strategy-card">
  <h4>æ ¸å¿ƒæ€æƒ³ï¼šä¸å¡å…¨éƒ¨æ–‡æ¡£ï¼Œåªæ£€ç´¢æœ€ç›¸å…³çš„ç‰‡æ®µ</h4>
  
  <div class="bilingual">
    <div class="zh">
      <p>è¿™æ˜¯æˆ‘ä»¬ Phase 1.3 æ·±å…¥å­¦è¿‡çš„ç­–ç•¥ã€‚åœ¨ token é™åˆ¶çš„è¯­å¢ƒä¸‹ï¼ŒRAG çš„æœ¬è´¨æ˜¯ï¼š<strong>æŠŠ "ä¿¡æ¯é€‰æ‹©" ä»æ¨¡å‹å†…éƒ¨ç§»åˆ°æ¨¡å‹å¤–éƒ¨</strong>ã€‚</p>
      <p>å…³é”®å†³ç­–ç‚¹ï¼š</p>
      <ul>
        <li>æ£€ç´¢ top-k ä¸ª chunkï¼Œk çš„é€‰æ‹©ç›´æ¥å½±å“ token ä½¿ç”¨</li>
        <li>Chunk size è¶Šå°ï¼Œæ£€ç´¢è¶Šç²¾å‡†ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡</li>
        <li>Reranking åæˆªæ–­ vs ç›´æ¥æˆªæ–­</li>
      </ul>
    </div>
    <div class="en">
      <p>This is what we covered deeply in Phase 1.3. In the token limit context, RAG's essence is: <strong>moving "information selection" from inside the model to outside</strong>.</p>
      <p>Key decisions:</p>
      <ul>
        <li>Retrieve top-k chunksâ€”k directly impacts token budget</li>
        <li>Smaller chunks = more precise retrieval but may lose context</li>
        <li>Rerank-then-truncate vs direct truncation</li>
      </ul>
    </div>
  </div>
</div>

<pre><code>// Token-aware RAG retrieval
function retrieveWithTokenBudget(
  query: string,
  maxContextTokens: number,
  reservedTokens: number  // for system prompt + output buffer
): Chunk[] {
  const budget = maxContextTokens - reservedTokens;
  
  // 1. æ£€ç´¢è¾ƒå¤šå€™é€‰
  const candidates = vectorSearch(query, topK: 20);
  
  // 2. Rerank
  const reranked = crossEncoderRerank(query, candidates);
  
  // 3. è´ªå¿ƒå¡«å……ï¼šæŒ‰ç›¸å…³åº¦ä¾æ¬¡åŠ å…¥ï¼Œç›´åˆ° budget è€—å°½
  const selected: Chunk[] = [];
  let usedTokens = 0;
  
  for (const chunk of reranked) {
    if (usedTokens + chunk.tokens <= budget) {
      selected.push(chunk);
      usedTokens += chunk.tokens;
    }
    // æ³¨æ„ï¼šä¸ breakï¼Œåé¢å¯èƒ½æœ‰æ›´å°çš„ chunk èƒ½å¡è¿›å»
    // ä½†å¦‚æœç›¸å…³åº¦å·²ç»å¾ˆä½ï¼Œå°± break
    if (chunk.relevanceScore < 0.3) break;
  }
  
  // 4. æŒ‰åŸæ–‡é¡ºåºæ’åˆ—ï¼ˆæå‡è¿è´¯æ€§ï¼‰
  return selected.sort((a, b) => a.originalPosition - b.originalPosition);
}</code></pre>

<h2>âœ¨ ç­–ç•¥å››ï¼šMapReduce / Hierarchical Processing</h2>

<div class="strategy-card">
  <h4>æ ¸å¿ƒæ€æƒ³ï¼šåˆ†è€Œæ²»ä¹‹ï¼ŒåŒ–æ•´ä¸ºé›¶</h4>
  
  <div class="bilingual">
    <div class="zh">
      <p>å½“æ–‡æ¡£å®åœ¨å¤ªé•¿ï¼ˆæ¯”å¦‚ 100 é¡µ PDFï¼‰ï¼Œå•æ¬¡ API call æ”¾ä¸ä¸‹æ—¶ï¼Œä½¿ç”¨ <strong>MapReduce æ¨¡å¼</strong>ï¼š</p>
      <ol>
        <li><strong>Split</strong>ï¼šå°†æ–‡æ¡£æ‹†åˆ†ä¸º N ä¸ª chunk</li>
        <li><strong>Map</strong>ï¼šå¯¹æ¯ä¸ª chunk ç‹¬ç«‹å¤„ç†ï¼ˆæ‘˜è¦/æå–/åˆ†æï¼‰</li>
        <li><strong>Reduce</strong>ï¼šåˆå¹¶æ‰€æœ‰ Map ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ</li>
      </ol>
      <p>å˜ä½“ï¼š<strong>Refine æ¨¡å¼</strong>â€”â€”ä¸²è¡Œå¤„ç†ï¼Œæ¯æ¬¡å°†ä¸Šä¸€æ¬¡çš„ç»“æœ + æ–° chunk ä¸€èµ·é€å…¥æ¨¡å‹ï¼Œé€æ­¥ç²¾ç‚¼ç­”æ¡ˆã€‚</p>
    </div>
    <div class="en">
      <p>When documents are too long (e.g., 100-page PDF), use the <strong>MapReduce pattern</strong>:</p>
      <ol>
        <li><strong>Split</strong>: Divide document into N chunks</li>
        <li><strong>Map</strong>: Process each chunk independently (summarize/extract/analyze)</li>
        <li><strong>Reduce</strong>: Merge all Map results into final answer</li>
      </ol>
      <p>Variant: <strong>Refine mode</strong>â€”process sequentially, feeding previous result + new chunk each time, progressively refining the answer.</p>
    </div>
  </div>
</div>

<div class="diagram">
MapReduce vs Refineï¼š

ã€MapReduce â€” å¹¶è¡Œï¼Œå¿«ä½†å¯èƒ½ä¸¢å…³è”ä¿¡æ¯ã€‘

  Doc_1 â”€â”€â†’ LLM â”€â”€â†’ Summary_1 â”€â”
  Doc_2 â”€â”€â†’ LLM â”€â”€â†’ Summary_2 â”€â”¼â”€â”€â†’ LLM â”€â”€â†’ Final Answer
  Doc_3 â”€â”€â†’ LLM â”€â”€â†’ Summary_3 â”€â”¤
  Doc_4 â”€â”€â†’ LLM â”€â”€â†’ Summary_4 â”€â”˜

  âœ… å¯å¹¶è¡Œ (å¿«)    âŒ è·¨ chunk å…³è”å¯èƒ½ä¸¢å¤±
  âœ… é€‚åˆæ‘˜è¦/æå–   âŒ ä¸é€‚åˆéœ€è¦å…¨å±€ç†è§£çš„ä»»åŠ¡

ã€Refine â€” ä¸²è¡Œï¼Œæ…¢ä½†ä¿ç•™è¿è´¯æ€§ã€‘

  Doc_1 â”€â”€â†’ LLM â”€â”€â†’ Answer_v1
  Answer_v1 + Doc_2 â”€â”€â†’ LLM â”€â”€â†’ Answer_v2
  Answer_v2 + Doc_3 â”€â”€â†’ LLM â”€â”€â†’ Answer_v3
  Answer_v3 + Doc_4 â”€â”€â†’ LLM â”€â”€â†’ Final Answer

  âœ… ä¿ç•™ä¸Šä¸‹æ–‡è¿è´¯æ€§  âŒ ä¸²è¡Œï¼ˆæ…¢ï¼‰
  âœ… é€æ­¥ç²¾ç‚¼          âŒ åé¢ chunk æƒé‡å¯èƒ½åé«˜

ã€Hierarchical â€” é‡‘å­—å¡”ï¼Œå¹³è¡¡æ–¹æ¡ˆã€‘

  Level 0: [C1][C2][C3][C4][C5][C6][C7][C8]
              â†“     â†“     â†“     â†“
  Level 1:  [S12]  [S34]  [S56]  [S78]
                â†“           â†“
  Level 2:    [S1234]    [S5678]
                    â†“
  Level 3:      [Final Summary]

  âœ… å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡  âœ… å¯å¹¶è¡Œæ¯å±‚
  âŒ å¤šå±‚å¯èƒ½ç´¯ç§¯ä¿¡æ¯æŸå¤±
</div>

<pre><code>// MapReduce implementation
async function mapReduceSummarize(
  document: string,
  chunkSize: number = 4000,
  question: string
): Promise&lt;string&gt; {
  // Split
  const chunks = splitIntoChunks(document, chunkSize);
  
  // Map (parallel)
  const mapPromises = chunks.map((chunk, i) =>
    llm.complete({
      prompt: `Analyze this section (${i+1}/${chunks.length}) 
               in context of: "${question}"
               
               Section content:
               ${chunk}
               
               Extract key points relevant to the question.`,
      maxTokens: 500
    })
  );
  const mapResults = await Promise.all(mapPromises);
  
  // Reduce
  const combined = mapResults.join("\n---\n");
  
  // å¦‚æœ combined ä»ç„¶å¤ªé•¿ï¼Œé€’å½’ reduce
  if (countTokens(combined) > chunkSize) {
    return mapReduceSummarize(combined, chunkSize, question);
  }
  
  return llm.complete({
    prompt: `Based on these extracted points from a long document,
             provide a comprehensive answer to: "${question}"
             
             Extracted points:
             ${combined}`,
    maxTokens: 1000
  });
}</code></pre>

<h2>âœ¨ ç­–ç•¥äº”ï¼šSliding Window & Conversation Memory</h2>

<div class="strategy-card">
  <h4>æ ¸å¿ƒæ€æƒ³ï¼šæ»‘åŠ¨çª—å£ + è®°å¿†å‹ç¼©ï¼Œå¤„ç†æ— é™å¯¹è¯</h4>
  
  <div class="bilingual">
    <div class="zh">
      <p>å¯¹è¯åœºæ™¯çš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼šä¸Šä¸‹æ–‡<strong>æŒç»­å¢é•¿</strong>ã€‚Phase 1.2 å­¦è¿‡çš„ Memory ç³»ç»Ÿå°±æ˜¯ä¸ºæ­¤è®¾è®¡çš„ï¼š</p>
      <ul>
        <li><strong>Buffer Window</strong>ï¼šåªä¿ç•™æœ€è¿‘ K è½®ï¼ˆæœ€ç®€å•ï¼‰</li>
        <li><strong>Summary Memory</strong>ï¼šå°†æ—§å¯¹è¯å‹ç¼©ä¸ºæ‘˜è¦</li>
        <li><strong>Token Buffer</strong>ï¼šä¿ç•™æœ€è¿‘ N tokens è€Œé K è½®</li>
        <li><strong>Hybrid</strong>ï¼šæ‘˜è¦ + æœ€è¿‘å‡ è½®å®Œæ•´ä¿ç•™</li>
      </ul>
    </div>
    <div class="en">
      <p>Chat scenarios face a unique challenge: context <strong>grows continuously</strong>. The Memory systems from Phase 1.2 address this:</p>
      <ul>
        <li><strong>Buffer Window</strong>: Keep only last K turns (simplest)</li>
        <li><strong>Summary Memory</strong>: Compress old conversations into summaries</li>
        <li><strong>Token Buffer</strong>: Keep last N tokens instead of K turns</li>
        <li><strong>Hybrid</strong>: Summary + last few turns preserved fully</li>
      </ul>
    </div>
  </div>
</div>

<pre><code>// Hybrid sliding window with summary compression
class ConversationManager {
  private maxTokens = 100_000;
  private reservedForSummary = 2000;
  private reservedForSystem = 3000;
  private reservedForOutput = 4000;
  
  async buildContext(history: Message[]): Promise&lt;Message[]&gt; {
    const budget = this.maxTokens - this.reservedForSystem - this.reservedForOutput;
    
    // 1. å§‹ç»ˆä¿ç•™æœ€è¿‘ 3 è½® (â‰ˆ6 messages)
    const recentMessages = history.slice(-6);
    let recentTokens = this.countTokens(recentMessages);
    
    // 2. å¦‚æœæœ€è¿‘ 3 è½®å°±è¶… budgetï¼Œæˆªæ–­æœ€æ—©çš„
    if (recentTokens > budget) {
      return this.truncateMessages(recentMessages, budget);
    }
    
    // 3. å‰©ä½™ç©ºé—´ï¼šå°è¯•å¡«å…¥æ›´å¤šå†å²
    const remainingBudget = budget - recentTokens - this.reservedForSummary;
    const olderMessages = history.slice(0, -6);
    
    if (this.countTokens(olderMessages) <= remainingBudget) {
      // å…¨éƒ¨æ”¾å¾—ä¸‹ï¼Œç›´æ¥ç”¨
      return [...olderMessages, ...recentMessages];
    }
    
    // 4. æ”¾ä¸ä¸‹ï¼šå‹ç¼©æ—§æ¶ˆæ¯ä¸ºæ‘˜è¦
    const summary = await this.summarize(olderMessages);
    return [
      { role: 'system', content: `Previous conversation summary:\n${summary}` },
      ...recentMessages
    ];
  }
  
  private async summarize(messages: Message[]): Promise&lt;string&gt; {
    return llm.complete({
      prompt: `Summarize this conversation, preserving:
               - Key decisions and agreements
               - Important facts mentioned
               - User preferences expressed
               - Unresolved questions
               
               Conversation:
               ${messages.map(m => `${m.role}: ${m.content}`).join('\n')}`,
      maxTokens: this.reservedForSummary
    });
  }
}</code></pre>

<h2>âœ¨ ç­–ç•¥å…­ï¼šContext Window Extension (æ¨¡å‹å±‚)</h2>

<div class="strategy-card">
  <h4>æ ¸å¿ƒæ€æƒ³ï¼šä»æ¨¡å‹æ¶æ„å±‚é¢æ‰©å±• context window</h4>
  
  <div class="bilingual">
    <div class="zh">
      <p>å‰ 5 ç§ç­–ç•¥éƒ½æ˜¯<strong>åº”ç”¨å±‚</strong>çš„â€”â€”å›´ç»•å›ºå®š window åšæ–‡ç« ã€‚ç¬¬å…­ç§æ˜¯<strong>æ¨¡å‹å±‚</strong>çš„â€”â€”ç›´æ¥æ‰©å¤§ windowï¼š</p>
      <ul>
        <li><strong>RoPE Scaling</strong>ï¼šä¿®æ”¹ Rotary Position Embedding çš„é¢‘ç‡ï¼Œå°† 4K context æ¨¡å‹æ‰©å±•åˆ° 128K+</li>
        <li><strong>ALiBi</strong>ï¼šç”¨çº¿æ€§åç½®æ›¿ä»£ä½ç½®ç¼–ç ï¼Œå¤©ç„¶æ”¯æŒå¤–æ¨</li>
        <li><strong>Sliding Window Attention</strong>ï¼šæ¯å±‚åªçœ‹å±€éƒ¨çª—å£ï¼Œé€šè¿‡å±‚å è·å¾—å…¨å±€è§†é‡ï¼ˆMistral ç”¨æ³•ï¼‰</li>
        <li><strong>Ring Attention</strong>ï¼šåˆ†å¸ƒå¼ attention è®¡ç®—ï¼Œç†è®ºä¸Šæ— é™ context</li>
      </ul>
    </div>
    <div class="en">
      <p>The first 5 strategies are <strong>application-level</strong>â€”working around a fixed window. The sixth is <strong>model-level</strong>â€”extending the window itself:</p>
      <ul>
        <li><strong>RoPE Scaling</strong>: Modify rotary position embedding frequencies to extend 4K models to 128K+</li>
        <li><strong>ALiBi</strong>: Replace position encodings with linear bias, naturally extrapolates</li>
        <li><strong>Sliding Window Attention</strong>: Each layer attends locally; stacked layers give global view (Mistral)</li>
        <li><strong>Ring Attention</strong>: Distributed attention across devicesâ€”theoretically infinite context</li>
      </ul>
    </div>
  </div>
</div>

<div class="diagram">
RoPE Scaling ç›´è§‰ç†è§£ï¼š

åŸå§‹ RoPE (è®­ç»ƒäº 4K context):
  Position 0    1000   2000   3000   4000   [è¶…å‡ºâ†’å´©æºƒ]
  è§’åº¦Î¸:  0Â°    90Â°    180Â°   270Â°   360Â°

NTK-aware Scaling (æ‰©å±•åˆ° 16K):
  Position 0    4000   8000   12000  16000  [å¹³æ»‘å¤–æ¨]
  è§’åº¦Î¸:  0Â°    90Â°    180Â°   270Â°   360Â°
  
  â†’ æŠŠ "å°ºå­" æ‹‰é•¿ï¼Œè®©åŒæ ·çš„è§’åº¦èŒƒå›´è¦†ç›–æ›´å¤šä½ç½®
  â†’ ä»£ä»·ï¼šç›¸é‚»ä½ç½®çš„åŒºåˆ†åº¦é™ä½

YaRN (Yet another RoPE extensioN):
  â†’ é€‰æ‹©æ€§ç¼©æ”¾ï¼šé«˜é¢‘ç»´åº¦ä¿æŒä¸å˜ï¼Œåªç¼©æ”¾ä½é¢‘ç»´åº¦
  â†’ æ›´å¥½çš„è¿‘è·ç¦»ç²¾åº¦ + è¿œè·ç¦»å¤–æ¨
</div>

<h2>ğŸ¯ ç­–ç•¥é€‰å‹çŸ©é˜µ</h2>

<table>
  <tr>
    <th>ç­–ç•¥</th>
    <th>é€‚ç”¨åœºæ™¯</th>
    <th>å‹ç¼©ç‡</th>
    <th>è´¨é‡æŸå¤±</th>
    <th>å®ç°å¤æ‚åº¦</th>
    <th>å»¶è¿Ÿå½±å“</th>
  </tr>
  <tr>
    <td>â‘  Truncation</td>
    <td>å¯¹è¯ã€ç®€å•æº¢å‡º</td>
    <td>ä»»æ„</td>
    <td>ä¸­-é«˜</td>
    <td>â­</td>
    <td>æ— é¢å¤–</td>
  </tr>
  <tr>
    <td>â‘¡ Prompt Compression</td>
    <td>é•¿ prompt ä¼˜åŒ–</td>
    <td>40-60%</td>
    <td>ä½</td>
    <td>â­â­</td>
    <td>+50-200ms</td>
  </tr>
  <tr>
    <td>â‘¢ RAG</td>
    <td>çŸ¥è¯†åº“é—®ç­”</td>
    <td>95%+</td>
    <td>ä½-ä¸­</td>
    <td>â­â­â­</td>
    <td>+100-500ms</td>
  </tr>
  <tr>
    <td>â‘£ MapReduce</td>
    <td>é•¿æ–‡æ¡£åˆ†æ</td>
    <td>80-95%</td>
    <td>ä¸­</td>
    <td>â­â­</td>
    <td>+NÃ—å•æ¬¡å»¶è¿Ÿ</td>
  </tr>
  <tr>
    <td>â‘¤ Sliding Window</td>
    <td>é•¿å¯¹è¯</td>
    <td>åŠ¨æ€</td>
    <td>ä½-ä¸­</td>
    <td>â­â­</td>
    <td>+æ‘˜è¦å»¶è¿Ÿ</td>
  </tr>
  <tr>
    <td>â‘¥ Window Extension</td>
    <td>è‡ªéƒ¨ç½²æ¨¡å‹</td>
    <td>æ— éœ€å‹ç¼©</td>
    <td>ä½</td>
    <td>â­â­â­â­</td>
    <td>O(nÂ²)å¢é•¿</td>
  </tr>
</table>

<h2>ğŸ”€ ç”Ÿäº§çº§ç»„åˆæ–¹æ¡ˆ</h2>

<div class="highlight-green">
  <h4>æ¨èç»„åˆï¼šå¤§å¤šæ•° LLM åº”ç”¨</h4>
  <p><strong>RAG + Prompt Compression + Priority Truncation</strong></p>
  <ol>
    <li>RAG æ£€ç´¢ç›¸å…³ chunks (top-10, ~8000 tokens)</li>
    <li>LLMLingua å‹ç¼©åˆ° 60% (~4800 tokens)</li>
    <li>å¦‚æœä»è¶… budgetï¼ŒæŒ‰ rerank score æˆªæ–­ä½åˆ† chunks</li>
    <li>ç³»ç»Ÿ prompt + å‹ç¼© context + æœ€è¿‘å¯¹è¯ â†’ é€å…¥æ¨¡å‹</li>
  </ol>
  <p>æ•ˆæœï¼šå¤„ç† 100K+ æ–‡æ¡£ï¼Œå®é™…åªç”¨ ~10K tokensï¼Œæˆæœ¬é™ä½ 90%</p>
</div>

<pre><code>// Production token budget manager
class TokenBudgetManager {
  constructor(private config: {
    maxContextTokens: number;   // e.g., 128000
    systemPromptTokens: number; // e.g., 2000
    outputReserve: number;      // e.g., 4000
    safetyMargin: number;       // e.g., 0.9 (use 90% of budget)
  }) {}

  get availableBudget(): number {
    return Math.floor(
      (this.config.maxContextTokens * this.config.safetyMargin)
      - this.config.systemPromptTokens
      - this.config.outputReserve
    );
  }

  allocate(): TokenBudgetAllocation {
    const total = this.availableBudget;
    return {
      ragContext: Math.floor(total * 0.50),      // 50% for retrieved docs
      conversationHistory: Math.floor(total * 0.30), // 30% for chat history
      toolResults: Math.floor(total * 0.15),     // 15% for tool outputs
      buffer: Math.floor(total * 0.05),          // 5% safety buffer
    };
  }

  // åŠ¨æ€è°ƒæ•´ï¼šå¦‚æœæŸéƒ¨åˆ†ç”¨ä¸å®Œï¼Œé‡æ–°åˆ†é…ç»™å…¶ä»–éƒ¨åˆ†
  reallocate(actual: Partial&lt;TokenBudgetAllocation&gt;): TokenBudgetAllocation {
    const allocation = this.allocate();
    let surplus = 0;
    
    for (const [key, actualTokens] of Object.entries(actual)) {
      const budgeted = allocation[key as keyof TokenBudgetAllocation];
      if (actualTokens !== undefined && actualTokens < budgeted) {
        surplus += budgeted - actualTokens;
        allocation[key as keyof TokenBudgetAllocation] = actualTokens;
      }
    }
    
    // æŠŠ surplus ç»™æœ€éœ€è¦çš„éƒ¨åˆ†
    allocation.ragContext += surplus;
    return allocation;
  }
}</code></pre>

<h2>ğŸ§© ä¸ Phase 2.1-2.2 çŸ¥è¯†çš„è¿æ¥</h2>

<div class="highlight-blue">
  <h4>è¿™äº›ç­–ç•¥åœ¨ä¹‹å‰çš„ç³»ç»Ÿè®¾è®¡ä¸­éƒ½å‡ºç°è¿‡ï¼š</h4>
  <table>
    <tr><th>ç³»ç»Ÿ</th><th>ç”¨åˆ°çš„ Token ç­–ç•¥</th></tr>
    <tr><td>ChatGPT (Day 1)</td><td>Sliding Window + Summary Memory</td></tr>
    <tr><td>æ¨èç³»ç»Ÿ (Day 2)</td><td>Feature truncation (embedding å‹ç¼©)</td></tr>
    <tr><td>è¯­ä¹‰æœç´¢ (Day 3)</td><td>RAG (æ£€ç´¢æ›¿ä»£å…¨æ–‡)</td></tr>
    <tr><td>RAG çŸ¥è¯†åº“ (Day 4)</td><td>RAG + Hierarchical Chunking</td></tr>
    <tr><td>Code Review (Day 5)</td><td>Context Engineering (é€‰æ‹©æ€§åŒ…å«ä»£ç )</td></tr>
    <tr><td>Feature Store (Day 7)</td><td>ç‚¹æŸ¥ vs å…¨é‡ (ç±»æ¯” RAG vs å…¨æ–‡)</td></tr>
    <tr><td>Latency vs Quality (Day 10)</td><td>Prompt Compression + Model Cascade</td></tr>
  </table>
  <p>Token ç®¡ç†ä¸æ˜¯å­¤ç«‹æŠ€æœ¯â€”â€”å®ƒæ˜¯<strong>æ¯ä¸ª LLM ç³»ç»Ÿè®¾è®¡çš„åº•å±‚çº¦æŸ</strong>ã€‚</p>
</div>

<h2>ğŸ¤ é¢è¯•é«˜é¢‘é—®é¢˜</h2>

<div class="interview">
  <h4>Q1: ä½ çš„ RAG ç³»ç»Ÿéœ€è¦å¤„ç† 500 é¡µçš„ PDFï¼Œç”¨æˆ·é—®äº†ä¸€ä¸ªéœ€è¦å…¨æ–‡ç†è§£çš„é—®é¢˜ï¼ˆå¦‚ "æ€»ç»“è¿™ä»½åˆåŒçš„å…³é”®é£é™©ç‚¹"ï¼‰ï¼Œä½ æ€ä¹ˆå¤„ç†ï¼Ÿ</h4>
  <p><strong>å‚è€ƒç­”æ¡ˆ</strong>ï¼š</p>
  <ul>
    <li>ä¸èƒ½åªç”¨ RAG top-kâ€”â€”éœ€è¦å…¨æ–‡è¦†ç›–ã€‚ç”¨ <strong>MapReduce</strong>ï¼šå…ˆå¯¹æ¯ä¸ª chunk æå–é£é™©ç›¸å…³ä¿¡æ¯ (Map)ï¼Œç„¶ååˆå¹¶æ‰€æœ‰æå–ç»“æœç”Ÿæˆæœ€ç»ˆæ€»ç»“ (Reduce)</li>
    <li>ä¼˜åŒ–ï¼šMap é˜¶æ®µå¯ä»¥å¹¶è¡Œï¼Œç”¨ cheaper model (GPT-4o-mini)ï¼›Reduce é˜¶æ®µç”¨ stronger model (GPT-4o)</li>
    <li>å¦‚æœ Reduce ç»“æœä»ç„¶å¤ªé•¿ï¼Œé€’å½’ Reduce æˆ–ç”¨ Hierarchical æ¨¡å¼</li>
    <li>æˆæœ¬ä¼°ç®—ï¼š500 é¡µ â‰ˆ 250K tokensï¼ŒMap é˜¶æ®µ ~$0.04ï¼ŒReduce ~$0.01ï¼Œæ€»è®¡ <$0.10</li>
  </ul>
</div>

<div class="interview">
  <h4>Q2: ä½ çš„ chatbot åœ¨ç¬¬ 50 è½®å¯¹è¯åå¼€å§‹ "å¿˜è®°" æ—©æœŸçš„ä¿¡æ¯ï¼Œæ€ä¹ˆè§£å†³ï¼Ÿ</h4>
  <p><strong>å‚è€ƒç­”æ¡ˆ</strong>ï¼š</p>
  <ul>
    <li>è¿™æ˜¯ç»å…¸çš„ context overflow é—®é¢˜ã€‚åˆ†ä¸‰å±‚è§£å†³ï¼š</li>
    <li><strong>L1 - Summary Memory</strong>ï¼šæ¯ 10 è½®è‡ªåŠ¨å°†æ—§å¯¹è¯å‹ç¼©ä¸ºæ‘˜è¦ï¼Œä¿ç•™å…³é”®äº‹å®</li>
    <li><strong>L2 - Entity Memory</strong>ï¼šæå–å¯¹è¯ä¸­çš„å®ä½“å’Œå…³ç³»ï¼Œå­˜å…¥ç»“æ„åŒ–å­˜å‚¨</li>
    <li><strong>L3 - Vector Memory</strong>ï¼šå°†å†å²å¯¹è¯ embed åå­˜å…¥å‘é‡åº“ï¼Œéœ€è¦æ—¶æ£€ç´¢ç›¸å…³ç‰‡æ®µ</li>
    <li>ç»„åˆç­–ç•¥ï¼šSystem prompt åŒ…å« entity memory + summaryï¼›RAG æ£€ç´¢è¡¥å……å…·ä½“ç»†èŠ‚</li>
    <li>æŒ‡æ ‡ç›‘æ§ï¼šå®šæœŸæµ‹è¯• "ç¬¬ N è½®è¿˜èƒ½å¦æ­£ç¡®å¼•ç”¨ç¬¬ 1 è½®çš„ä¿¡æ¯"</li>
  </ul>
</div>

<div class="interview">
  <h4>Q3: Prompt Compression å’Œç›´æ¥æˆªæ–­æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ</h4>
  <p><strong>å‚è€ƒç­”æ¡ˆ</strong>ï¼š</p>
  <ul>
    <li><strong>æˆªæ–­</strong>æ˜¯ lossyâ€”â€”ç›´æ¥ä¸¢å¼ƒä¿¡æ¯å—ã€‚<strong>å‹ç¼©</strong>æ˜¯ lossy ä½† information-preservingâ€”â€”ä¿ç•™è¯­ä¹‰ï¼Œå»é™¤å†—ä½™</li>
    <li>ç±»æ¯”ï¼šæˆªæ–­ = æ’•æ‰ä¹¦çš„å‡ é¡µï¼›å‹ç¼© = ç”¨è‡ªå·±çš„è¯ç¼©å†™å…¨ä¹¦</li>
    <li>ç”¨æˆªæ–­ï¼šä¿¡æ¯å—è¾¹ç•Œæ¸…æ™°ï¼ˆå¦‚ç‹¬ç«‹çš„ chat messagesï¼‰ï¼Œä¼˜å…ˆçº§æ˜ç¡®</li>
    <li>ç”¨å‹ç¼©ï¼šä¿¡æ¯å¯†åº¦ä¸å‡åŒ€ï¼ˆå¦‚é•¿æ–‡æ¡£ï¼‰ï¼Œéœ€è¦ä¿ç•™å…¨å±€ä¿¡æ¯</li>
    <li>æœ€ä½³å®è·µï¼šå…ˆå‹ç¼©ï¼ˆLLMLinguaï¼‰ï¼Œå¦‚æœä»è¶…é¢„ç®—å†æŒ‰ä¼˜å…ˆçº§æˆªæ–­</li>
  </ul>
</div>

<div class="interview">
  <h4>Q4: è®¾è®¡ä¸€ä¸ª Token Budget Managerï¼Œéœ€è¦è€ƒè™‘å“ªäº›å› ç´ ï¼Ÿ</h4>
  <p><strong>å‚è€ƒç­”æ¡ˆ</strong>ï¼š</p>
  <ul>
    <li><strong>é™æ€åˆ†é…</strong>ï¼šsystem prompt / context / history / output å„å æ¯”ä¾‹</li>
    <li><strong>åŠ¨æ€é‡åˆ†é…</strong>ï¼šå¦‚æœæŸéƒ¨åˆ†ç”¨ä¸å®Œï¼ˆå¦‚æ—  tool resultsï¼‰ï¼ŒæŠŠä½™é‡ç»™å…¶ä»–éƒ¨åˆ†</li>
    <li><strong>å®‰å…¨è¾¹é™…</strong>ï¼šé¢„ç•™ 5-10% bufferï¼Œå› ä¸º tokenizer è®¡ç®—å¯èƒ½æœ‰è¯¯å·®</li>
    <li><strong>è¾“å‡ºé¢„ç•™</strong>ï¼šmax_tokens è®¾å¤ªå¤§æµªè´¹é¢„ç®—ï¼Œå¤ªå°å›ç­”æˆªæ–­</li>
    <li><strong>å¤šæ¨¡å‹é€‚é…</strong>ï¼šä¸åŒæ¨¡å‹ context window ä¸åŒï¼ŒManager éœ€è¦å‚æ•°åŒ–</li>
    <li><strong>ç›‘æ§å‘Šè­¦</strong>ï¼štoken ä½¿ç”¨ç‡ >90% è§¦å‘å‘Šè­¦ï¼Œ>95% è‡ªåŠ¨é™çº§ï¼ˆå‹ç¼©æˆ–æˆªæ–­ï¼‰</li>
  </ul>
</div>

<h2>ğŸ’­ æ€è€ƒé¢˜</h2>

<div class="highlight">
  <p><strong>Question</strong>ï¼šGoogle Gemini å·²ç»æ”¯æŒ 2M tokens çš„ context windowã€‚å¦‚æœæœªæ¥æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒæ— é™ contextï¼Œä¸Šé¢è¿™äº›ç­–ç•¥è¿˜æœ‰æ„ä¹‰å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ</p>
  <p><em>æç¤ºï¼šæƒ³æƒ³æˆæœ¬ã€å»¶è¿Ÿã€Lost-in-the-Middleã€ä»¥åŠ "èƒ½æ”¾ â‰  åº”è¯¥æ”¾"</em></p>
</div>

<hr>
<p style="color: #888; font-size: 14px;">
  ğŸ“š Knowledge Agent | Phase 2.3: LLM ç‰¹æœ‰é—®é¢˜ Day 1/5 | 2026-02-20<br>
  ä¸‹ä¸€ç¯‡é¢„å‘Šï¼šDay 2 â€” æˆæœ¬ä¼˜åŒ–ï¼ˆç¼“å­˜ã€è·¯ç”±ã€å°æ¨¡å‹åˆ†æµï¼‰
</p>

</body>
</html>
