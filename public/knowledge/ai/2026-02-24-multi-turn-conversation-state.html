<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-24 - Multi-turn å¯¹è¯çŠ¶æ€ç®¡ç†</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.7; color: #333; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; }
    h2 { color: #1a73e8; border-bottom: 1px solid #e8eaed; padding-bottom: 8px; margin-top: 40px; }
    h3 { color: #5f6368; }
    .architecture { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 13px; }
    .highlight { background: #fff3cd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight h4 { margin-top: 0; color: #856404; }
    .diagram { text-align: center; margin: 20px 0; }
    .follow-up { margin-top: 30px; padding: 15px; border: 1px dashed #dadce0; border-radius: 8px; }
    .series-nav { background: #e8f0fe; padding: 12px; border-radius: 8px; margin: 15px 0; font-size: 14px; }
    table { border-collapse: collapse; width: 100%; margin: 15px 0; }
    th, td { border: 1px solid #dadce0; padding: 10px; text-align: left; }
    th { background: #f8f9fa; }
    .interview { background: #fce4ec; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .interview h4 { color: #c62828; margin-top: 0; }
    code { background: #f1f3f4; padding: 2px 6px; border-radius: 3px; font-size: 13px; }
    .tip { background: #e8f5e9; padding: 12px; border-radius: 8px; margin: 10px 0; }
  </style>
</head>
<body>

  <div class="series-nav">
    ğŸ“š <strong>LLM ç‰¹æœ‰é—®é¢˜ç³»åˆ— Day 5 / 5 â€” ç³»åˆ—å®Œç»“ç¯‡ï¼ğŸ‰</strong><br>
    Day 1: <a href="2026-02-20-token-limit-handling-strategies.html">Token é™åˆ¶å¤„ç†</a> â†’
    Day 2: <a href="2026-02-21-llm-cost-optimization.html">æˆæœ¬ä¼˜åŒ–</a> â†’
    Day 3: <a href="2026-02-22-hallucination-detection-mitigation.html">Hallucination</a> â†’
    Day 4: <a href="2026-02-23-streaming-output-architecture.html">æµå¼è¾“å‡º</a> â†’
    <strong>Day 5: Multi-turn å¯¹è¯çŠ¶æ€ç®¡ç†</strong> âœ¨
  </div>

  <h1>ğŸ”„ Multi-turn å¯¹è¯çŠ¶æ€ç®¡ç†</h1>
  <p>ğŸ“… 2026-02-24 | Phase 2.3 LLM ç‰¹æœ‰é—®é¢˜ Â· Day 5 å®Œç»“ç¯‡</p>

  <h2>ğŸ“– æ¦‚è¿° / Overview</h2>
  <div class="bilingual">
    <div class="zh">
      <p>Multi-turn å¯¹è¯æ˜¯ LLM åº”ç”¨æœ€æ ¸å¿ƒçš„äº¤äº’æ¨¡å¼ã€‚ä½†è¡¨é¢ç®€å•çš„"å¸¦ä¸Šå†å²æ¶ˆæ¯"èƒŒåï¼Œéšè—ç€ <strong>ä¸Šä¸‹æ–‡è†¨èƒ€ã€çŠ¶æ€ä¸€è‡´æ€§ã€KV Cache å¤ç”¨ã€ä¼šè¯éš”ç¦»</strong>ç­‰ä¸€ç³»åˆ—å·¥ç¨‹æŒ‘æˆ˜ã€‚</p>
      <p>ä»Šå¤©æˆ‘ä»¬ä»<strong>åº”ç”¨å±‚åˆ°åŸºç¡€è®¾æ–½å±‚</strong>ï¼Œå…¨é¢æ‹†è§£ç”Ÿäº§çº§ multi-turn ç³»ç»Ÿçš„è®¾è®¡ï¼Œè¿™ä¹Ÿæ˜¯é¢è¯•ä¸­ç»å¸¸è¢«è¿½é—®çš„æ·±æ°´åŒºã€‚</p>
    </div>
    <div class="en">
      <p>Multi-turn conversation is the core interaction mode for LLM apps. Behind the simple "append chat history" lies a complex set of engineering challenges: <strong>context explosion, state consistency, KV cache reuse, and session isolation</strong>.</p>
      <p>Today we dissect production multi-turn systems from <strong>application layer to infrastructure</strong> â€” a frequent deep-dive area in system design interviews.</p>
    </div>
  </div>

  <h2>ğŸ—ï¸ æ•´ä½“æ¶æ„ / Architecture Overview</h2>
  <div class="architecture">
<pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Client (Web/Mobile/API)                      â”‚
â”‚   session_id + message â†’ POST /chat/completions                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Gateway / Router                        â”‚
â”‚  Rate Limit Â· Auth Â· Session Routing Â· Load Balance              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Conversation Orchestrator                        â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Session   â”‚  â”‚ Context   â”‚  â”‚ History  â”‚  â”‚ State Machine â”‚  â”‚
â”‚  â”‚ Manager   â”‚  â”‚ Window    â”‚  â”‚ Storage  â”‚  â”‚ (Optional)    â”‚  â”‚
â”‚  â”‚           â”‚  â”‚ Builder   â”‚  â”‚          â”‚  â”‚               â”‚  â”‚
â”‚  â”‚ â€¢ Create  â”‚  â”‚ â€¢ Trunc   â”‚  â”‚ â€¢ Redis  â”‚  â”‚ â€¢ Intent FSM  â”‚
â”‚  â”‚ â€¢ Resume  â”‚  â”‚ â€¢ Summar  â”‚  â”‚ â€¢ DB     â”‚  â”‚ â€¢ Task Flow   â”‚
â”‚  â”‚ â€¢ Fork    â”‚  â”‚ â€¢ RAG     â”‚  â”‚ â€¢ S3     â”‚  â”‚ â€¢ Slot Fill   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚        Context Assembly Pipeline      â”‚                        â”‚
â”‚  â”‚  System Prompt + Summary + Recent     â”‚                        â”‚
â”‚  â”‚  + Retrieved Context + User Message   â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM Inference Layer                            â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ KV Cache Mgr â”‚    â”‚ Prefix Cache â”‚    â”‚ Session-Aware   â”‚    â”‚
â”‚  â”‚ (CachedAttn) â”‚    â”‚ (Shared)     â”‚    â”‚ Scheduling      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
  </div>

  <h2>ğŸ§© å…­å¤§æ ¸å¿ƒæŒ‘æˆ˜ä¸æ–¹æ¡ˆ / Six Core Challenges</h2>

  <!-- Challenge 1: Context Window Management -->
  <h3>æŒ‘æˆ˜ 1: ä¸Šä¸‹æ–‡çª—å£ç®¡ç† / Context Window Management</h3>
  <div class="bilingual">
    <div class="zh">
      <p>å¤šè½®å¯¹è¯æœ€ç›´æ¥çš„æŒ‘æˆ˜ï¼š<strong>å†å²æ¶ˆæ¯ä¸æ–­ç´¯ç§¯ï¼Œtoken æ•°è¿Ÿæ—©è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£</strong>ã€‚Day 1 è®²äº† token é™åˆ¶ç­–ç•¥ï¼Œè¿™é‡Œèšç„¦ multi-turn åœºæ™¯çš„ä¸‰ç§ç»å…¸æ–¹æ¡ˆï¼š</p>
    </div>
    <div class="en">
      <p>The most immediate challenge: <strong>message history grows unbounded</strong>. We covered token limits in Day 1; here we focus on three classic patterns for multi-turn scenarios:</p>
    </div>
  </div>

  <table>
    <tr>
      <th>ç­–ç•¥</th>
      <th>åŸç†</th>
      <th>ä¼˜ç‚¹</th>
      <th>ç¼ºç‚¹</th>
      <th>é€‚ç”¨åœºæ™¯</th>
    </tr>
    <tr>
      <td><strong>Sliding Window</strong></td>
      <td>åªä¿ç•™æœ€è¿‘ N è½®å¯¹è¯</td>
      <td>ç®€å•ã€å¯é¢„æµ‹ã€ä½å»¶è¿Ÿ</td>
      <td>ä¸¢å¤±æ—©æœŸä¸Šä¸‹æ–‡</td>
      <td>çŸ­å¯¹è¯ã€å®¢æœ</td>
    </tr>
    <tr>
      <td><strong>Summary + Recent</strong></td>
      <td>å¯¹æ—©æœŸå¯¹è¯åšæ‘˜è¦ï¼Œä¿ç•™æœ€è¿‘å‡ è½®åŸæ–‡</td>
      <td>ä¿ç•™å…¨å±€è¯­ä¹‰ã€å¯æ§ token</td>
      <td>æ‘˜è¦å¯èƒ½ä¸¢ç»†èŠ‚ã€é¢å¤– LLM è°ƒç”¨</td>
      <td>é•¿å¯¹è¯ã€Agent</td>
    </tr>
    <tr>
      <td><strong>History Fusion</strong></td>
      <td>å¯¹å¤šè½® Q&A è¿›è¡Œ query rewriteï¼Œèåˆä¸Šä¸‹æ–‡åˆ°å½“å‰ query ä¸­</td>
      <td>RAG å‹å¥½ã€æœç´¢å‹å¥½</td>
      <td>rewrite è´¨é‡ä¾èµ– LLM</td>
      <td>RAG ç³»ç»Ÿã€æœç´¢å‹å¯¹è¯</td>
    </tr>
  </table>

  <div class="highlight">
    <h4>ğŸ’¡ ç”Ÿäº§çº§ç»„åˆæ–¹æ¡ˆï¼šä¸‰å±‚ä¸Šä¸‹æ–‡ç»„è£…</h4>
<pre>
Token Budget: 128K total

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  System Prompt (å›ºå®š)         ~2K  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conversation Summary        ~1K  â”‚  â† å‰ N-10 è½®çš„æ‘˜è¦
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Retrieved Context (RAG)     ~4K  â”‚  â† å¦‚æœéœ€è¦å¤–éƒ¨çŸ¥è¯†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Recent Messages (åŸæ–‡)      ~8K  â”‚  â† æœ€è¿‘ 8-10 è½®å®Œæ•´æ¶ˆæ¯
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Current User Message        ~2K  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Reserved for Response      ~4K+  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
    <p><strong>å…³é”®è®¾è®¡</strong>ï¼šToken Budget Manager ä»ä¸‹å¾€ä¸Šåˆ†é… â€” å…ˆä¿è¯ response ç©ºé—´ï¼Œå†ä¿è¯ recent messagesï¼Œæœ€ååˆ†é…ç»™ summary å’Œ RAGã€‚</p>
  </div>

<pre><code>// TypeScript: Context Window Builder
interface ConversationContext {
  systemPrompt: string;
  summary: string | null;
  recentMessages: Message[];
  retrievedContext: string[];
  currentMessage: string;
}

class ContextWindowBuilder {
  constructor(
    private maxTokens: number,
    private reserveForResponse: number = 4096,
    private recentWindowSize: number = 10,
  ) {}

  async build(
    sessionId: string,
    currentMessage: string,
  ): Promise<ConversationContext> {
    const budget = this.maxTokens - this.reserveForResponse;
    let remaining = budget;

    // 1. System prompt (fixed cost)
    const systemPrompt = await this.getSystemPrompt(sessionId);
    remaining -= countTokens(systemPrompt);

    // 2. Current message (must include)
    remaining -= countTokens(currentMessage);

    // 3. Recent messages (high priority)
    const allMessages = await this.getHistory(sessionId);
    const recent = allMessages.slice(-this.recentWindowSize);
    const recentTokens = recent.reduce(
      (sum, m) => sum + countTokens(m.content), 0
    );

    let recentMessages: Message[];
    if (recentTokens <= remaining * 0.6) {
      recentMessages = recent;
      remaining -= recentTokens;
    } else {
      // Truncate from oldest in recent window
      recentMessages = this.fitMessages(recent, remaining * 0.6);
      remaining -= recentMessages.reduce(
        (s, m) => s + countTokens(m.content), 0
      );
    }

    // 4. Summary of older messages (if any)
    const olderMessages = allMessages.slice(0, -this.recentWindowSize);
    let summary: string | null = null;
    if (olderMessages.length > 0) {
      summary = await this.getOrCreateSummary(sessionId, olderMessages);
      remaining -= countTokens(summary);
    }

    // 5. RAG context (use remaining budget)
    const retrievedContext = remaining > 500
      ? await this.retrieveContext(currentMessage, remaining)
      : [];

    return {
      systemPrompt, summary, recentMessages,
      retrievedContext, currentMessage
    };
  }

  private async getOrCreateSummary(
    sessionId: string,
    messages: Message[]
  ): Promise&lt;string&gt; {
    // Incremental summarization: append new messages to existing summary
    const cached = await redis.get(`summary:${sessionId}`);
    const lastSummarizedIdx = await redis.get(`summary_idx:${sessionId}`);
    
    const newMessages = messages.slice(Number(lastSummarizedIdx) || 0);
    if (newMessages.length === 0 && cached) return cached;

    const updatedSummary = await llm.complete({
      messages: [{
        role: 'system',
        content: `Update this conversation summary with new messages.
Previous summary: ${cached || 'None'}
New messages: ${formatMessages(newMessages)}
Write a concise summary covering all key points, decisions, and context.`
      }]
    });

    await redis.set(`summary:${sessionId}`, updatedSummary);
    await redis.set(`summary_idx:${sessionId}`, messages.length);
    return updatedSummary;
  }
}</code></pre>

  <!-- Challenge 2: Session State Storage -->
  <h3>æŒ‘æˆ˜ 2: ä¼šè¯çŠ¶æ€å­˜å‚¨ / Session State Storage</h3>
  <div class="bilingual">
    <div class="zh">
      <p>æ¯ä¸ªå¯¹è¯ä¼šè¯éœ€è¦æŒä¹…åŒ–çš„çŠ¶æ€è¿œä¸åªæ˜¯æ¶ˆæ¯åˆ—è¡¨ã€‚ä¸€ä¸ªå®Œæ•´çš„ Session åŒ…å«ï¼š</p>
      <ul>
        <li><strong>æ¶ˆæ¯å†å²</strong> â€” å®Œæ•´çš„ user/assistant/tool messages</li>
        <li><strong>æ‘˜è¦ç¼“å­˜</strong> â€” å¢é‡ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦</li>
        <li><strong>ä¼šè¯å…ƒæ•°æ®</strong> â€” modelã€temperatureã€system prompt ç‰ˆæœ¬</li>
        <li><strong>Slot/Entity çŠ¶æ€</strong> â€” ä»»åŠ¡å‹å¯¹è¯çš„å·²æ”¶é›†ä¿¡æ¯</li>
        <li><strong>Tool è°ƒç”¨è®°å½•</strong> â€” function calling çš„ä¸­é—´çŠ¶æ€</li>
      </ul>
    </div>
    <div class="en">
      <p>Session state goes far beyond just the message list:</p>
      <ul>
        <li><strong>Message history</strong> â€” full user/assistant/tool messages</li>
        <li><strong>Summary cache</strong> â€” incrementally generated summaries</li>
        <li><strong>Session metadata</strong> â€” model, temperature, system prompt version</li>
        <li><strong>Slot/Entity state</strong> â€” collected info for task-oriented dialogues</li>
        <li><strong>Tool call records</strong> â€” intermediate function calling states</li>
      </ul>
    </div>
  </div>

<pre><code>// Session State Schema
interface SessionState {
  // Identity
  sessionId: string;
  userId: string;
  createdAt: Date;
  lastActiveAt: Date;
  
  // Conversation
  messages: Message[];              // Full history
  summary: string | null;           // Compressed older context
  summaryUpToIndex: number;         // Messages already summarized
  
  // Configuration (per-session overrides)
  model: string;                    // "gpt-4o" | "claude-sonnet"
  systemPromptVersion: string;      // For A/B testing prompts
  temperature: number;
  
  // Task State (for structured dialogues)
  slots: Record&lt;string, any&gt;;       // e.g., { destination: "Tokyo", dates: null }
  currentIntent: string | null;     // e.g., "book_flight"
  pendingToolCalls: ToolCall[];     // Awaiting user confirmation
  
  // Metadata
  turnCount: number;
  totalTokensUsed: number;
  parentSessionId: string | null;   // For forked conversations
}

// Storage Architecture: Hot/Warm/Cold
// Hot  (Redis)    â€” Active sessions (last 30 min), sub-ms reads
// Warm (Postgres) â€” Recent sessions (last 7 days), indexed queries
// Cold (S3/GCS)   â€” Archived sessions, compliance & analytics</code></pre>

  <div class="highlight">
    <h4>ğŸ’¡ è®¾è®¡å†³ç­–ï¼šRedis vs Database çš„åˆ†å±‚ç­–ç•¥</h4>
    <table>
      <tr><th>å±‚</th><th>å­˜å‚¨</th><th>æ•°æ®</th><th>å»¶è¿Ÿ</th><th>TTL</th></tr>
      <tr><td>Hot</td><td>Redis Cluster</td><td>æ´»è·ƒ session çš„ messages + slots</td><td>&lt;1ms</td><td>30min idle</td></tr>
      <tr><td>Warm</td><td>PostgreSQL (JSONB)</td><td>7å¤©å†… sessionsï¼Œæ”¯æŒ query</td><td>~5ms</td><td>7 days</td></tr>
      <tr><td>Cold</td><td>S3 + Parquet</td><td>å½’æ¡£ sessionsï¼Œåˆ†æç”¨</td><td>~100ms</td><td>æŒ‰åˆè§„è¦æ±‚</td></tr>
    </table>
    <p><strong>å…³é”®ä¼˜åŒ–</strong>ï¼šSession resume æ—¶ï¼Œå…ˆæŸ¥ Redisï¼›miss åˆ™ä» Postgres åŠ è½½å¹¶å›å¡« Redisã€‚é¿å…æ¯æ¬¡éƒ½æŸ¥æ•°æ®åº“ã€‚</p>
  </div>

  <!-- Challenge 3: Incremental Summarization -->
  <h3>æŒ‘æˆ˜ 3: å¢é‡æ‘˜è¦ / Incremental Summarization</h3>
  <div class="bilingual">
    <div class="zh">
      <p>éšç€å¯¹è¯å˜é•¿ï¼Œé‡æ–°æ‘˜è¦æ•´ä¸ªå¯¹è¯å†å²çš„æˆæœ¬çº¿æ€§å¢é•¿ã€‚ç”Ÿäº§ç³»ç»Ÿéœ€è¦<strong>å¢é‡æ‘˜è¦</strong>ï¼šæ¯æ¬¡åªå¤„ç†æ–°å¢çš„æ¶ˆæ¯ï¼Œåœ¨æ—§æ‘˜è¦åŸºç¡€ä¸Šæ›´æ–°ã€‚</p>
    </div>
    <div class="en">
      <p>Re-summarizing the entire history each turn is expensive. Production systems use <strong>incremental summarization</strong>: process only new messages, updating the existing summary.</p>
    </div>
  </div>

<pre><code>// Three Summarization Strategies

// 1. Rolling Summary â€” æ¯æ¬¡ append æ–°æ¶ˆæ¯åæ›´æ–°
// Cost: 1 LLM call per trigger (not every turn)
// Trigger: when recent window > threshold
async function rollingSummary(
  existingSummary: string,
  newMessages: Message[]
): Promise&lt;string&gt; {
  return llm.complete({
    model: 'gpt-4o-mini',  // ç”¨å°æ¨¡å‹åšæ‘˜è¦ï¼Œçœæˆæœ¬
    messages: [{
      role: 'user',
      content: `Here is the existing conversation summary:
${existingSummary}

Here are new messages to incorporate:
${formatMessages(newMessages)}

Update the summary to include all key information, decisions, 
user preferences, and unresolved questions. Keep it concise.`
    }]
  });
}

// 2. Hierarchical Summary â€” åˆ†å±‚å‹ç¼©
// Level 0: Raw messages (recent 10 turns)
// Level 1: Summary of turns 11-30  (~200 tokens)  
// Level 2: Summary of turns 31-100 (~100 tokens)
// Level 3: Summary of turns 100+   (~50 tokens)

// 3. Entity-Focused Summary â€” æå–å…³é”®å®ä½“å’Œå…³ç³»
// é€‚åˆä»»åŠ¡å‹å¯¹è¯ï¼ˆè®¢ç¥¨ã€å¡«è¡¨ï¼‰
// è¾“å‡º: { entities: { name: "å¼ ä¸‰", flight: "CA123", ... }, 
//         decisions: [...], openQuestions: [...] }</code></pre>

  <div class="tip">
    <strong>ğŸ¯ é¢è¯•è¦ç‚¹</strong>ï¼šå¢é‡æ‘˜è¦è§¦å‘æ—¶æœºå¾ˆå…³é”®ã€‚ä¸æ˜¯æ¯è½®éƒ½æ‘˜è¦ï¼ˆå¤ªè´µï¼‰ï¼Œè€Œæ˜¯å½“ recent window æ»¡äº†æ‰è§¦å‘ã€‚å…¸å‹ç­–ç•¥ï¼šrecent window 10 è½®ï¼Œæ¯æ¬¡æ»¡äº†æŠŠæœ€è€çš„ 5 è½®æ‘˜è¦è¿› summaryï¼Œä¿ç•™æœ€æ–° 5 è½®ã€‚
  </div>

  <!-- Challenge 4: KV Cache Reuse -->
  <h3>æŒ‘æˆ˜ 4: KV Cache å¤ç”¨ / KV Cache Reuse for Multi-turn</h3>
  <div class="bilingual">
    <div class="zh">
      <p>è¿™æ˜¯ multi-turn æœ€é‡è¦çš„<strong>åŸºç¡€è®¾æ–½å±‚ä¼˜åŒ–</strong>ã€‚æ¯è½®å¯¹è¯éƒ½è¦å‘é€å®Œæ•´çš„å†å²æ¶ˆæ¯ç»™ LLMï¼Œå¦‚æœæ¯æ¬¡éƒ½ä»å¤´è®¡ç®— attentionï¼Œå†å²éƒ¨åˆ†çš„ prefill å®Œå…¨æ˜¯é‡å¤è®¡ç®—ã€‚</p>
      <p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šä¸Šä¸€è½®å·²ç»è®¡ç®—è¿‡çš„ KV cacheï¼Œä¸‹ä¸€è½®åº”è¯¥å¤ç”¨ï¼Œåªéœ€è¦ prefill æ–°å¢çš„ tokensã€‚</p>
    </div>
    <div class="en">
      <p>This is the most important <strong>infrastructure-level optimization</strong>. Each turn sends the full history â€” recomputing attention for already-processed tokens is pure waste.</p>
      <p><strong>Key insight</strong>: Reuse the KV cache from the previous turn; only prefill newly added tokens.</p>
    </div>
  </div>

  <div class="architecture">
<pre>
Turn 1: [System] [User: "What is RAG?"]
        â†â”€â”€ Compute KV for all tokens â”€â”€â†’
        Save KV cache (session=abc, turn=1)

Turn 2: [System] [User: "What is RAG?"] [Asst: "RAG is..."] [User: "How to chunk?"]
        â†â”€â”€ Reuse KV â”€â”€â†’ â†â”€â”€ Only compute new â”€â”€â†’
        Load KV(abc,1)    Prefill delta tokens

Turn 3: [System] [...history...] [User: "Compare strategies?"]
        â†â”€â”€ Reuse KV â”€â”€â†’ â†â”€â”€ New â”€â”€â†’
        Load KV(abc,2)    Delta only

Savings: Turn N only prefills ~last response + new message
         Instead of re-prefilling entire conversation!
</pre>
  </div>

  <table>
    <tr><th>æ–¹æ¡ˆ</th><th>ä»£è¡¨ç³»ç»Ÿ</th><th>åŸç†</th><th>å¤æ‚åº¦</th></tr>
    <tr>
      <td><strong>Prefix Caching</strong></td>
      <td>vLLM, SGLang</td>
      <td>å¯¹ prompt prefix åš hashï¼Œç›¸åŒ prefix å¤ç”¨ KV</td>
      <td>ä¸­ç­‰ï¼ˆRadixTree ç®¡ç†ï¼‰</td>
    </tr>
    <tr>
      <td><strong>CachedAttention</strong></td>
      <td>Moonshot (FAST'25)</td>
      <td>å°† idle session çš„ KV cache offload åˆ° CPU/SSDï¼Œresume æ—¶åŠ è½½</td>
      <td>é«˜ï¼ˆå¤šå±‚å­˜å‚¨ + é¢„å–ï¼‰</td>
    </tr>
    <tr>
      <td><strong>HCache</strong></td>
      <td>EuroSys'25</td>
      <td>åºåˆ—åŒ– KV + hidden states åˆ° SSDï¼Œå¿«é€Ÿæ¢å¤å®Œæ•´æ¨ç†çŠ¶æ€</td>
      <td>é«˜ï¼ˆstate serializationï¼‰</td>
    </tr>
    <tr>
      <td><strong>Session-Aware Scheduling</strong></td>
      <td>è‡ªå»ºæ–¹æ¡ˆ</td>
      <td>è°ƒåº¦å™¨å°†åŒä¸€ session çš„è¯·æ±‚è·¯ç”±åˆ°åŒä¸€ GPUï¼Œæœ€å¤§åŒ– cache hit</td>
      <td>ä¸­ç­‰ï¼ˆsticky routingï¼‰</td>
    </tr>
  </table>

  <div class="highlight">
    <h4>ğŸ’¡ Prefix Caching çš„å·¥ä½œåŸç†ï¼ˆvLLM/SGLangï¼‰</h4>
<pre>
RadixTree for KV Cache:

Root
â”œâ”€â”€ hash("[System] You are a helpful...") â†’ KV block A
â”‚   â”œâ”€â”€ hash("[User] What is RAG?") â†’ KV block B
â”‚   â”‚   â””â”€â”€ hash("[Asst] RAG is...") â†’ KV block C  â† Turn 1 complete
â”‚   â”‚       â””â”€â”€ hash("[User] How to chunk?") â†’ Only this needs prefill!
â”‚   â””â”€â”€ hash("[User] Different question") â†’ KV block D (different branch)
â””â”€â”€ hash("[System] You are a coding...") â†’ KV block E (different system prompt)
</pre>
    <p><strong>ä¸ºä»€ä¹ˆå¥½</strong>ï¼šåŒä¸€ system prompt çš„æ‰€æœ‰ session å…±äº«å‰ç¼€ KV cacheã€‚Multi-turn ä¸­ä¸Šä¸€è½®çš„æ‰€æœ‰ tokens éƒ½è‡ªåŠ¨æˆä¸ºä¸‹ä¸€è½®çš„ prefixã€‚</p>
  </div>

  <!-- Challenge 5: Conversation Branching & Forking -->
  <h3>æŒ‘æˆ˜ 5: å¯¹è¯åˆ†æ”¯ä¸å›æº¯ / Conversation Branching</h3>
  <div class="bilingual">
    <div class="zh">
      <p>ç”¨æˆ·ç»å¸¸æƒ³"å›åˆ°ä¹‹å‰æŸä¸€æ­¥é‡æ–°èŠ"æˆ–"ä»è¿™é‡Œåˆ†å‡ºä¸€ä¸ªæ–°æ–¹å‘"ã€‚ChatGPT çš„ edit message åŠŸèƒ½å°±æ˜¯å…¸å‹çš„å¯¹è¯åˆ†æ”¯ã€‚è¿™è¦æ±‚ç³»ç»Ÿæ”¯æŒï¼š</p>
      <ul>
        <li><strong>Edit & Regenerate</strong> â€” ä¿®æ”¹å†å²æ¶ˆæ¯ï¼Œä»è¯¥ç‚¹é‡æ–°ç”Ÿæˆ</li>
        <li><strong>Fork</strong> â€” ä»æŸä¸€è½®åˆ†å‰å‡ºæ–°çš„å¯¹è¯åˆ†æ”¯</li>
        <li><strong>Undo/Redo</strong> â€” æ’¤é”€æœ€è¿‘çš„å¯¹è¯è½®æ¬¡</li>
      </ul>
    </div>
    <div class="en">
      <p>Users often want to "go back and try a different direction." ChatGPT's edit message feature is a classic example. The system must support:</p>
      <ul>
        <li><strong>Edit & Regenerate</strong> â€” Modify a past message and regenerate from that point</li>
        <li><strong>Fork</strong> â€” Branch a new conversation from any turn</li>
        <li><strong>Undo/Redo</strong> â€” Roll back recent turns</li>
      </ul>
    </div>
  </div>

<pre><code>// Tree-structured conversation (like ChatGPT internally)
interface MessageNode {
  id: string;
  parentId: string | null;      // Points to previous message
  children: string[];           // Multiple children = branches
  role: 'user' | 'assistant' | 'system' | 'tool';
  content: string;
  createdAt: Date;
  metadata: {
    model: string;
    tokenCount: number;
    isActive: boolean;          // Current path through the tree
  };
}

// The "conversation" is a path from root to leaf
// Edit = create new child of the edited message's parent
// Fork = create new child of any existing message
// Regenerate = create sibling of an assistant message

class ConversationTree {
  private nodes: Map&lt;string, MessageNode&gt;;
  private activeLeafId: string;

  // Get the current conversation path (for sending to LLM)
  getActivePath(): Message[] {
    const path: MessageNode[] = [];
    let current = this.nodes.get(this.activeLeafId);
    while (current) {
      path.unshift(current);
      current = current.parentId 
        ? this.nodes.get(current.parentId) 
        : null;
    }
    return path.map(n => ({ role: n.role, content: n.content }));
  }

  // Edit a message: create new branch from parent
  editMessage(messageId: string, newContent: string): string {
    const original = this.nodes.get(messageId)!;
    const newNode: MessageNode = {
      id: generateId(),
      parentId: original.parentId,
      children: [],
      role: original.role,
      content: newContent,
      createdAt: new Date(),
      metadata: { ...original.metadata, isActive: true },
    };
    // Add as sibling (new child of same parent)
    const parent = this.nodes.get(original.parentId!)!;
    parent.children.push(newNode.id);
    // Mark old branch as inactive, new branch as active
    this.deactivateBranch(messageId);
    this.nodes.set(newNode.id, newNode);
    this.activeLeafId = newNode.id;
    return newNode.id;  // Caller should regenerate from here
  }
}</code></pre>

  <div class="tip">
    <strong>ğŸ¯ é¢è¯•è¦ç‚¹</strong>ï¼šChatGPT å†…éƒ¨ç”¨ <strong>Tree + Active Path</strong> æ¨¡å‹ï¼Œä¸æ˜¯ç®€å•çš„æ•°ç»„ã€‚è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆä½ å¯ä»¥ edit ä»»æ„å†å²æ¶ˆæ¯ã€regenerateã€åœ¨åˆ†æ”¯é—´åˆ‡æ¢ã€‚æ•°æ®åº“å­˜ tree ç»“æ„ï¼ˆadjacency listï¼‰ï¼Œå‰ç«¯åªæ¸²æŸ“ active pathã€‚
  </div>

  <!-- Challenge 6: Session Isolation & Multi-tenancy -->
  <h3>æŒ‘æˆ˜ 6: ä¼šè¯éš”ç¦»ä¸å¤šç§Ÿæˆ· / Session Isolation</h3>
  <div class="bilingual">
    <div class="zh">
      <p>ç”Ÿäº§ç³»ç»Ÿå¿…é¡»ç¡®ä¿ï¼šä¸åŒç”¨æˆ·çš„ä¼šè¯çŠ¶æ€<strong>ç»å¯¹éš”ç¦»</strong>ï¼ŒåŒä¸€ç”¨æˆ·çš„ä¸åŒä¼šè¯<strong>äº’ä¸å¹²æ‰°</strong>ã€‚å¸¸è§é™·é˜±ï¼š</p>
      <ul>
        <li>âŒ KV Cache å¤ç”¨æ—¶ï¼Œprefix hash ç¢°æ’å¯¼è‡´ä¸² session</li>
        <li>âŒ Redis key è®¾è®¡ä¸å½“ï¼Œç”¨æˆ· A è¯»åˆ°ç”¨æˆ· B çš„æ¶ˆæ¯</li>
        <li>âŒ Summary ç¼“å­˜æ²¡æœ‰ç»‘å®š session_idï¼Œå¤šä¸ªä¼šè¯å…±ç”¨åŒä¸€æ‘˜è¦</li>
        <li>âŒ System prompt ç‰ˆæœ¬å‡çº§åï¼Œæ—§ session çš„ KV cache æ— æ•ˆä½†ä»è¢«å¤ç”¨</li>
      </ul>
    </div>
    <div class="en">
      <p>Production systems must guarantee <strong>absolute isolation</strong>. Common pitfalls:</p>
      <ul>
        <li>âŒ Prefix hash collision in KV cache â†’ cross-session data leak</li>
        <li>âŒ Poor Redis key design â†’ User A reads User B's messages</li>
        <li>âŒ Summary cache not scoped to session_id</li>
        <li>âŒ System prompt version change invalidates cached KV but stale cache is reused</li>
      </ul>
    </div>
  </div>

<pre><code>// Key design for session isolation
const redisKeyPattern = {
  messages:  `chat:${tenantId}:${userId}:${sessionId}:messages`,
  summary:   `chat:${tenantId}:${userId}:${sessionId}:summary`,
  slots:     `chat:${tenantId}:${userId}:${sessionId}:slots`,
  kvCacheId: `kv:${tenantId}:${userId}:${sessionId}:${turnIndex}`,
};

// KV Cache prefix hash MUST include:
// 1. tenantId (multi-tenant isolation)
// 2. systemPromptVersion (invalidate on prompt change)
// 3. modelVersion (invalidate on model change)
function kvCachePrefixKey(session: SessionState): string {
  return hash([
    session.tenantId,
    session.systemPromptVersion,
    session.model,
    ...session.messages.map(m => m.content)
  ].join('|'));
}</code></pre>

  <h2>ğŸ“Š å®Œæ•´ç”Ÿå‘½å‘¨æœŸ / Full Lifecycle of a Multi-turn Request</h2>
  <div class="architecture">
<pre>
User sends message in existing session (session_id = "abc123")
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. Load Session State          â”‚
    â”‚     Redis.get("chat:...:abc123")â”‚
    â”‚     Cache miss? â†’ Load from DB  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. Append User Message         â”‚
    â”‚     messages.push(userMsg)      â”‚
    â”‚     turnCount++                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. Build Context Window        â”‚
    â”‚     Check if summarization neededâ”‚
    â”‚     (recent window full?)       â”‚
    â”‚     Assemble: system + summary  â”‚
    â”‚     + recent + RAG + current    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  4. Route to LLM Instance       â”‚
    â”‚     Session-aware routing       â”‚
    â”‚     (sticky to same GPU for     â”‚
    â”‚      KV cache reuse)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  5. Inference (with KV reuse)   â”‚
    â”‚     Prefix cache hit? â†’ Skip    â”‚
    â”‚     prefill for cached portion  â”‚
    â”‚     Stream tokens back          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  6. Post-process & Save         â”‚
    â”‚     Append assistant message    â”‚
    â”‚     Update token count          â”‚
    â”‚     Persist to Redis + async DB â”‚
    â”‚     Update KV cache reference   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
  </div>

  <h2>âš¡ å®¹é‡ä¼°ç®— / Capacity Estimation</h2>
  <div class="architecture">
<pre>
å‡è®¾ï¼š100K DAU çš„å¯¹è¯åº”ç”¨ï¼Œå¹³å‡æ¯ç”¨æˆ· 3 ä¸ª sessionï¼Œæ¯ session 20 è½®

Session æ•°æ®é‡:
  - æ´»è·ƒ sessions (30min window): ~15K sessions
  - æ¯ session çŠ¶æ€: ~50KB (messages + summary + metadata)
  - Hot storage (Redis): 15K Ã— 50KB = ~750MB â† ä¸€å° Redis è½»æ¾æå®š

æ¶ˆæ¯å­˜å‚¨:
  - æ¯å¤©æ–°æ¶ˆæ¯: 100K Ã— 3 Ã— 20 Ã— 2 (user+asst) = 12M messages/day
  - æ¯æ¡æ¶ˆæ¯ ~500 bytes avg
  - æ—¥å­˜å‚¨é‡: 12M Ã— 500B = ~6GB/day
  - Postgres æœˆå­˜å‚¨: ~180GB â† éœ€è¦åˆ†åŒºè¡¨ (by date)

KV Cache (GPU ä¾§):
  - æ´»è·ƒè¯·æ±‚åŒæ—¶åœ¨çº¿: ~500 concurrent
  - æ¯è¯·æ±‚ KV cache (8K context, 70B model): ~2GB
  - å³°å€¼ KV cache: 500 Ã— 2GB = ~1TB GPU memory
  - éœ€è¦ KV cache offloading åˆ° CPU/SSD

LLM è°ƒç”¨:
  - æ¯è½®å¹³å‡ input: 4K tokens (context) + output: 500 tokens
  - æ—¥ token é‡: 12M/2 turns Ã— 4.5K = ~27B tokens/day
  - æˆæœ¬ (GPT-4o): input $2.5/M + output $10/M â‰ˆ ~$94K/day ğŸ˜±
  - â†’ å¿…é¡»ç”¨ Day 2 çš„æˆæœ¬ä¼˜åŒ–ç­–ç•¥ (cache + routing + cascade)
</pre>
  </div>

  <h2>ğŸ¯ é¢è¯•é«˜é¢‘é—®é¢˜ / Interview Questions</h2>

  <div class="interview">
    <h4>Q1: è®¾è®¡ä¸€ä¸ªæ”¯æŒ multi-turn çš„ ChatBotï¼Œå¦‚ä½•ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡ï¼Ÿ</h4>
    <p><strong>è€ƒå¯Ÿç‚¹</strong>ï¼šContext window management + storage design</p>
    <p><strong>ç­”é¢˜æ¡†æ¶</strong>ï¼š</p>
    <ol>
      <li><strong>å­˜å‚¨åˆ†å±‚</strong>ï¼šRedis (hot) â†’ DB (warm) â†’ Object Storage (cold)</li>
      <li><strong>ä¸Šä¸‹æ–‡ç»„è£…</strong>ï¼šSystem Prompt + Summary + Recent Messages + Current</li>
      <li><strong>æ‘˜è¦ç­–ç•¥</strong>ï¼šIncremental rolling summaryï¼Œç”¨å°æ¨¡å‹ (GPT-4o-mini) é™ä½æˆæœ¬</li>
      <li><strong>Token budget</strong>ï¼šä» response å‘ä¸Šåˆ†é…ï¼Œä¿è¯ recent window å®Œæ•´æ€§</li>
    </ol>
  </div>

  <div class="interview">
    <h4>Q2: ç”¨æˆ·åœ¨ç¬¬ 50 è½®å¼•ç”¨äº†ç¬¬ 3 è½®æåˆ°çš„ä¸€ä¸ªæ¦‚å¿µï¼Œä½ çš„ç³»ç»Ÿæ€ä¹ˆå¤„ç†ï¼Ÿ</h4>
    <p><strong>è€ƒå¯Ÿç‚¹</strong>ï¼šLong-range dependency in conversation</p>
    <p><strong>ç­”é¢˜æ¡†æ¶</strong>ï¼š</p>
    <ol>
      <li><strong>Summary ä¿ç•™</strong>ï¼šå…³é”®å®ä½“/æ¦‚å¿µåº”è¯¥è¢«æ‘˜è¦ä¿ç•™ï¼ˆentity-focused summaryï¼‰</li>
      <li><strong>Hybrid approach</strong>ï¼šå¯¹ç”¨æˆ·çš„ query åš embeddingï¼Œæ£€ç´¢å†å²æ¶ˆæ¯ä¸­è¯­ä¹‰ç›¸å…³çš„ turnsï¼ˆæŠŠå†å²å½“ RAG è¯­æ–™ï¼‰</li>
      <li><strong>Explicit tracking</strong>ï¼šç»´æŠ¤ entity/slot storeï¼Œå…³é”®æ¦‚å¿µä»¥ç»“æ„åŒ–å½¢å¼ä¿å­˜</li>
      <li><strong>Fallback</strong>ï¼šå¦‚æœæ— æ³•å¬å›ï¼Œæ‰¿è®¤å¹¶è¯·ç”¨æˆ·é‡è¿°ï¼Œæ¯”å¹»è§‰å¼º</li>
    </ol>
  </div>

  <div class="interview">
    <h4>Q3: Multi-turn åœºæ™¯ä¸‹å¦‚ä½•ä¼˜åŒ–æ¨ç†å»¶è¿Ÿï¼Ÿ</h4>
    <p><strong>è€ƒå¯Ÿç‚¹</strong>ï¼šKV cache reuse + infra optimization</p>
    <p><strong>ç­”é¢˜æ¡†æ¶</strong>ï¼š</p>
    <ol>
      <li><strong>Prefix Caching</strong>ï¼švLLM RadixAttentionï¼Œç›¸åŒå‰ç¼€å¤ç”¨ KV cache</li>
      <li><strong>Session-Aware Routing</strong>ï¼šè°ƒåº¦å™¨å°†åŒä¸€ session è·¯ç”±åˆ°åŒä¸€ GPUï¼Œæœ€å¤§åŒ– cache hit</li>
      <li><strong>KV Cache Offloading</strong>ï¼šidle session çš„ KV ä» GPU â†’ CPU â†’ SSD åˆ†çº§å­˜å‚¨</li>
      <li><strong>Incremental Prefill</strong>ï¼šæ¯è½®åª prefill æ–°å¢ tokens (ä¸Šä¸€è½® response + æ–° user message)</li>
      <li><strong>é‡åŒ–</strong>ï¼šKV cache ç”¨ FP8 æˆ– INT8 é‡åŒ–ï¼Œå‡å°‘å­˜å‚¨å’Œä¼ è¾“</li>
    </ol>
  </div>

  <div class="interview">
    <h4>Q4: ChatGPT çš„ "edit message" åŠŸèƒ½æ€ä¹ˆè®¾è®¡ï¼Ÿ</h4>
    <p><strong>è€ƒå¯Ÿç‚¹</strong>ï¼šConversation tree structure</p>
    <p><strong>ç­”é¢˜æ¡†æ¶</strong>ï¼š</p>
    <ol>
      <li><strong>Tree æ¨¡å‹</strong>ï¼šå¯¹è¯ä¸æ˜¯ array è€Œæ˜¯ treeï¼Œæ¯ä¸ª message æœ‰ parentId å’Œ children[]</li>
      <li><strong>Active Path</strong>ï¼šå½“å‰å¯¹è¯æ˜¯ root â†’ leaf çš„ä¸€æ¡è·¯å¾„</li>
      <li><strong>Edit</strong>ï¼šåœ¨è¢«ç¼–è¾‘æ¶ˆæ¯çš„ parent ä¸‹åˆ›å»ºæ–° childï¼ˆsibling branchï¼‰</li>
      <li><strong>åˆ‡æ¢åˆ†æ”¯</strong>ï¼šæ›´æ–° active pathï¼ŒUI æ˜¾ç¤º "1/3" å·¦å³ç®­å¤´åˆ‡æ¢</li>
      <li><strong>KV Cache</strong>ï¼šåˆ†æ”¯ç‚¹ä¹‹å‰çš„ KV cache å¯ä»¥å¤ç”¨ï¼Œä¹‹åéœ€è¦é‡æ–°è®¡ç®—</li>
    </ol>
  </div>

  <h2>ğŸ”— ç³»åˆ—æ€»ç»“ / Series Recap</h2>

  <div class="highlight">
    <h4>ğŸ‰ LLM ç‰¹æœ‰é—®é¢˜ç³»åˆ—å®Œç»“ï¼Phase 2.3 å®Œæˆï¼</h4>
    <table>
      <tr><th>Day</th><th>ä¸»é¢˜</th><th>æ ¸å¿ƒæ”¶è·</th></tr>
      <tr><td>1</td><td>Token é™åˆ¶å¤„ç†</td><td>6 å¤§ç­–ç•¥ï¼šä» Truncation åˆ° Context Extension</td></tr>
      <tr><td>2</td><td>æˆæœ¬ä¼˜åŒ–</td><td>6 å±‚æ¡†æ¶ï¼šCache â†’ Route â†’ Cascadeï¼ŒèŠ‚çœ 91%</td></tr>
      <tr><td>3</td><td>Hallucination</td><td>4 ç±»åˆ†ç±» + 6 å¤§æ£€æµ‹æ–¹æ³• + 6 å±‚é˜²å¾¡</td></tr>
      <tr><td>4</td><td>æµå¼è¾“å‡º</td><td>SSE åè®® + 6 å¤§å·¥ç¨‹æŒ‘æˆ˜ + API Gateway ç®¡é“</td></tr>
      <tr><td>5</td><td>Multi-turn çŠ¶æ€ç®¡ç†</td><td>ä¸Šä¸‹æ–‡ç»„è£… + KV Cache å¤ç”¨ + Tree ç»“æ„ + å­˜å‚¨åˆ†å±‚</td></tr>
    </table>
    <p><strong>è·¨ Day è”ç³»</strong>ï¼šè¿™ 5 ä¸ªé—®é¢˜åœ¨ç”Ÿäº§ä¸­é«˜åº¦è€¦åˆ â€” multi-turn çš„ context è†¨èƒ€éœ€è¦ Day 1 çš„ token ç®¡ç†ï¼›é•¿å¯¹è¯çš„é«˜æˆæœ¬éœ€è¦ Day 2 çš„ä¼˜åŒ–ï¼›æ¯è½®è¾“å‡ºéœ€è¦ Day 3 çš„ hallucination æ£€æŸ¥ï¼›å®æ—¶ä½“éªŒéœ€è¦ Day 4 çš„æµå¼æ¶æ„ã€‚</p>
  </div>

  <div class="follow-up">
    <h3>ğŸ“ˆ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®® / What's Next</h3>
    <p>Phase 2.3 å®Œæˆï¼æˆ‘ä»¬å·²ç»ç³»ç»Ÿè¦†ç›–äº†ï¼š</p>
    <ul>
      <li>âœ… Phase 1: Agent + RAG å¼€å‘</li>
      <li>âœ… Phase 2.1: AI System Design é¢è¯•é¢˜ (5 ä¸ªç»å…¸ç³»ç»Ÿ)</li>
      <li>âœ… Phase 2.2: ML ç³»ç»Ÿè®¾è®¡è¦ç‚¹ (5 ä¸ªæ ¸å¿ƒä¸»é¢˜)</li>
      <li>âœ… Phase 2.3: LLM ç‰¹æœ‰é—®é¢˜ (5 ä¸ªæ·±åº¦ä¸»é¢˜)</li>
    </ul>
    <p>å¯ä»¥ç»§ç»­æ¢ç´¢ï¼š<strong>Prompt Engineering æŠ€å·§</strong> (Phase 1.1 è¿˜æœªè¦†ç›–) æˆ– <strong>LLM åº”ç”¨æ¡†æ¶å¯¹æ¯”</strong> (Phase 3)ï¼Œæˆ–è€…åˆ‡æ¢åˆ°æ–°çš„çƒ­é—¨é¡¹ç›®åˆ†æã€‚</p>
  </div>

</body>
</html>
