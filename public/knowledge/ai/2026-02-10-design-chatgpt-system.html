<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-10 - è®¾è®¡ ChatGPTï¼šç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿ System Design</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; background: #fff; color: #1a1a1a; line-height: 1.7; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
    @media (max-width: 700px) { .bilingual { grid-template-columns: 1fr; } }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; border-bottom: 2px solid #f9ab00; padding-bottom: 10px; }
    h2 { color: #1a73e8; margin-top: 40px; }
    h3 { color: #3c4043; }
    .architecture { background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 13px; }
    code { font-family: 'SF Mono', 'Fira Code', monospace; }
    .highlight { background: #fff3cd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-blue { background: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-red { background: #fce4ec; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .highlight-green { background: #e8f5e9; padding: 15px; border-radius: 8px; margin: 15px 0; }
    .diagram { background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'SF Mono', monospace; font-size: 12px; white-space: pre; overflow-x: auto; line-height: 1.4; }
    .interview-q { background: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #9c27b0; }
    .interview-q h4 { color: #9c27b0; margin-top: 0; }
    table { border-collapse: collapse; width: 100%; margin: 15px 0; }
    th, td { border: 1px solid #dadce0; padding: 10px; text-align: left; }
    th { background: #f8f9fa; }
    .tag { display: inline-block; background: #e8eaed; padding: 2px 8px; border-radius: 12px; font-size: 12px; margin: 2px; }
    .series-badge { background: #1a73e8; color: white; padding: 4px 12px; border-radius: 16px; font-size: 13px; display: inline-block; margin-bottom: 10px; }
  </style>
</head>
<body>
  <span class="series-badge">ğŸ¯ Phase 2: AI System Design é¢è¯•é¢˜ â€” Day 1</span>
  <h1>ğŸ—ï¸ è®¾è®¡ ChatGPT / Design ChatGPT</h1>
  <p>ğŸ“… 2026-02-10 | ğŸ¯ FAANG+ é¢è¯•ç»å…¸é¢˜ | â±ï¸ 45 min interview format</p>

  <h2>ğŸ“– é¢è¯•åœºæ™¯ / Interview Scenario</h2>
  <div class="bilingual">
    <div class="zh">
      <p><strong>"è¯·è®¾è®¡ä¸€ä¸ªç±»ä¼¼ ChatGPT çš„å¯¹è¯ç³»ç»Ÿ"</strong></p>
      <p>è¿™æ˜¯ 2025-2026 å¹´ AI æ–¹å‘é¢è¯•ä¸­æœ€é«˜é¢‘çš„ System Design é¢˜ã€‚å®ƒæ¶µç›–äº† LLM servingã€å®æ—¶æµå¼è¾“å‡ºã€å¯¹è¯çŠ¶æ€ç®¡ç†ã€æˆæœ¬æ§åˆ¶ã€å®‰å…¨é˜²æŠ¤ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚</p>
      <p>å’Œä¼ ç»Ÿ System Design ä¸åŒï¼Œè¿™é“é¢˜çš„æ ¸å¿ƒéš¾ç‚¹ä¸æ˜¯ QPS å’Œæ•°æ®åº“é€‰å‹ï¼Œè€Œæ˜¯ <strong>token ç»æµå­¦</strong>ã€<strong>æ¨ç†å»¶è¿Ÿ</strong> å’Œ <strong>éç¡®å®šæ€§è¾“å‡º</strong>ã€‚</p>
    </div>
    <div class="en">
      <p><strong>"Design a conversational AI system like ChatGPT"</strong></p>
      <p>This is the #1 most-asked AI System Design question in FAANG+ interviews (2025-2026). It covers LLM serving, streaming output, conversation state management, cost control, and safety.</p>
      <p>Unlike traditional SD, the core challenges are <strong>token economics</strong>, <strong>inference latency</strong>, and <strong>non-deterministic outputs</strong> â€” not QPS or DB selection.</p>
    </div>
  </div>

  <h2>ğŸ¯ Step 1: éœ€æ±‚æ¾„æ¸… / Requirements Clarification</h2>
  <div class="highlight">
    <h4>âš¡ é¢è¯•æŠ€å·§ï¼šå…ˆé—®è¿™äº›é—®é¢˜å±•ç¤º LLM æ„è¯†</h4>
    <ul>
      <li>"æ”¯æŒå¤šå°‘å¹¶å‘ç”¨æˆ·ï¼Ÿ" â†’ ä¼°ç®— GPU èµ„æºéœ€æ±‚</li>
      <li>"ç›®æ ‡å»¶è¿Ÿï¼Ÿé¦– token å»¶è¿Ÿ vs æ•´ä½“å»¶è¿Ÿï¼Ÿ" â†’ TTFT vs TPS</li>
      <li>"æ”¯æŒå¤šæ¨¡æ€å—ï¼ˆå›¾ç‰‡/æ–‡ä»¶ï¼‰ï¼Ÿ" â†’ ç¡®å®šèŒƒå›´</li>
      <li>"éœ€è¦ RAG/å·¥å…·è°ƒç”¨å—ï¼Ÿ" â†’ æ¶æ„å¤æ‚åº¦</li>
      <li>"æˆæœ¬é¢„ç®—ï¼Ÿ" â†’ æ¨¡å‹é€‰æ‹©å’Œç¼“å­˜ç­–ç•¥</li>
    </ul>
  </div>

  <h3>åŠŸèƒ½æ€§éœ€æ±‚ / Functional Requirements</h3>
  <table>
    <tr><th>éœ€æ±‚</th><th>è¯¦æƒ…</th></tr>
    <tr><td>å¤šè½®å¯¹è¯</td><td>ç»´æŠ¤å¯¹è¯å†å²ï¼Œä¸Šä¸‹æ–‡è¿è´¯</td></tr>
    <tr><td>æµå¼è¾“å‡º</td><td>Token-by-token å®æ—¶è¿”å›ï¼ˆSSEï¼‰</td></tr>
    <tr><td>å¯¹è¯ç®¡ç†</td><td>åˆ›å»º/åˆ é™¤/é‡å‘½åå¯¹è¯ï¼Œå†å²æŒä¹…åŒ–</td></tr>
    <tr><td>æ¨¡å‹é€‰æ‹©</td><td>ç”¨æˆ·å¯åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼ˆGPT-4, Claude, etc.ï¼‰</td></tr>
    <tr><td>System Prompt</td><td>è‡ªå®šä¹‰ Instructions / Custom GPTs</td></tr>
  </table>

  <h3>éåŠŸèƒ½æ€§éœ€æ±‚ / Non-Functional Requirements</h3>
  <table>
    <tr><th>ç»´åº¦</th><th>ç›®æ ‡</th><th>åŸå› </th></tr>
    <tr><td>TTFT (Time to First Token)</td><td>< 500ms (p95)</td><td>ç”¨æˆ·æ„ŸçŸ¥å“åº”é€Ÿåº¦</td></tr>
    <tr><td>Throughput</td><td>30-50 tokens/sec per user</td><td>æ¥è¿‘é˜…è¯»é€Ÿåº¦</td></tr>
    <tr><td>å¯ç”¨æ€§</td><td>99.9%</td><td>ä»˜è´¹ç”¨æˆ· SLA</td></tr>
    <tr><td>å®‰å…¨æ€§</td><td>é˜² prompt injection, å†…å®¹è¿‡æ»¤</td><td>å…¬å¼€æœåŠ¡å¿…é¡»é˜²æŠ¤</td></tr>
    <tr><td>æˆæœ¬</td><td>$0.01-0.05 per conversation</td><td>å•†ä¸šå¯è¡Œæ€§</td></tr>
  </table>

  <h2>ğŸ—ï¸ Step 2: é«˜å±‚æ¶æ„ / High-Level Architecture</h2>
  <div class="diagram">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â–¶â”‚  API Gateway â”‚â”€â”€â”€â”€â–¶â”‚  Chat Service   â”‚
â”‚  (Web/App)  â”‚â—€â”€â”€â”€â”€â”‚  (Auth+Rate  â”‚â—€â”€â”€â”€â”€â”‚  (Orchestrator) â”‚
â”‚   SSE/WS    â”‚     â”‚   Limit)     â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                              â”‚                      â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Convo DB  â”‚              â”‚  LLM Gateway    â”‚    â”‚  Safety       â”‚
              â”‚ (History  â”‚              â”‚  (Router +      â”‚    â”‚  Service      â”‚
              â”‚  + State) â”‚              â”‚   Load Balance) â”‚    â”‚  (Filter)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚             â”‚             â”‚
                              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                              â”‚ GPU     â”‚   â”‚ GPU     â”‚  â”‚ Externalâ”‚
                              â”‚ Cluster â”‚   â”‚ Cluster â”‚  â”‚ APIs    â”‚
                              â”‚ (vLLM)  â”‚   â”‚ (TRT)   â”‚  â”‚ (OpenAI)â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </div>

  <div class="bilingual">
    <div class="zh">
      <h4>æ ¸å¿ƒç»„ä»¶ï¼š</h4>
      <ol>
        <li><strong>API Gateway</strong> â€” è®¤è¯ã€é™æµã€è·¯ç”±</li>
        <li><strong>Chat Service (Orchestrator)</strong> â€” æ ¸å¿ƒç¼–æ’å±‚ï¼Œç»„è£… promptã€ç®¡ç†ä¸Šä¸‹æ–‡</li>
        <li><strong>Conversation DB</strong> â€” å­˜å‚¨å¯¹è¯å†å²å’Œå…ƒæ•°æ®</li>
        <li><strong>LLM Gateway</strong> â€” æ¨¡å‹è·¯ç”±ã€è´Ÿè½½å‡è¡¡ã€fallback</li>
        <li><strong>Safety Service</strong> â€” è¾“å…¥/è¾“å‡ºå†…å®¹å®‰å…¨è¿‡æ»¤</li>
        <li><strong>Inference Backend</strong> â€” å®é™… GPU æ¨ç†é›†ç¾¤</li>
      </ol>
    </div>
    <div class="en">
      <h4>Core Components:</h4>
      <ol>
        <li><strong>API Gateway</strong> â€” Auth, rate limiting, routing</li>
        <li><strong>Chat Service (Orchestrator)</strong> â€” Core orchestration: prompt assembly, context management</li>
        <li><strong>Conversation DB</strong> â€” Conversation history + metadata storage</li>
        <li><strong>LLM Gateway</strong> â€” Model routing, load balancing, fallback</li>
        <li><strong>Safety Service</strong> â€” Input/output content filtering</li>
        <li><strong>Inference Backend</strong> â€” GPU inference clusters</li>
      </ol>
    </div>
  </div>

  <h2>âš™ï¸ Step 3: æ ¸å¿ƒæ·±å…¥ / Deep Dives</h2>

  <h3>3.1 å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† / Conversation Context Management</h3>
  <div class="highlight">
    <h4>ğŸ’¡ è¿™æ˜¯é¢è¯•ä¸­æœ€å®¹æ˜“å‡ºå½©çš„éƒ¨åˆ†</h4>
    <p>LLM æ˜¯æ— çŠ¶æ€çš„ â€” æ¯æ¬¡è¯·æ±‚å¿…é¡»æŠŠå®Œæ•´å¯¹è¯å†å²å‘é€è¿‡å»ã€‚ä½† context window æœ‰é™ï¼ˆ128K tokensï¼‰ï¼Œé•¿å¯¹è¯æ€ä¹ˆåŠï¼Ÿ</p>
  </div>

  <pre><code>// Chat Service: Context Window Management
interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
  tokenCount: number;
  timestamp: number;
}

class ContextManager {
  private maxTokens: number;        // e.g., 128000
  private reserveForOutput: number; // e.g., 4096
  
  buildPrompt(conversation: Message[], systemPrompt: string): Message[] {
    const budget = this.maxTokens - this.reserveForOutput;
    const system: Message = { role: 'system', content: systemPrompt, ... };
    let used = system.tokenCount;
    
    // Strategy 1: Sliding Window â€” keep most recent messages
    const selected: Message[] = [system];
    for (let i = conversation.length - 1; i >= 0; i--) {
      if (used + conversation[i].tokenCount > budget) break;
      selected.unshift(conversation[i]);  // prepend (keep order)
      used += conversation[i].tokenCount;
    }
    
    // Strategy 2: Summarize old context (production approach)
    if (conversation.length > selected.length - 1) {
      const droppedMessages = conversation.slice(0, conversation.length - selected.length + 1);
      const summary = await this.summarize(droppedMessages);
      selected.splice(1, 0, { role: 'system', content: `Previous context: ${summary}`, ... });
    }
    
    return selected;
  }
}</code></pre>

  <div class="bilingual">
    <div class="zh">
      <h4>ä¸‰ç§ç­–ç•¥å¯¹æ¯”ï¼š</h4>
      <table>
        <tr><th>ç­–ç•¥</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr>
        <tr><td>Sliding Window</td><td>ç®€å•å¯é </td><td>ä¸¢å¤±æ—©æœŸä¸Šä¸‹æ–‡</td></tr>
        <tr><td>Summarization</td><td>ä¿ç•™å…³é”®ä¿¡æ¯</td><td>é¢å¤– LLM è°ƒç”¨ = æˆæœ¬ + å»¶è¿Ÿ</td></tr>
        <tr><td>RAG on History</td><td>ç²¾å‡†å¬å›</td><td>æ¶æ„å¤æ‚</td></tr>
      </table>
      <p><strong>é¢è¯•å»ºè®®ï¼š</strong>å…ˆè¯´ Sliding Windowï¼Œå†æ Summarization ä½œä¸ºä¼˜åŒ–ã€‚è¯´åˆ° RAG on History ä¼šæ˜¯åŠ åˆ†é¡¹ã€‚</p>
    </div>
    <div class="en">
      <h4>Three strategy comparison:</h4>
      <table>
        <tr><th>Strategy</th><th>Pros</th><th>Cons</th></tr>
        <tr><td>Sliding Window</td><td>Simple, reliable</td><td>Loses early context</td></tr>
        <tr><td>Summarization</td><td>Preserves key info</td><td>Extra LLM call = cost + latency</td></tr>
        <tr><td>RAG on History</td><td>Precise recall</td><td>Architectural complexity</td></tr>
      </table>
      <p><strong>Interview tip:</strong> Start with Sliding Window, then propose Summarization as an optimization. Mentioning RAG on History is a bonus.</p>
    </div>
  </div>

  <h3>3.2 æµå¼è¾“å‡ºæ¶æ„ / Streaming Architecture</h3>
  <div class="diagram">
Client (Browser)                Chat Service              LLM Inference
      â”‚                              â”‚                         â”‚
      â”‚â”€â”€ POST /chat/completions â”€â”€â–¶â”‚                         â”‚
      â”‚                              â”‚â”€â”€ gRPC stream â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
      â”‚â—€â”€â”€ SSE: data: {"token":"H"} â”‚â—€â”€â”€ token: "H" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚â—€â”€â”€ SSE: data: {"token":"i"} â”‚â—€â”€â”€ token: "i" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚â—€â”€â”€ SSE: data: {"token":"!"} â”‚â—€â”€â”€ token: "!" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚â—€â”€â”€ SSE: data: [DONE]        â”‚â—€â”€â”€ EOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚                              â”‚                         â”‚
      â”‚                              â”‚â”€â”€ Save to DB â”€â”€â–¶ (async)â”‚
  </div>

  <pre><code>// Server-Sent Events (SSE) Streaming
app.post('/v1/chat/completions', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  
  const stream = await llmGateway.createCompletion({
    model: req.body.model,
    messages: contextManager.buildPrompt(req.body.messages),
    stream: true,
  });
  
  let fullResponse = '';
  
  for await (const chunk of stream) {
    const token = chunk.choices[0]?.delta?.content || '';
    fullResponse += token;
    
    // Send token to client immediately
    res.write(`data: ${JSON.stringify({ choices: [{ delta: { content: token } }] })}\n\n`);
  }
  
  res.write('data: [DONE]\n\n');
  res.end();
  
  // Async: save to DB, run safety check on full response
  await Promise.all([
    conversationDB.appendMessage(conversationId, 'assistant', fullResponse),
    safetyService.checkOutput(fullResponse),
  ]);
});</code></pre>

  <div class="highlight-blue">
    <h4>ğŸ”‘ é¢è¯•å…³é”®ç‚¹ï¼šSSE vs WebSocket</h4>
    <ul>
      <li><strong>SSE</strong>ï¼šå•å‘ï¼ˆæœåŠ¡å™¨â†’å®¢æˆ·ç«¯ï¼‰ï¼Œç®€å•å¯é ï¼ŒHTTP/2 è‡ªåŠ¨å¤ç”¨ï¼ŒChatGPT çš„é€‰æ‹©</li>
      <li><strong>WebSocket</strong>ï¼šåŒå‘ï¼Œæ›´ä½å»¶è¿Ÿï¼Œä½†æ›´å¤æ‚ï¼ˆéœ€è¦å¿ƒè·³ã€é‡è¿é€»è¾‘ï¼‰</li>
      <li><strong>é¢è¯•ç­”æ¡ˆï¼š</strong>ç”¨ SSE å³å¯ã€‚ChatGPT ä¹Ÿæ˜¯ SSEã€‚åªåœ¨éœ€è¦å®æ—¶åŒå‘ï¼ˆå¦‚è¯­éŸ³ï¼‰æ—¶æ‰ç”¨ WebSocket</li>
    </ul>
  </div>

  <h3>3.3 LLM Gateway / Model Router</h3>
  <div class="bilingual">
    <div class="zh">
      <p>LLM Gateway æ˜¯æœ€æ ¸å¿ƒçš„åŸºç¡€è®¾æ–½ç»„ä»¶ï¼Œè´Ÿè´£ï¼š</p>
      <ol>
        <li><strong>æ¨¡å‹è·¯ç”±</strong> â€” æ ¹æ®è¯·æ±‚å¤æ‚åº¦é€‰æ‹©æ¨¡å‹ï¼ˆå°é—®é¢˜ç”¨å°æ¨¡å‹çœé’±ï¼‰</li>
        <li><strong>è´Ÿè½½å‡è¡¡</strong> â€” åˆ†é…è¯·æ±‚åˆ°ä¸åŒ GPU é›†ç¾¤</li>
        <li><strong>Fallback</strong> â€” ä¸»æ¨¡å‹ä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§</li>
        <li><strong>ç¼“å­˜</strong> â€” Semantic Cache é¿å…é‡å¤æ¨ç†</li>
        <li><strong>é™æµ</strong> â€” ä¿æŠ¤ GPU èµ„æºä¸è¢«æ‰“çˆ†</li>
      </ol>
    </div>
    <div class="en">
      <p>The LLM Gateway is the most critical infrastructure component:</p>
      <ol>
        <li><strong>Model routing</strong> â€” Route by complexity (small questions â†’ small models to save cost)</li>
        <li><strong>Load balancing</strong> â€” Distribute across GPU clusters</li>
        <li><strong>Fallback</strong> â€” Auto-degrade when primary model is unavailable</li>
        <li><strong>Caching</strong> â€” Semantic cache to avoid redundant inference</li>
        <li><strong>Rate limiting</strong> â€” Protect GPU resources from overload</li>
      </ol>
    </div>
  </div>

  <pre><code>// Model Router: Cost-Aware Routing
class ModelRouter {
  async route(request: ChatRequest): Promise&lt;ModelConfig&gt; {
    const complexity = await this.classifyComplexity(request);
    //  â†‘ å¯ä»¥ç”¨å°æ¨¡å‹/è§„åˆ™æ¥åˆ†ç±»è¯·æ±‚å¤æ‚åº¦
    
    // Tier 1: Simple queries â†’ cheap model ($0.001/req)
    if (complexity === 'simple') {
      return { model: 'gpt-4o-mini', maxTokens: 1024 };
    }
    
    // Tier 2: Standard queries â†’ balanced model ($0.01/req)
    if (complexity === 'standard') {
      return { model: 'gpt-4o', maxTokens: 4096 };
    }
    
    // Tier 3: Complex reasoning â†’ premium model ($0.05/req)
    return { model: 'o1', maxTokens: 8192 };
  }
  
  // Semantic Cache: hash embedding of the query
  async checkCache(messages: Message[]): Promise&lt;string | null&gt; {
    const embedding = await embed(messages[messages.length - 1].content);
    const cached = await vectorDB.search(embedding, { threshold: 0.98 });
    return cached?.response || null;
  }
}</code></pre>

  <div class="highlight">
    <h4>ğŸ’° æˆæœ¬ä¼˜åŒ–æ•ˆæœï¼ˆé¢è¯•æ•°æ®å‚è€ƒï¼‰</h4>
    <table>
      <tr><th>ç­–ç•¥</th><th>èŠ‚çœ</th><th>å¤æ‚åº¦</th></tr>
      <tr><td>Model Routing (å°é¢˜å°æ¨¡å‹)</td><td>40-60%</td><td>ä¸­</td></tr>
      <tr><td>Semantic Cache</td><td>20-30%</td><td>ä¸­</td></tr>
      <tr><td>Prompt Compression</td><td>10-20%</td><td>ä½</td></tr>
      <tr><td>KV Cache (æ¨ç†å±‚)</td><td>30-50% latency</td><td>é«˜ï¼ˆvLLMï¼‰</td></tr>
      <tr><td><strong>ç»„åˆä½¿ç”¨</strong></td><td><strong>60-80%</strong></td><td></td></tr>
    </table>
  </div>

  <h3>3.4 å®‰å…¨ä¸é˜²æŠ¤ / Safety & Security</h3>
  <div class="diagram">
User Input                                           Output to User
    â”‚                                                      â–²
    â–¼                                                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Safety   â”‚                              â”‚ Output Safety    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ PII Filter â”‚ â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚ â”‚ Toxicity     â”‚ â”‚
â”‚ â”‚ Injection  â”‚ â”‚â”€â”€â–¶â”‚  LLM Core    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â”‚ Hallucinationâ”‚ â”‚
â”‚ â”‚ Detection  â”‚ â”‚   â”‚  (Generation)â”‚           â”‚ â”‚ PII Leakage  â”‚ â”‚
â”‚ â”‚ Toxicity   â”‚ â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ â”‚ Code Safety  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </div>

  <div class="highlight-red">
    <h4>âš ï¸ é¢è¯•å¿…æçš„å®‰å…¨ç‚¹</h4>
    <ol>
      <li><strong>Prompt Injection é˜²æŠ¤</strong> â€” ç”¨æˆ·è¯•å›¾è¦†ç›– system promptï¼ˆ"Ignore previous instructions..."ï¼‰
        <br>â†’ å¯¹ç­–ï¼šè¾“å…¥æ£€æµ‹ + system/user æ¶ˆæ¯éš”ç¦» + è¾“å‡ºéªŒè¯</li>
      <li><strong>PII è¿‡æ»¤</strong> â€” é˜²æ­¢æ¨¡å‹æ³„éœ²è®­ç»ƒæ•°æ®ä¸­çš„ä¸ªäººä¿¡æ¯
        <br>â†’ å¯¹ç­–ï¼šNER æ£€æµ‹ + è¾“å‡ºè¿‡æ»¤ + å·®åˆ†éšç§</li>
      <li><strong>Hallucination ç¼“è§£</strong> â€” æ¨¡å‹ç¼–é€ ä¸å­˜åœ¨çš„äº‹å®
        <br>â†’ å¯¹ç­–ï¼šGrounding (RAG) + ç½®ä¿¡åº¦æ£€æµ‹ + å¼•ç”¨æ¥æº</li>
      <li><strong>å†…å®¹å®‰å…¨</strong> â€” æœ‰å®³/è¿æ³•å†…å®¹ç”Ÿæˆ
        <br>â†’ å¯¹ç­–ï¼šåˆ†ç±»å™¨è¿‡æ»¤ + RLHF å¯¹é½ + äººå·¥å®¡æ ¸é˜Ÿåˆ—</li>
    </ol>
  </div>

  <h3>3.5 æ•°æ®å­˜å‚¨è®¾è®¡ / Storage Design</h3>
  <pre><code>// Conversation Storage Schema
CREATE TABLE conversations (
  id          UUID PRIMARY KEY,
  user_id     UUID NOT NULL,
  title       TEXT,           -- Auto-generated from first message
  model       VARCHAR(50),
  system_prompt TEXT,
  created_at  TIMESTAMP,
  updated_at  TIMESTAMP,
  message_count INT DEFAULT 0,
  total_tokens  INT DEFAULT 0  -- For billing
);

CREATE TABLE messages (
  id              UUID PRIMARY KEY,
  conversation_id UUID REFERENCES conversations(id),
  role            VARCHAR(20),   -- system/user/assistant/tool
  content         TEXT,
  token_count     INT,
  model           VARCHAR(50),   -- Which model generated this
  latency_ms      INT,           -- For monitoring
  created_at      TIMESTAMP,
  sequence_num    INT            -- Ordering within conversation
);

-- Index for fast conversation loading
CREATE INDEX idx_messages_conv_seq ON messages(conversation_id, sequence_num);

-- Partitioning by date for old conversation archival
-- (Conversations older than 90 days â†’ cold storage)</code></pre>

  <div class="bilingual">
    <div class="zh">
      <h4>å­˜å‚¨å†³ç­–ï¼š</h4>
      <ul>
        <li><strong>PostgreSQL</strong> å­˜å¯¹è¯å’Œæ¶ˆæ¯ â€” ç»“æ„åŒ–æ•°æ®ï¼ŒACID ä¿è¯</li>
        <li><strong>Redis</strong> ç¼“å­˜æ´»è·ƒå¯¹è¯ â€” å‡å°‘ DB è¯»å–ï¼Œæ¯«ç§’çº§åŠ è½½</li>
        <li><strong>S3/GCS</strong> å½’æ¡£è€å¯¹è¯ â€” 90å¤©+ çš„å¯¹è¯è½¬å†·å­˜å‚¨</li>
        <li><strong>ä¸ç”¨ NoSQLï¼Ÿ</strong> â€” æ¶ˆæ¯æœ‰ä¸¥æ ¼é¡ºåºæ€§ï¼Œå…³ç³»å‹æ›´åˆé€‚</li>
      </ul>
    </div>
    <div class="en">
      <h4>Storage Decisions:</h4>
      <ul>
        <li><strong>PostgreSQL</strong> for conversations & messages â€” structured data, ACID</li>
        <li><strong>Redis</strong> to cache active conversations â€” reduce DB reads, ms-level loading</li>
        <li><strong>S3/GCS</strong> for archival â€” conversations 90+ days old â†’ cold storage</li>
        <li><strong>Why not NoSQL?</strong> â€” Messages have strict ordering, relational fits better</li>
      </ul>
    </div>
  </div>

  <h2>ğŸ“ Step 4: æ‰©å±•æ€§è®¾è®¡ / Scalability</h2>
  <div class="bilingual">
    <div class="zh">
      <h4>å®¹é‡ä¼°ç®—ï¼ˆé¢è¯•å¸¸é—®ï¼‰ï¼š</h4>
      <ul>
        <li>æ—¥æ´»ç”¨æˆ· 10Mï¼Œæ¯äººå¹³å‡ 5 æ¬¡å¯¹è¯</li>
        <li>æ¯æ¬¡å¯¹è¯å¹³å‡ 2000 tokens (è¾“å…¥+è¾“å‡º)</li>
        <li>å³°å€¼ QPSï¼š~50K requests/sec</li>
        <li>GPU éœ€æ±‚ï¼šçº¦ 10K A100 (è‡ªå»º) or ç­‰ä»·äº‘èµ„æº</li>
        <li>å­˜å‚¨ï¼š~500TB/å¹´ å¯¹è¯æ•°æ®</li>
      </ul>
    </div>
    <div class="en">
      <h4>Capacity Estimation (common in interviews):</h4>
      <ul>
        <li>10M DAU, average 5 conversations each</li>
        <li>Average 2000 tokens per conversation (in+out)</li>
        <li>Peak QPS: ~50K requests/sec</li>
        <li>GPU needs: ~10K A100 (self-hosted) or equivalent cloud</li>
        <li>Storage: ~500TB/year conversation data</li>
      </ul>
    </div>
  </div>

  <div class="highlight-green">
    <h4>ğŸ”‘ æ‰©å±•æ€§å…³é”®å†³ç­–</h4>
    <ul>
      <li><strong>GPU æ˜¯ç“¶é¢ˆï¼Œä¸æ˜¯ CPU</strong> â€” ä¼ ç»Ÿç³»ç»ŸåŠ æœºå™¨å°±è¡Œï¼ŒLLM ç³»ç»ŸåŠ  GPU æˆæœ¬æé«˜</li>
      <li><strong>è¯·æ±‚æ’é˜Ÿ</strong> â€” GPU æ»¡è½½æ—¶æ’é˜Ÿè€Œéæ‹’ç»ï¼Œç”¨ priority queueï¼ˆä»˜è´¹ç”¨æˆ·ä¼˜å…ˆï¼‰</li>
      <li><strong>Continuous Batching</strong> â€” vLLM çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå¤šè¯·æ±‚å…±äº« GPUï¼ˆå›é¡¾ Day 2 vLLM æ¨é€ï¼‰</li>
      <li><strong>KV Cache ç®¡ç†</strong> â€” PagedAttention é¿å…æ˜¾å­˜æµªè´¹ï¼ˆå›é¡¾ Day 2 æ¨é€ï¼‰</li>
      <li><strong>æ°´å¹³æ‰©å±• Chat Service</strong> â€” æ— çŠ¶æ€è®¾è®¡ï¼Œå¯¹è¯çŠ¶æ€åœ¨ DB/Redis</li>
    </ul>
  </div>

  <h2>ğŸ¤ Step 5: é¢è¯•é«˜é¢‘é—®é¢˜ / Interview Questions</h2>

  <div class="interview-q">
    <h4>Q1: "å¦‚ä½•å¤„ç† 100 ä¸‡ç”¨æˆ·åŒæ—¶å‘é€è¯·æ±‚ï¼Ÿ"</h4>
    <div class="bilingual">
      <div class="zh">
        <p><strong>ç­”é¢˜æ€è·¯ï¼š</strong></p>
        <ol>
          <li>API Gateway é™æµ â€” æ¯ç”¨æˆ· RPM é™åˆ¶</li>
          <li>Priority Queue â€” ä»˜è´¹ç”¨æˆ·ä¼˜å…ˆï¼Œå…è´¹ç”¨æˆ·æ’é˜Ÿ</li>
          <li>GPU Cluster æ°´å¹³æ‰©å±• + Continuous Batching</li>
          <li>Model Routing â€” ç®€å•è¯·æ±‚ç”¨å°æ¨¡å‹ï¼Œé™ä½ GPU å‹åŠ›</li>
          <li>Semantic Cache â€” ç›¸ä¼¼é—®é¢˜ç›´æ¥è¿”å›ç¼“å­˜</li>
          <li>é™çº§ç­–ç•¥ â€” æç«¯æƒ…å†µï¼šé™ä½ max_tokens / ç”¨æ›´å°æ¨¡å‹</li>
        </ol>
      </div>
      <div class="en">
        <p><strong>Answer framework:</strong></p>
        <ol>
          <li>API Gateway rate limiting â€” per-user RPM limits</li>
          <li>Priority Queue â€” paid users first, free users queue</li>
          <li>GPU horizontal scaling + Continuous Batching</li>
          <li>Model Routing â€” simple queries â†’ small models, reduce GPU pressure</li>
          <li>Semantic Cache â€” similar queries return cached responses</li>
          <li>Degradation â€” extreme case: reduce max_tokens / use smaller models</li>
        </ol>
      </div>
    </div>
  </div>

  <div class="interview-q">
    <h4>Q2: "å¯¹è¯å†å²è¶Šæ¥è¶Šé•¿æ€ä¹ˆåŠï¼Ÿ"</h4>
    <div class="bilingual">
      <div class="zh">
        <p><strong>å±‚å±‚é€’è¿›å›ç­”ï¼š</strong></p>
        <ol>
          <li><strong>Level 1:</strong> Sliding Window â€” åªä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯</li>
          <li><strong>Level 2:</strong> å‹ç¼© + æ‘˜è¦ â€” è€æ¶ˆæ¯ç”¨ LLM å‹ç¼©æˆæ‘˜è¦</li>
          <li><strong>Level 3:</strong> RAG on History â€” æŠŠå†å²æ¶ˆæ¯å­˜å…¥å‘é‡åº“ï¼ŒæŒ‰éœ€æ£€ç´¢</li>
          <li><strong>Level 4:</strong> Hierarchical Memory â€” çŸ­æœŸ(æ»‘çª—) + ä¸­æœŸ(æ‘˜è¦) + é•¿æœŸ(å‘é‡åº“) ä¸‰å±‚è®°å¿†</li>
        </ol>
        <p>ğŸ’¡ <strong>åŠ åˆ†å›ç­”ï¼š</strong>æåˆ° OpenClaw / Claude çš„ memory æœºåˆ¶ â€” æ–‡ä»¶ç³»ç»Ÿ + è¯­ä¹‰æœç´¢æ··åˆæ–¹æ¡ˆ</p>
      </div>
      <div class="en">
        <p><strong>Progressive answer:</strong></p>
        <ol>
          <li><strong>Level 1:</strong> Sliding Window â€” keep only recent N messages</li>
          <li><strong>Level 2:</strong> Compression + Summary â€” summarize old messages via LLM</li>
          <li><strong>Level 3:</strong> RAG on History â€” store history in vector DB, retrieve on demand</li>
          <li><strong>Level 4:</strong> Hierarchical Memory â€” short-term (window) + mid-term (summary) + long-term (vector DB)</li>
        </ol>
        <p>ğŸ’¡ <strong>Bonus:</strong> Mention OpenClaw/Claude's memory â€” file system + semantic search hybrid</p>
      </div>
    </div>
  </div>

  <div class="interview-q">
    <h4>Q3: "å¦‚ä½•é˜²æ­¢ Prompt Injectionï¼Ÿ"</h4>
    <div class="bilingual">
      <div class="zh">
        <ol>
          <li><strong>è¾“å…¥æ£€æµ‹</strong> â€” åˆ†ç±»å™¨æ£€æµ‹æ¶æ„ prompt æ¨¡å¼</li>
          <li><strong>è§’è‰²éš”ç¦»</strong> â€” System prompt ç”¨ç‰¹æ®Š delimiter éš”ç¦»ï¼Œæ¨¡å‹è®­ç»ƒæ—¶å¼ºåŒ–è¾¹ç•Œ</li>
          <li><strong>è¾“å‡ºéªŒè¯</strong> â€” æ£€æµ‹è¾“å‡ºæ˜¯å¦è¿å system prompt çº¦æŸ</li>
          <li><strong>Sandwich Defense</strong> â€” åœ¨ç”¨æˆ·æ¶ˆæ¯å‰ååŠ é˜²æŠ¤ prompt</li>
          <li><strong>æœ€å°æƒé™</strong> â€” Tool Use æ—¶é™åˆ¶å¯è°ƒç”¨å·¥å…·çš„èŒƒå›´</li>
        </ol>
      </div>
      <div class="en">
        <ol>
          <li><strong>Input detection</strong> â€” Classifier to detect malicious prompt patterns</li>
          <li><strong>Role isolation</strong> â€” System prompt with special delimiters, reinforced during training</li>
          <li><strong>Output validation</strong> â€” Check if output violates system prompt constraints</li>
          <li><strong>Sandwich Defense</strong> â€” Add safety prompts before/after user message</li>
          <li><strong>Least privilege</strong> â€” Limit callable tools scope in Tool Use</li>
        </ol>
      </div>
    </div>
  </div>

  <div class="interview-q">
    <h4>Q4: "å¦‚ä½•è®¾è®¡ä»˜è´¹åˆ†å±‚ï¼ˆFree vs Proï¼‰ï¼Ÿ"</h4>
    <div class="bilingual">
      <div class="zh">
        <table>
          <tr><th>ç»´åº¦</th><th>Free</th><th>Pro ($20/mo)</th></tr>
          <tr><td>æ¨¡å‹</td><td>GPT-4o-mini</td><td>GPT-4o / o1</td></tr>
          <tr><td>RPM</td><td>10/min</td><td>60/min</td></tr>
          <tr><td>ä¸Šä¸‹æ–‡</td><td>8K tokens</td><td>128K tokens</td></tr>
          <tr><td>æ’é˜Ÿä¼˜å…ˆçº§</td><td>ä½</td><td>é«˜ï¼ˆä¸æ’é˜Ÿï¼‰</td></tr>
          <tr><td>åŠŸèƒ½</td><td>åŸºæœ¬å¯¹è¯</td><td>+ æ–‡ä»¶ä¸Šä¼  + å›¾ç‰‡ç”Ÿæˆ + Tool Use</td></tr>
        </table>
        <p>å®ç°ï¼šåœ¨ API Gateway å±‚æŒ‰ user tier åšè·¯ç”±å’Œé™æµã€‚</p>
      </div>
      <div class="en">
        <table>
          <tr><th>Dimension</th><th>Free</th><th>Pro ($20/mo)</th></tr>
          <tr><td>Model</td><td>GPT-4o-mini</td><td>GPT-4o / o1</td></tr>
          <tr><td>RPM</td><td>10/min</td><td>60/min</td></tr>
          <tr><td>Context</td><td>8K tokens</td><td>128K tokens</td></tr>
          <tr><td>Queue Priority</td><td>Low</td><td>High (no queue)</td></tr>
          <tr><td>Features</td><td>Basic chat</td><td>+ File upload + Image gen + Tool Use</td></tr>
        </table>
        <p>Implementation: Route and rate-limit at API Gateway layer based on user tier.</p>
      </div>
    </div>
  </div>

  <h2>ğŸ“š å¯å­¦ä¹ çš„æ¨¡å¼ / Patterns to Learn</h2>
  <div class="highlight-green">
    <h4>ä» ChatGPT æ¶æ„ä¸­æç‚¼çš„é€šç”¨æ¨¡å¼ï¼š</h4>
    <ol>
      <li><strong>ğŸ”€ Model Router Pattern</strong> â€” æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€é€‰æ¨¡å‹ï¼ˆé€‚ç”¨æ‰€æœ‰ LLM åº”ç”¨ï¼‰</li>
      <li><strong>ğŸ“¡ SSE Streaming Pattern</strong> â€” Token-level æµå¼è¾“å‡ºï¼ˆæ¯”ä¼ ç»Ÿ request-response æ›´å¥½çš„ UXï¼‰</li>
      <li><strong>ğŸ§  Hierarchical Memory</strong> â€” å¤šå±‚è®°å¿†ç®¡ç†ï¼ˆçŸ­/ä¸­/é•¿æœŸï¼‰ï¼Œå¯ç”¨äºä»»ä½•æœ‰ä¸Šä¸‹æ–‡çš„ç³»ç»Ÿ</li>
      <li><strong>ğŸ›¡ï¸ Safety Sandwich</strong> â€” è¾“å…¥è¿‡æ»¤ â†’ å¤„ç† â†’ è¾“å‡ºè¿‡æ»¤çš„åŒé‡é˜²æŠ¤æ¨¡å¼</li>
      <li><strong>ğŸ’° Token Budget Management</strong> â€” åƒç®¡ç†é’±ä¸€æ ·ç®¡ç† tokensï¼ˆè¿™æ˜¯ LLM ç³»ç»Ÿç‹¬æœ‰çš„æ€ç»´ï¼‰</li>
      <li><strong>ğŸ“Š Async Post-Processing</strong> â€” å…ˆè¿”å›ç»“æœï¼Œåå°å¼‚æ­¥åš safety check / å­˜å‚¨ / åˆ†æ</li>
    </ol>
  </div>

  <h2>ğŸ—ºï¸ é¢è¯•ç­”é¢˜æ¡†æ¶ / Interview Answer Framework</h2>
  <div class="architecture">
    <h4>45 åˆ†é’Ÿåˆ†é…å»ºè®®ï¼š</h4>
    <table>
      <tr><th>é˜¶æ®µ</th><th>æ—¶é—´</th><th>å†…å®¹</th></tr>
      <tr><td>1. éœ€æ±‚æ¾„æ¸…</td><td>5 min</td><td>é—® LLM ç‰¹æœ‰é—®é¢˜ï¼ˆtoken budget, å»¶è¿Ÿ, å¤šæ¨¡æ€ï¼‰</td></tr>
      <tr><td>2. é«˜å±‚æ¶æ„</td><td>10 min</td><td>ç”» 6 ä¸ªæ ¸å¿ƒç»„ä»¶ + æ•°æ®æµ</td></tr>
      <tr><td>3. æ ¸å¿ƒæ·±å…¥</td><td>20 min</td><td>ä¸Šä¸‹æ–‡ç®¡ç† + æµå¼è¾“å‡º + æ¨¡å‹è·¯ç”±ï¼ˆé¢è¯•å®˜ä¼šå¼•å¯¼é€‰ 1-2 ä¸ªï¼‰</td></tr>
      <tr><td>4. æ‰©å±• + å®‰å…¨</td><td>8 min</td><td>å®¹é‡ä¼°ç®— + Safety å››è¦ç´ </td></tr>
      <tr><td>5. æ€»ç»“</td><td>2 min</td><td>Trade-offs å›é¡¾ + æœªæ¥ä¼˜åŒ–æ–¹å‘</td></tr>
    </table>
  </div>

  <div class="follow-up">
    <h3>ğŸ’¬ ç³»åˆ—é¢„å‘Š / Series Preview</h3>
    <p>ğŸ¯ <strong>Phase 2: AI System Design é¢è¯•é¢˜ç³»åˆ—</strong></p>
    <ul>
      <li>âœ… Day 1: è®¾è®¡ ChatGPTï¼ˆä»Šå¤©ï¼‰</li>
      <li>ğŸ“‹ Day 2: è®¾è®¡æ¨èç³»ç»Ÿ (Netflix/YouTube)</li>
      <li>ğŸ“‹ Day 3: è®¾è®¡è¯­ä¹‰æœç´¢å¼•æ“</li>
      <li>ğŸ“‹ Day 4: è®¾è®¡ RAG çŸ¥è¯†åº“é—®ç­”</li>
      <li>ğŸ“‹ Day 5: è®¾è®¡ AI Code Review ç³»ç»Ÿ</li>
    </ul>
    <p>æ¯ç¯‡éƒ½æ˜¯å®Œæ•´çš„ 45 åˆ†é’Ÿé¢è¯•å›ç­”æ¨¡æ¿ï¼Œå¯ä»¥ç›´æ¥ç”¨äºé¢è¯•å‡†å¤‡ï¼</p>
  </div>

  <p style="text-align: center; color: #999; margin-top: 40px;">
    Friday's AI Knowledge Push âœ¨ | 2026-02-10 | Phase 2 Day 1
  </p>
</body>
</html>