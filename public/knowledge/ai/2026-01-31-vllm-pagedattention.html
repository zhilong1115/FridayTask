<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-01-31 - vLLM & PagedAttention</title>
  <style>
    body { font-family: -apple-system, system-ui, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.7; color: #202124; }
    .bilingual { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }
    @media (max-width: 768px) { .bilingual { grid-template-columns: 1fr; } }
    .zh { border-left: 3px solid #f9ab00; padding-left: 15px; }
    .en { border-left: 3px solid #1a73e8; padding-left: 15px; }
    h1 { color: #3c4043; border-bottom: 2px solid #f9ab00; padding-bottom: 10px; }
    h2 { color: #1a73e8; margin-top: 40px; }
    h3 { color: #5f6368; }
    .meta { color: #5f6368; font-size: 0.9em; margin-bottom: 30px; }
    .meta a { color: #1a73e8; }
    .architecture { background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; font-family: 'SF Mono', Monaco, monospace; font-size: 0.85em; white-space: pre; overflow-x: auto; line-height: 1.4; }
    pre { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; overflow-x: auto; font-size: 0.9em; }
    code { font-family: 'SF Mono', Monaco, Consolas, monospace; }
    .inline-code { background: #f1f3f4; padding: 2px 6px; border-radius: 4px; font-size: 0.9em; }
    .highlight { background: #fff3cd; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #f9ab00; }
    .highlight h4 { margin-top: 0; color: #856404; }
    .key-insight { background: #d4edda; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #28a745; }
    .key-insight h4 { margin-top: 0; color: #155724; }
    .comparison { background: #e8f4fd; padding: 15px; border-radius: 8px; margin: 15px 0; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    th, td { border: 1px solid #dadce0; padding: 10px; text-align: left; }
    th { background: #f8f9fa; }
    .follow-up { margin-top: 40px; padding: 20px; border: 2px dashed #dadce0; border-radius: 8px; background: #fafafa; }
    .emoji { font-size: 1.2em; }
  </style>
</head>
<body>
  <h1>ğŸš€ vLLM & PagedAttention</h1>
  <p class="meta">
    <a href="https://github.com/vllm-project/vllm">GitHub</a> | 
    â­ 40k+ stars | 
    ğŸ“„ <a href="https://arxiv.org/abs/2309.06180">SOSP 2023 Paper</a> |
    ğŸ›ï¸ UC Berkeley Sky Lab
  </p>

  <h2>ğŸ“– é¡¹ç›®ç®€ä»‹ / Overview</h2>
  <div class="bilingual">
    <div class="zh">
      <p><strong>vLLM</strong> æ˜¯ç›®å‰æœ€æµè¡Œçš„ LLM æ¨ç†æœåŠ¡å¼•æ“ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯ <strong>PagedAttention</strong>â€”â€”ä¸€ç§å—æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜å¯å‘çš„æ³¨æ„åŠ›ç®—æ³•ã€‚</p>
      <p>å…³é”®æ´å¯Ÿï¼šLLM æ¨ç†çš„ç“¶é¢ˆä¸æ˜¯è®¡ç®—ï¼Œè€Œæ˜¯ <strong>å†…å­˜ç®¡ç†</strong>ã€‚ä¼ ç»Ÿç³»ç»Ÿå› ä¸º KV Cache çš„ç¢ç‰‡åŒ–å’Œè¿‡åº¦é¢„ç•™ï¼Œæµªè´¹äº† 60%-80% çš„ GPU å†…å­˜ï¼</p>
      <p>vLLM é€šè¿‡ PagedAttention å°†å†…å­˜æµªè´¹é™åˆ° 4% ä»¥ä¸‹ï¼Œååé‡æå‡ 2-24 å€ã€‚</p>
    </div>
    <div class="en">
      <p><strong>vLLM</strong> is the most popular LLM inference engine. Its core innovation is <strong>PagedAttention</strong>â€”an attention algorithm inspired by OS virtual memory.</p>
      <p>Key insight: LLM inference bottleneck isn't compute, it's <strong>memory management</strong>. Traditional systems waste 60%-80% of GPU memory due to KV cache fragmentation!</p>
      <p>vLLM reduces memory waste to under 4%, achieving 2-24x throughput improvement.</p>
    </div>
  </div>

  <h2>ğŸ”¥ ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ / Why This Matters</h2>
  <div class="bilingual">
    <div class="zh">
      <p>æ˜¨å¤©æˆ‘ä»¬è®²äº† DeepSeek-V3 å¦‚ä½•ç”¨åˆ›æ–°æ–¹æ³•<strong>è®­ç»ƒ</strong>æ¨¡å‹ã€‚ä»Šå¤©çš„ vLLM è§£å†³çš„æ˜¯å¦ä¸€åŠé—®é¢˜ï¼šå¦‚ä½•é«˜æ•ˆåœ°<strong>æœåŠ¡</strong>æ¨¡å‹ã€‚</p>
      <ul>
        <li><strong>è®­ç»ƒ</strong>ï¼šä¸€æ¬¡æ€§æˆæœ¬ï¼Œç”¨æœ€å¥½çš„ç¡¬ä»¶</li>
        <li><strong>æ¨ç†</strong>ï¼šæŒç»­æˆæœ¬ï¼Œæ¯ä¸ªè¯·æ±‚éƒ½è¦ç®—</li>
      </ul>
      <p>å¯¹äºç”Ÿäº§ç³»ç»Ÿï¼Œæ¨ç†æˆæœ¬å¾€å¾€æ˜¯è®­ç»ƒçš„ 10-100 å€ã€‚vLLM çš„ä¼˜åŒ–ç›´æ¥å½±å“è¿è¥æˆæœ¬ã€‚</p>
    </div>
    <div class="en">
      <p>Yesterday we covered how DeepSeek-V3 innovates in <strong>training</strong>. Today's vLLM solves the other half: efficiently <strong>serving</strong> models.</p>
      <ul>
        <li><strong>Training</strong>: One-time cost, use best hardware</li>
        <li><strong>Inference</strong>: Ongoing cost, every request counts</li>
      </ul>
      <p>For production, inference costs are often 10-100x training. vLLM optimizations directly impact operational costs.</p>
    </div>
  </div>

  <h2>ğŸ—ï¸ æ¶æ„åˆ†æ / Architecture</h2>
  
  <h3>é—®é¢˜ï¼šKV Cache çš„å†…å­˜å›°å¢ƒ</h3>
  <div class="architecture">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ä¼ ç»Ÿ KV Cache ç®¡ç†                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Request 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]    â”‚
â”‚              â†‘ å®é™…ä½¿ç”¨      â†‘ é¢„ç•™ä½†æœªä½¿ç”¨ (æµªè´¹!)         â”‚
â”‚                                                             â”‚
â”‚  Request 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]                     â”‚
â”‚                                                             â”‚
â”‚  Request 3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ é—®é¢˜ 1: ç¢ç‰‡åŒ– - è¯·æ±‚ä¹‹é—´çš„å†…å­˜ç©ºéš™æ— æ³•åˆ©ç”¨           â”‚   â”‚
â”‚  â”‚ é—®é¢˜ 2: è¿‡åº¦é¢„ç•™ - ä¸çŸ¥é“åºåˆ—ä¼šå¤šé•¿ï¼Œåªèƒ½é¢„ç•™æœ€å¤§å€¼   â”‚   â”‚
â”‚  â”‚ é—®é¢˜ 3: å†…å­˜æµªè´¹ç‡ 60%-80%ï¼                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </div>

  <h3>è§£å†³æ–¹æ¡ˆï¼šPagedAttention</h3>
  <div class="architecture">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PagedAttention è®¾è®¡                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  é€»è¾‘è§†å›¾ (Logical View)          ç‰©ç†å†…å­˜ (Physical Memory)â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Seq 1: [B0][B1][B2] â”‚    â”Œâ”€â”€â”€â”€â†’â”‚ Block 7: â–ˆâ–ˆâ–ˆâ–ˆ      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚         â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’â”‚ Block 3: â–ˆâ–ˆâ–ˆâ–ˆ      â”‚  â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’â”‚ Block 1: â–ˆâ–ˆâ–ˆâ–ˆ      â”‚  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚                                   â”‚ Block 2: â–ˆâ–ˆâ–ˆâ–ˆ (Seq2)â”‚  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Seq 2: [B0][B1]     â”‚    â”Œâ”€â”€â”€â”€â†’â”‚ Block 5: â–ˆâ–ˆâ–ˆâ–ˆ      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Block 4: (free)    â”‚  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Block 6: (free)    â”‚  â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ å…³é”®è®¾è®¡ï¼š                                            â”‚   â”‚
â”‚  â”‚ â€¢ Block = å›ºå®šæ•°é‡çš„ tokens (å¦‚ 16 ä¸ª)                â”‚   â”‚
â”‚  â”‚ â€¢ é€»è¾‘å— â†’ ç‰©ç†å—çš„æ˜ å°„è¡¨ (åƒ OS é¡µè¡¨)                â”‚   â”‚
â”‚  â”‚ â€¢ æŒ‰éœ€åˆ†é…ï¼Œåªåœ¨ç”Ÿæˆæ–° token æ—¶æ‰åˆ†é…æ–°å—             â”‚   â”‚
â”‚  â”‚ â€¢ å†…å­˜æµªè´¹åªå‘ç”Ÿåœ¨æœ€åä¸€ä¸ªå— â†’ <4% æµªè´¹ç‡            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </div>

  <div class="key-insight">
    <h4>ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿï¼šå€Ÿé‰´æ“ä½œç³»ç»Ÿçš„ç»å…¸è®¾è®¡</h4>
    <table>
      <tr><th>OS æ¦‚å¿µ</th><th>vLLM å¯¹åº”</th></tr>
      <tr><td>è™šæ‹Ÿå†…å­˜ (Virtual Memory)</td><td>é€»è¾‘ KV Cache ç©ºé—´</td></tr>
      <tr><td>ç‰©ç†å†…å­˜ (Physical Memory)</td><td>GPU æ˜¾å­˜</td></tr>
      <tr><td>é¡µ (Page)</td><td>KV Block (16 tokens)</td></tr>
      <tr><td>é¡µè¡¨ (Page Table)</td><td>Block Table</td></tr>
      <tr><td>è¿›ç¨‹ (Process)</td><td>è¯·æ±‚åºåˆ— (Sequence)</td></tr>
      <tr><td>Copy-on-Write</td><td>å…±äº« prompt çš„ KV Cache</td></tr>
    </table>
  </div>

  <h2>âœ¨ è®¾è®¡äº®ç‚¹ / Design Highlights</h2>

  <div class="highlight">
    <h4>äº®ç‚¹ 1: æŒ‰éœ€åˆ†é… (On-Demand Allocation)</h4>
    <div class="bilingual">
      <div class="zh">
        <p>ä¼ ç»Ÿæ–¹æ³•ï¼šé¢„å…ˆåˆ†é… max_seq_len çš„å†…å­˜ï¼Œå³ä½¿åºåˆ—å¾ˆçŸ­ä¹Ÿå æ»¡ç©ºé—´ã€‚</p>
        <p>PagedAttentionï¼šæ¯ç”Ÿæˆ 16 ä¸ª token æ‰åˆ†é…ä¸€ä¸ªæ–° blockã€‚åºåˆ—ç»“æŸæ—¶ç«‹å³é‡Šæ”¾ã€‚</p>
        <p><strong>æ•ˆæœ</strong>ï¼šåŒæ ·çš„ GPU å†…å­˜å¯ä»¥åŒæ—¶å¤„ç†æ›´å¤šè¯·æ±‚ â†’ ååé‡é£™å‡ã€‚</p>
      </div>
      <div class="en">
        <p>Traditional: Pre-allocate max_seq_len memory, even short sequences take full space.</p>
        <p>PagedAttention: Allocate new block only when 16 new tokens are generated. Release immediately when done.</p>
        <p><strong>Result</strong>: Same GPU memory handles more concurrent requests â†’ throughput skyrockets.</p>
      </div>
    </div>
  </div>

  <div class="highlight">
    <h4>äº®ç‚¹ 2: Copy-on-Write å†…å­˜å…±äº«</h4>
    <div class="bilingual">
      <div class="zh">
        <p><strong>åœºæ™¯</strong>ï¼šParallel samplingï¼ˆåŒä¸€ä¸ª prompt ç”Ÿæˆå¤šä¸ªå›å¤ï¼‰</p>
        <p>ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ªå›å¤éƒ½å¤åˆ¶ä¸€ä»½ prompt çš„ KV cacheã€‚</p>
        <p>PagedAttentionï¼šæ‰€æœ‰å›å¤å…±äº« prompt çš„ blocksï¼Œåªæœ‰å½“æŸä¸ªå›å¤éœ€è¦ä¿®æ”¹æ—¶æ‰å¤åˆ¶ã€‚</p>
        <p><strong>æ•ˆæœ</strong>ï¼šParallel sampling å†…å­˜ä½¿ç”¨å‡å°‘ 55%ï¼Œååé‡æå‡ 2.2 å€ã€‚</p>
      </div>
      <div class="en">
        <p><strong>Scenario</strong>: Parallel sampling (same prompt, multiple completions)</p>
        <p>Traditional: Each completion copies prompt's KV cache.</p>
        <p>PagedAttention: All completions share prompt blocks, copy only when modification needed.</p>
        <p><strong>Result</strong>: 55% memory reduction for parallel sampling, 2.2x throughput boost.</p>
      </div>
    </div>
  </div>

  <div class="highlight">
    <h4>äº®ç‚¹ 3: Continuous Batching</h4>
    <div class="bilingual">
      <div class="zh">
        <p>ä¼ ç»Ÿæ–¹æ³•ï¼šç­‰ä¸€ä¸ª batch å…¨éƒ¨å®Œæˆæ‰å¼€å§‹ä¸‹ä¸€ä¸ª batchã€‚</p>
        <p>vLLMï¼šæŸä¸ªè¯·æ±‚å®Œæˆåç«‹å³ç”¨æ–°è¯·æ±‚å¡«è¡¥ç©ºä½ã€‚GPU æ°¸è¿œæ»¡è½½ã€‚</p>
        <p>é…åˆ PagedAttention çš„çµæ´»å†…å­˜ç®¡ç†ï¼Œæ–°è¯·æ±‚å¯ä»¥æ— ç¼åŠ å…¥æ­£åœ¨è¿è¡Œçš„ batchã€‚</p>
      </div>
      <div class="en">
        <p>Traditional: Wait for entire batch to complete before starting next.</p>
        <p>vLLM: When one request finishes, immediately fill the slot with new request. GPU always busy.</p>
        <p>Combined with PagedAttention's flexible memory, new requests seamlessly join running batches.</p>
      </div>
    </div>
  </div>

  <h2>ğŸ’» å…³é”®ä»£ç  / Key Code</h2>
  
  <h3>Block Manager æ ¸å¿ƒé€»è¾‘</h3>
  <pre><code class="language-python"># ç®€åŒ–çš„ Block Manager å®ç°æ€è·¯
class BlockManager:
    def __init__(self, block_size: int, num_gpu_blocks: int):
        self.block_size = block_size  # æ¯ä¸ª block å­˜å‚¨çš„ token æ•°
        self.free_blocks: List[int] = list(range(num_gpu_blocks))
        self.block_tables: Dict[int, List[int]] = {}  # seq_id -> [block_ids]
    
    def allocate(self, seq_id: int) -> int:
        """ä¸ºåºåˆ—åˆ†é…ä¸€ä¸ªæ–°çš„ç‰©ç†å—"""
        if not self.free_blocks:
            raise OutOfMemoryError("No free blocks available")
        
        block_id = self.free_blocks.pop()
        if seq_id not in self.block_tables:
            self.block_tables[seq_id] = []
        self.block_tables[seq_id].append(block_id)
        return block_id
    
    def can_append_slot(self, seq_id: int) -> bool:
        """æ£€æŸ¥å½“å‰å—æ˜¯å¦è¿˜æœ‰ç©ºé—´"""
        if seq_id not in self.block_tables:
            return False
        num_tokens = self._get_num_tokens(seq_id)
        return num_tokens % self.block_size != 0
    
    def fork(self, parent_seq_id: int, child_seq_id: int):
        """Copy-on-Write: å­åºåˆ—å…±äº«çˆ¶åºåˆ—çš„å—"""
        parent_blocks = self.block_tables[parent_seq_id]
        self.block_tables[child_seq_id] = parent_blocks.copy()
        # å¢åŠ å¼•ç”¨è®¡æ•°ï¼Œå®é™…å†™å…¥æ—¶æ‰å¤åˆ¶
        for block_id in parent_blocks:
            self._increment_ref_count(block_id)</code></pre>

  <h3>PagedAttention Kernel (CUDA ä¼ªä»£ç )</h3>
  <pre><code class="language-cpp">// ç®€åŒ–çš„ PagedAttention è®¡ç®—
__global__ void paged_attention_kernel(
    float* output,           // [num_seqs, num_heads, head_dim]
    const float* query,      // [num_seqs, num_heads, head_dim]
    const float* key_cache,  // [num_blocks, block_size, num_heads, head_dim]
    const float* value_cache,
    const int* block_tables, // [num_seqs, max_blocks_per_seq]
    const int* seq_lens
) {
    int seq_idx = blockIdx.x;
    int head_idx = blockIdx.y;
    int seq_len = seq_lens[seq_idx];
    
    float qk_max = -INFINITY;
    
    // éå†è¯¥åºåˆ—çš„æ‰€æœ‰é€»è¾‘å—
    for (int i = 0; i < seq_len; i += BLOCK_SIZE) {
        // æŸ¥è¡¨è·å–ç‰©ç†å—ä½ç½®
        int logical_block = i / BLOCK_SIZE;
        int physical_block = block_tables[seq_idx * MAX_BLOCKS + logical_block];
        
        // ä»éè¿ç»­çš„ç‰©ç†å—ä¸­è·å– K, V
        float* k_ptr = key_cache + physical_block * BLOCK_SIZE * HEAD_DIM;
        float* v_ptr = value_cache + physical_block * BLOCK_SIZE * HEAD_DIM;
        
        // è®¡ç®—æ³¨æ„åŠ› (QK^T / sqrt(d))
        // ...
    }
    // Softmax + V åŠ æƒæ±‚å’Œ
    // ...
}</code></pre>

  <h2>ğŸ“Š æ€§èƒ½å¯¹æ¯” / Performance Comparison</h2>
  
  <div class="comparison">
    <table>
      <tr>
        <th>ç³»ç»Ÿ</th>
        <th>ååé‡ (ç›¸å¯¹)</th>
        <th>å†…å­˜æ•ˆç‡</th>
        <th>ç‰¹ç‚¹</th>
      </tr>
      <tr>
        <td>HuggingFace Transformers</td>
        <td>1x (baseline)</td>
        <td>~20-40%</td>
        <td>ç®€å•æ˜“ç”¨ï¼Œæœªä¼˜åŒ–</td>
      </tr>
      <tr>
        <td>TGI (HuggingFace)</td>
        <td>3-8x</td>
        <td>~50-60%</td>
        <td>è¿ç»­æ‰¹å¤„ç†</td>
      </tr>
      <tr>
        <td><strong>vLLM</strong></td>
        <td><strong>8-24x</strong></td>
        <td><strong>>96%</strong></td>
        <td>PagedAttention</td>
      </tr>
    </table>
  </div>

  <h2>ğŸ“š å¯å­¦ä¹ çš„æ¨¡å¼ / Patterns to Learn</h2>
  
  <div class="bilingual">
    <div class="zh">
      <h4>1. ä»ç»å…¸ç³»ç»Ÿè®¾è®¡å€Ÿé‰´ (OS â†’ ML)</h4>
      <p>PagedAttention ä¸æ˜¯å‡­ç©ºåˆ›é€ çš„æ–°ç®—æ³•ï¼Œè€Œæ˜¯å°†æ“ä½œç³»ç»Ÿ 50 å¹´å‰çš„è™šæ‹Ÿå†…å­˜æŠ€æœ¯åº”ç”¨åˆ° GPU å†…å­˜ç®¡ç†ã€‚</p>
      <p><strong>å¯ç¤º</strong>ï¼šè§£å†³æ–°é—®é¢˜æ—¶ï¼Œå…ˆæƒ³æƒ³æœ‰æ²¡æœ‰ç»å…¸æ–¹æ¡ˆå¯ä»¥å€Ÿé‰´ã€‚</p>

      <h4>2. æ‰¾åˆ°çœŸæ­£çš„ç“¶é¢ˆ</h4>
      <p>å¾ˆå¤šäººè®¤ä¸º LLM æ¨ç†æ…¢æ˜¯å› ä¸ºè®¡ç®—é‡å¤§ã€‚vLLM å›¢é˜Ÿå‘ç°çœŸæ­£ç“¶é¢ˆæ˜¯å†…å­˜ç®¡ç†ã€‚</p>
      <p><strong>å¯ç¤º</strong>ï¼šProfile first, optimize later. ä¸è¦å‡è®¾ç“¶é¢ˆåœ¨å“ªé‡Œã€‚</p>

      <h4>3. ç®€å•çš„ APIï¼Œå¤æ‚çš„å®ç°</h4>
      <p>vLLM å¯¹å¤–æš´éœ²çš„æ˜¯æ ‡å‡† OpenAI APIï¼Œç”¨æˆ·æ— éœ€äº†è§£ PagedAttentionã€‚</p>
      <p><strong>å¯ç¤º</strong>ï¼šå¥½çš„æ¶æ„éšè—å¤æ‚æ€§ï¼Œæä¾›ç®€å•æ¥å£ã€‚</p>
    </div>
    <div class="en">
      <h4>1. Borrow from Classic System Design</h4>
      <p>PagedAttention isn't a novel inventionâ€”it applies 50-year-old OS virtual memory to GPU memory management.</p>
      <p><strong>Lesson</strong>: When solving new problems, look for classic solutions to adapt.</p>

      <h4>2. Find the Real Bottleneck</h4>
      <p>Many assumed LLM inference is slow due to compute. vLLM team found memory management was the real issue.</p>
      <p><strong>Lesson</strong>: Profile first, optimize later. Don't assume where bottlenecks are.</p>

      <h4>3. Simple API, Complex Implementation</h4>
      <p>vLLM exposes standard OpenAI APIâ€”users don't need to understand PagedAttention.</p>
      <p><strong>Lesson</strong>: Good architecture hides complexity, provides simple interfaces.</p>
    </div>
  </div>

  <h2>ğŸ”— ä¸ DeepSeek çš„è”ç³» / Connection to DeepSeek</h2>
  
  <div class="key-insight">
    <h4>å®Œæ•´çš„ LLM ç”Ÿå‘½å‘¨æœŸä¼˜åŒ–</h4>
    <table>
      <tr>
        <th>é˜¶æ®µ</th>
        <th>é¡¹ç›®</th>
        <th>åˆ›æ–°ç‚¹</th>
      </tr>
      <tr>
        <td>è®­ç»ƒ</td>
        <td>DeepSeek-V3 (æ˜¨å¤©)</td>
        <td>FP8 è®­ç»ƒã€MoE è´Ÿè½½å‡è¡¡</td>
      </tr>
      <tr>
        <td>æ¨ç†</td>
        <td>vLLM (ä»Šå¤©)</td>
        <td>PagedAttention å†…å­˜ç®¡ç†</td>
      </tr>
    </table>
    <p>æœ‰è¶£çš„æ˜¯ï¼ŒvLLM å·²ç»æ”¯æŒ DeepSeek-V3 æ¨¡å‹çš„æ¨ç†ï¼ä¸¤ä¸ªé¡¹ç›®çš„ä¼˜åŒ–å¯ä»¥å åŠ ã€‚</p>
  </div>

  <h2>ğŸ› ï¸ å®è·µå»ºè®® / Practical Tips</h2>
  
  <pre><code class="language-bash"># å®‰è£… vLLM
pip install vllm

# å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --tensor-parallel-size 1

# ä½¿ç”¨æ ‡å‡† OpenAI API è°ƒç”¨
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "prompt": "Hello, my name is",
        "max_tokens": 100
    }'</code></pre>

  <div class="follow-up">
    <h3>ğŸ’¬ åç»­è®¨è®º / Follow-up Discussion</h3>
    <p>æƒ³æ·±å…¥äº†è§£å“ªä¸ªæ–¹é¢ï¼Ÿ</p>
    <ul>
      <li>ğŸ”§ vLLM çš„ CUDA kernel å®ç°ç»†èŠ‚ï¼Ÿ</li>
      <li>ğŸ“Š å¦‚ä½•åœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šéƒ¨ç½²å’Œè°ƒä¼˜ï¼Ÿ</li>
      <li>ğŸ”„ vLLM vs TGI vs Triton çš„è¯¦ç»†å¯¹æ¯”ï¼Ÿ</li>
      <li>ğŸ§ª PagedAttention çš„æ•°å­¦åŸç†å’Œæ­£ç¡®æ€§è¯æ˜ï¼Ÿ</li>
    </ul>
    <p><em>ï¼ˆZhilong æé—®åè¡¥å……ï¼‰</em></p>
  </div>

  <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #dadce0; color: #5f6368; font-size: 0.9em;">
    <p>ğŸ“… 2026-01-31 | Friday's Daily AI Push | Phase 2.1: æ¨ç†ä¼˜åŒ–</p>
  </footer>
</body>
</html>
